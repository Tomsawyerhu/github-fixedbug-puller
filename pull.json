[
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57352,
  "title": "r2.7 cherry-pick: 7cdf9d4d208 \"Fix QuantizedAvgPool invalid rank issue.\"",
  "tags": [],
  "closed_time": "2022-08-21T23:22:06Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/quantized_pooling_ops.cc",
    "code1": "// See docs in ../ops/nn_ops.cc.#include \"tensorflow/core/framework/op_requires.h\"#include \"tensorflow/core/platform/errors.h\"#define EIGEN_USE_THREADS#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"#include \"tensorflow/core/framework/numeric_op.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/kernels/ops_util.h\"#include \"tensorflow/core/kernels/pooling_ops_common.h\"#include \"tensorflow/core/lib/core/errors.h\"#include \"tensorflow/core/platform/logging.h\"#include \"tensorflow/core/util/padding.h\"#include \"tensorflow/core/util/tensor_format.h\"",
    "code2": "// See docs in ../ops/nn_ops.cc.#define EIGEN_USE_THREADS#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"#include \"tensorflow/core/framework/numeric_op.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/op_requires.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/kernels/ops_util.h\"#include \"tensorflow/core/kernels/pooling_ops_common.h\"#include \"tensorflow/core/lib/core/errors.h\"#include \"tensorflow/core/platform/errors.h\"#include \"tensorflow/core/platform/logging.h\"#include \"tensorflow/core/util/padding.h\"#include \"tensorflow/core/util/tensor_format.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/quantized_pooling_ops.cc",
    "code1": " return;    } const float min_input = context->input(1).flat<float>()(0); const float max_input = context->input(2).flat<float>()(0); OP_REQUIRES(context, params.depth_window == 1, errors::Unimplemented(\"Non-spatial pooling is not \"",
    "code2": " return;    } const Tensor& min_input_tensor = context->input(1); const Tensor& max_input_tensor = context->input(2); OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_input_tensor.shape()), errors::InvalidArgument( \"min_input shape must be rank 0 but is rank \",                    min_input_tensor.dims(), \", received shape: \", min_input_tensor.shape())); OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_input_tensor.shape()), errors::InvalidArgument( \"max_input shape must be rank 0 but is rank \",                    max_input_tensor.dims(), \", received shape: \", max_input_tensor.shape())); const float min_input = context->input(1).scalar<float>()(); const float max_input = context->input(2).scalar<float>()(); OP_REQUIRES(context, params.depth_window == 1, errors::Unimplemented(\"Non-spatial pooling is not \"",
    "method_name": "class QuantizedAvgPoolingOp : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/quantized_pooling_ops.cc",
    "code1": "      : MaxPoolingOp<Device, T>(context) {} void Compute(OpKernelContext* context) override { auto min_input_tensor = context->input(1); auto max_input_tensor = context->input(2); OP_REQUIRES( context, min_input_tensor.NumElements() == 1, errors::InvalidArgument( \"min_input must be a scalar float value, got tensor with shape \",            min_input_tensor.shape())); OP_REQUIRES( context, max_input_tensor.NumElements() == 1, errors::InvalidArgument( \"max_input must be a scalar float value, got tensor with shape \",            max_input_tensor.shape())); const float min_input = context->input(1).flat<float>()(0); const float max_input = context->input(2).flat<float>()(0);    MaxPoolingOp<Device, T>::Compute(context);    Tensor* output_min = nullptr; OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));",
    "code2": "      : MaxPoolingOp<Device, T>(context) {} void Compute(OpKernelContext* context) override { const Tensor& min_input_tensor = context->input(1); const Tensor& max_input_tensor = context->input(2); OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_input_tensor.shape()),  errors::InvalidArgument(  \"min_input shape must be rank 0 but is rank \",         min_input_tensor.dims(),  \", received shape: \", min_input_tensor.shape())); OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_input_tensor.shape()),  errors::InvalidArgument(  \"max_input shape must be rank 0 but is rank \",         max_input_tensor.dims(),  \", received shape: \", max_input_tensor.shape())); const float min_input = context->input(1).scalar<float>()(); const float max_input = context->input(2).scalar<float>()();    MaxPoolingOp<Device, T>::Compute(context);    Tensor* output_min = nullptr; OP_REQUIRES_OK(context, context->allocate_output(1, {}, &output_min));",
    "method_name": "class QuantizedMaxPoolingOp : public MaxPoolingOp<Device, T> {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57341,
  "title": "r2.7 cherry-pick: 37e64539cd2 \"Fix overflow issue\"",
  "tags": [],
  "closed_time": "2022-08-19T22:05:05Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/ops/math_ops.cc",
    "code1": "    return errors::InvalidArgument(\"Requires delta != 0\");  }  auto size = (std::is_integral<T>::value                   ? ((Eigen::numext::abs(limit - start) +                       Eigen::numext::abs(delta) - T(1)) /                      Eigen::numext::abs(delta))                   : (Eigen::numext::ceil(                         Eigen::numext::abs((limit - start) / delta))));  // Undefined behaviour if size will not fit into int64_t  if (size > std::numeric_limits<int64_t>::max()) {    return errors::InvalidArgument(\"Requires ((limit - start) / delta) <= \",                                   std::numeric_limits<int64_t>::max());  }  c->set_output(0, c->Vector(static_cast<int64_t>(size)));",
    "code2": "    return errors::InvalidArgument(\"Requires delta != 0\");  }  int64_t size;  if (std::is_integral<T>::value) {    size = Eigen::divup(static_cast<int64_t>(Eigen::numext::abs(limit - start)),                        static_cast<int64_t>(Eigen::numext::abs(delta)));  } else {    auto size_auto =        Eigen::numext::ceil(Eigen::numext::abs((limit - start) / delta));    if (size_auto > std::numeric_limits<int64_t>::max()) {      return errors::InvalidArgument(\"Requires ((limit - start) / delta) <= \",                                     std::numeric_limits<int64_t>::max());    }    size = static_cast<int64_t>(size_auto);  }  c->set_output(0, c->Vector(static_cast<int64_t>(size)));",
    "method_name": "Status RangeSize(const Tensor* start_t, const Tensor* limit_t,"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57340,
  "title": "r2.8 cherry-pick: 37e64539cd2 \"Fix overflow issue\"",
  "tags": [],
  "closed_time": "2022-08-19T22:05:42Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/ops/math_ops.cc",
    "code1": "    return errors::InvalidArgument(\"Requires delta != 0\");  }  auto size = (std::is_integral<T>::value                   ? ((Eigen::numext::abs(limit - start) +                       Eigen::numext::abs(delta) - T(1)) /                      Eigen::numext::abs(delta))                   : (Eigen::numext::ceil(                         Eigen::numext::abs((limit - start) / delta))));  // Undefined behaviour if size will not fit into int64_t  if (size > std::numeric_limits<int64_t>::max()) {    return errors::InvalidArgument(\"Requires ((limit - start) / delta) <= \",                                   std::numeric_limits<int64_t>::max());  }  c->set_output(0, c->Vector(static_cast<int64_t>(size)));",
    "code2": "    return errors::InvalidArgument(\"Requires delta != 0\");  }  int64_t size;  if (std::is_integral<T>::value) {    size = Eigen::divup(static_cast<int64_t>(Eigen::numext::abs(limit - start)),                        static_cast<int64_t>(Eigen::numext::abs(delta)));  } else {    auto size_auto =        Eigen::numext::ceil(Eigen::numext::abs((limit - start) / delta));    if (size_auto > std::numeric_limits<int64_t>::max()) {      return errors::InvalidArgument(\"Requires ((limit - start) / delta) <= \",                                     std::numeric_limits<int64_t>::max());    }    size = static_cast<int64_t>(size_auto);  }  c->set_output(0, c->Vector(static_cast<int64_t>(size)));",
    "method_name": "Status RangeSize(const Tensor* start_t, const Tensor* limit_t,"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57339,
  "title": "r2.9 cherry-pick: 37e64539cd2 \"Fix overflow issue\"",
  "tags": [],
  "closed_time": "2022-08-19T22:05:43Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/ops/math_ops.cc",
    "code1": " return errors::InvalidArgument(\"Requires delta != 0\");  } auto size = (std::is_integral<T>::value                   ? ((Eigen::numext::abs(limit - start) + Eigen::numext::abs(delta) - T(1)) / Eigen::numext::abs(delta))                   : (Eigen::numext::ceil( Eigen::numext::abs((limit - start) / delta)))); // Undefined behaviour if size will not fit into int64_t if (size > std::numeric_limits<int64_t>::max()) { return errors::InvalidArgument(\"Requires ((limit - start) / delta) <= \",                                   std::numeric_limits<int64_t>::max());  }  c->set_output(0, c->Vector(static_cast<int64_t>(size)));",
    "code2": " return errors::InvalidArgument(\"Requires delta != 0\");  } int64_t size; if (std::is_integral<T>::value) {    size = Eigen::divup(static_cast<int64_t>(Eigen::numext::abs(limit - start)), static_cast<int64_t>(Eigen::numext::abs(delta)));  } else { auto size_auto = Eigen::numext::ceil(Eigen::numext::abs((limit - start) / delta)); if (size_auto > std::numeric_limits<int64_t>::max()) { return errors::InvalidArgument(\"Requires ((limit - start) / delta) <= \",                                     std::numeric_limits<int64_t>::max());    }    size = static_cast<int64_t>(size_auto);  }  c->set_output(0, c->Vector(static_cast<int64_t>(size)));",
    "method_name": "Status RangeSize(const Tensor* start_t, const Tensor* limit_t,"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57335,
  "title": "r2.8 cherry-pick: 1cf45b831ee \"[tfg][functiondef_import] Emit error on empty function attributes\"",
  "tags": [],
  "closed_time": "2022-08-19T21:11:46Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/ir/importexport/functiondef_import.cc",
    "code1": "  // Import the function attributes with a `tf.` prefix to match the current  // infrastructure expectations.  for (const auto& namedAttr : func.attr()) {    const std::string& name = \"tf.\" + namedAttr.first;    const AttrValue& tf_attr = namedAttr.second;    TF_ASSIGN_OR_RETURN(Attribute attr,",
    "code2": "  // Import the function attributes with a `tf.` prefix to match the current  // infrastructure expectations.  for (const auto& namedAttr : func.attr()) {    if (namedAttr.first.empty())      return InvalidArgument(\"Invalid function attribute name\");    const std::string& name = \"tf.\" + namedAttr.first;    const AttrValue& tf_attr = namedAttr.second;    TF_ASSIGN_OR_RETURN(Attribute attr,",
    "method_name": "Status ImportGenericFunction("
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57334,
  "title": "r2.9 cherry-pick: 1cf45b831ee \"[tfg][functiondef_import] Emit error on empty function attributes\"",
  "tags": [],
  "closed_time": "2022-08-19T21:12:00Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/ir/importexport/functiondef_import.cc",
    "code1": "  // Import the function attributes with a `tf.` prefix to match the current  // infrastructure expectations.  for (const auto& namedAttr : func.attr()) {    const std::string& name = \"tf.\" + namedAttr.first;    const AttrValue& tf_attr = namedAttr.second;    TF_ASSIGN_OR_RETURN(Attribute attr,",
    "code2": "  // Import the function attributes with a `tf.` prefix to match the current  // infrastructure expectations.  for (const auto& namedAttr : func.attr()) {    if (namedAttr.first.empty())      return InvalidArgument(\"Invalid function attribute name\");    const std::string& name = \"tf.\" + namedAttr.first;    const AttrValue& tf_attr = namedAttr.second;    TF_ASSIGN_OR_RETURN(Attribute attr,",
    "method_name": "Status ImportGenericFunction("
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57333,
  "title": "r2.8 cherry-pick: aed36912609 \"Check correct input/output scalar types for LinearAlgebraOp.\"",
  "tags": [],
  "closed_time": "2022-08-19T21:11:08Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/linalg/linalg_ops_common.cc",
    "code1": "#include \"tensorflow/core/kernels/linalg/linalg_ops_common.h\"#include <utility>#include \"third_party/eigen3/Eigen/Core\"#include \"tensorflow/core/framework/device_base.h\"#include \"tensorflow/core/framework/kernel_def_builder.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/lib/core/errors.h\"#include \"tensorflow/core/platform/logging.h\"#include \"tensorflow/core/platform/types.h\"",
    "code2": "#include \"tensorflow/core/kernels/linalg/linalg_ops_common.h\"#include <initializer_list>#include <utility>#include \"third_party/eigen3/Eigen/Core\"#include \"tensorflow/core/framework/device_base.h\"#include \"tensorflow/core/framework/kernel_def_builder.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/framework/types.h\"#include \"tensorflow/core/lib/core/errors.h\"#include \"tensorflow/core/platform/errors.h\"#include \"tensorflow/core/platform/logging.h\"#include \"tensorflow/core/platform/types.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/linalg/linalg_ops_common.cc",
    "code1": "    input_matrix_shapes->emplace_back(        std::initializer_list<int64_t>({num_rows, num_cols}));    inputs->emplace_back(&in);  } // Have the derived class validate that the inputs are as expected. ValidateInputMatrixShapes(context, *input_matrix_shapes);",
    "code2": "    input_matrix_shapes->emplace_back(        std::initializer_list<int64_t>({num_rows, num_cols}));    inputs->emplace_back(&in); OP_REQUIRES(        context, in.dtype() == DataTypeToEnum<InputScalar>::v(), errors::InvalidArgument(\"Invalid input dtype \", in.dtype(), \" vs \",                                DataTypeToEnum<InputScalar>::v()));  } // Have the derived class validate that the inputs are as expected. ValidateInputMatrixShapes(context, *input_matrix_shapes);",
    "method_name": "void LinearAlgebraOp<InputScalar, OutputScalar>::AnalyzeInputs("
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/linalg/linalg_ops_common.cc",
    "code1": " OP_REQUIRES_OK(context, context->allocate_output(                                  output_idx, output_tensor_shape, &out));    }    outputs->emplace_back(out);  }}",
    "code2": " OP_REQUIRES_OK(context, context->allocate_output(                                  output_idx, output_tensor_shape, &out));    } OP_REQUIRES(        context, out->dtype() == DataTypeToEnum<OutputScalar>::v(), errors::InvalidArgument(\"Invalid output dtype \", out->dtype(), \" vs \",                                DataTypeToEnum<OutputScalar>::v()));    outputs->emplace_back(out);  }}",
    "method_name": "void LinearAlgebraOp<InputScalar, OutputScalar>::PrepareOutputs("
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57332,
  "title": "r2.7 cherry-pick: aed36912609 \"Check correct input/output scalar types for LinearAlgebraOp.\"",
  "tags": [],
  "closed_time": "2022-08-19T21:11:42Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/linalg/linalg_ops_common.cc",
    "code1": "#include \"tensorflow/core/kernels/linalg/linalg_ops_common.h\"#include <utility>#include \"third_party/eigen3/Eigen/Core\"#include \"tensorflow/core/framework/device_base.h\"#include \"tensorflow/core/framework/kernel_def_builder.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/lib/core/errors.h\"#include \"tensorflow/core/platform/logging.h\"#include \"tensorflow/core/platform/types.h\"",
    "code2": "#include \"tensorflow/core/kernels/linalg/linalg_ops_common.h\"#include <initializer_list>#include <utility>#include \"third_party/eigen3/Eigen/Core\"#include \"tensorflow/core/framework/device_base.h\"#include \"tensorflow/core/framework/kernel_def_builder.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/framework/types.h\"#include \"tensorflow/core/lib/core/errors.h\"#include \"tensorflow/core/platform/errors.h\"#include \"tensorflow/core/platform/logging.h\"#include \"tensorflow/core/platform/types.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/linalg/linalg_ops_common.cc",
    "code1": "    input_matrix_shapes->emplace_back(        std::initializer_list<int64_t>({num_rows, num_cols}));    inputs->emplace_back(&in);  } // Have the derived class validate that the inputs are as expected. ValidateInputMatrixShapes(context, *input_matrix_shapes);",
    "code2": "    input_matrix_shapes->emplace_back(        std::initializer_list<int64_t>({num_rows, num_cols}));    inputs->emplace_back(&in); OP_REQUIRES(        context, in.dtype() == DataTypeToEnum<InputScalar>::v(), errors::InvalidArgument(\"Invalid input dtype \", in.dtype(), \" vs \",                                DataTypeToEnum<InputScalar>::v()));  } // Have the derived class validate that the inputs are as expected. ValidateInputMatrixShapes(context, *input_matrix_shapes);",
    "method_name": "void LinearAlgebraOp<InputScalar, OutputScalar>::AnalyzeInputs("
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/linalg/linalg_ops_common.cc",
    "code1": " OP_REQUIRES_OK(context, context->allocate_output(                                  output_idx, output_tensor_shape, &out));    }    outputs->emplace_back(out);  }}",
    "code2": " OP_REQUIRES_OK(context, context->allocate_output(                                  output_idx, output_tensor_shape, &out));    } OP_REQUIRES(        context, out->dtype() == DataTypeToEnum<OutputScalar>::v(), errors::InvalidArgument(\"Invalid output dtype \", out->dtype(), \" vs \",                                DataTypeToEnum<OutputScalar>::v()));    outputs->emplace_back(out);  }}",
    "method_name": "void LinearAlgebraOp<InputScalar, OutputScalar>::PrepareOutputs("
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57331,
  "title": "r2.9 cherry-pick: aed36912609 \"Check correct input/output scalar types for LinearAlgebraOp.\"",
  "tags": [],
  "closed_time": "2022-08-19T21:11:41Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/linalg/linalg_ops_common.cc",
    "code1": "#include \"tensorflow/core/kernels/linalg/linalg_ops_common.h\"#include <utility>#include \"third_party/eigen3/Eigen/Core\"#include \"tensorflow/core/framework/device_base.h\"#include \"tensorflow/core/framework/kernel_def_builder.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/lib/core/errors.h\"#include \"tensorflow/core/platform/logging.h\"#include \"tensorflow/core/platform/types.h\"",
    "code2": "#include \"tensorflow/core/kernels/linalg/linalg_ops_common.h\"#include <initializer_list>#include <utility>#include \"third_party/eigen3/Eigen/Core\"#include \"tensorflow/core/framework/device_base.h\"#include \"tensorflow/core/framework/kernel_def_builder.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/framework/types.h\"#include \"tensorflow/core/lib/core/errors.h\"#include \"tensorflow/core/platform/errors.h\"#include \"tensorflow/core/platform/logging.h\"#include \"tensorflow/core/platform/types.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/linalg/linalg_ops_common.cc",
    "code1": "    input_matrix_shapes->emplace_back(        std::initializer_list<int64_t>({num_rows, num_cols}));    inputs->emplace_back(&in);  } // Have the derived class validate that the inputs are as expected. ValidateInputMatrixShapes(context, *input_matrix_shapes);",
    "code2": "    input_matrix_shapes->emplace_back(        std::initializer_list<int64_t>({num_rows, num_cols}));    inputs->emplace_back(&in); OP_REQUIRES(        context, in.dtype() == DataTypeToEnum<InputScalar>::v(), errors::InvalidArgument(\"Invalid input dtype \", in.dtype(), \" vs \",                                DataTypeToEnum<InputScalar>::v()));  } // Have the derived class validate that the inputs are as expected. ValidateInputMatrixShapes(context, *input_matrix_shapes);",
    "method_name": "void LinearAlgebraOp<InputScalar, OutputScalar>::AnalyzeInputs("
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/linalg/linalg_ops_common.cc",
    "code1": " OP_REQUIRES_OK(context, context->allocate_output(                                  output_idx, output_tensor_shape, &out));    }    outputs->emplace_back(out);  }}",
    "code2": " OP_REQUIRES_OK(context, context->allocate_output(                                  output_idx, output_tensor_shape, &out));    } OP_REQUIRES(        context, out->dtype() == DataTypeToEnum<OutputScalar>::v(), errors::InvalidArgument(\"Invalid output dtype \", out->dtype(), \" vs \",                                DataTypeToEnum<OutputScalar>::v()));    outputs->emplace_back(out);  }}",
    "method_name": "void LinearAlgebraOp<InputScalar, OutputScalar>::PrepareOutputs("
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57330,
  "title": "r2.7 cherry-pick: da0d65cdc12 \"Fix dtype bug in draw bounding boxes.\"",
  "tags": [],
  "closed_time": "2022-08-19T21:02:15Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/image/draw_bounding_box_op.cc",
    "code1": "    for (int64_t b = 0; b < batch_size; ++b) {      const int64_t num_boxes = boxes.dim_size(1);      const auto tboxes = boxes.tensor<T, 3>();      for (int64_t bb = 0; bb < num_boxes; ++bb) {        int64_t color_index = bb % color_table.size();        const int64_t min_box_row =",
    "code2": "    for (int64_t b = 0; b < batch_size; ++b) {      const int64_t num_boxes = boxes.dim_size(1);      const auto tboxes = boxes.tensor<float, 3>();      for (int64_t bb = 0; bb < num_boxes; ++bb) {        int64_t color_index = bb % color_table.size();        const int64_t min_box_row =",
    "method_name": "class DrawBoundingBoxesOp : public OpKernel {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57329,
  "title": "r2.8 cherry-pick: da0d65cdc12 \"Fix dtype bug in draw bounding boxes.\"",
  "tags": [],
  "closed_time": "2022-08-19T21:01:46Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/image/draw_bounding_box_op.cc",
    "code1": "    for (int64_t b = 0; b < batch_size; ++b) {      const int64_t num_boxes = boxes.dim_size(1);      const auto tboxes = boxes.tensor<T, 3>();      for (int64_t bb = 0; bb < num_boxes; ++bb) {        int64_t color_index = bb % color_table.size();        const int64_t min_box_row =",
    "code2": "    for (int64_t b = 0; b < batch_size; ++b) {      const int64_t num_boxes = boxes.dim_size(1);      const auto tboxes = boxes.tensor<float, 3>();      for (int64_t bb = 0; bb < num_boxes; ++bb) {        int64_t color_index = bb % color_table.size();        const int64_t min_box_row =",
    "method_name": "class DrawBoundingBoxesOp : public OpKernel {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57328,
  "title": "r2.9 cherry-pick: da0d65cdc12 \"Fix dtype bug in draw bounding boxes.\"",
  "tags": [],
  "closed_time": "2022-08-19T21:02:00Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/image/draw_bounding_box_op.cc",
    "code1": "    for (int64_t b = 0; b < batch_size; ++b) {      const int64_t num_boxes = boxes.dim_size(1);      const auto tboxes = boxes.tensor<T, 3>();      for (int64_t bb = 0; bb < num_boxes; ++bb) {        int64_t color_index = bb % color_table.size();        const int64_t min_box_row =",
    "code2": "    for (int64_t b = 0; b < batch_size; ++b) {      const int64_t num_boxes = boxes.dim_size(1);      const auto tboxes = boxes.tensor<float, 3>();      for (int64_t bb = 0; bb < num_boxes; ++bb) {        int64_t color_index = bb % color_table.size();        const int64_t min_box_row =",
    "method_name": "class DrawBoundingBoxesOp : public OpKernel {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57298,
  "title": "r2.9 cherry-pick: 27a65a43cf7 \"Fix GPU/CPU Conv2DBackpropInputOp check error.\"",
  "tags": [],
  "closed_time": "2022-08-19T20:10:44Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57297,
  "title": "r2.8 cherry-pick: 27a65a43cf7 \"Fix GPU/CPU Conv2DBackpropInputOp check error.\"",
  "tags": [],
  "closed_time": "2022-08-19T20:10:30Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57296,
  "title": "r2.7 cherry-pick: 27a65a43cf7 \"Fix GPU/CPU Conv2DBackpropInputOp check error.\"",
  "tags": [],
  "closed_time": "2022-08-19T20:10:17Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57292,
  "title": "r2.8 cherry-pick: c55b476aa0e \"Fix empty batch issue in svd.\"",
  "tags": [],
  "closed_time": "2022-08-19T20:05:54Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/linalg/svd_op_gpu.cu.cc",
    "code1": "    OP_REQUIRES_OK_ASYNC(context, context->allocate_output(2, shapeV, &outputV),                         done);    if (n == 0 || m == 0) {      if (n == m || !compute_uv_ || !full_matrices_) {        // S, U, and V are all empty. Nothing to do.",
    "code2": "    OP_REQUIRES_OK_ASYNC(context, context->allocate_output(2, shapeV, &outputV),                         done);    // If there are zero batches, we are done.    if (shapeRaw.num_elements() == 0) {      done();      return;    }    if (n == 0 || m == 0) {      if (n == m || !compute_uv_ || !full_matrices_) {        // S, U, and V are all empty. Nothing to do.",
    "method_name": "class SvdOpGpu : public AsyncOpKernel {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57291,
  "title": "r2.7 cherry-pick: c55b476aa0e \"Fix empty batch issue in svd.\"",
  "tags": [],
  "closed_time": "2022-08-19T20:05:11Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/linalg/svd_op_gpu.cu.cc",
    "code1": " OP_REQUIRES_OK_ASYNC(context, context->allocate_output(2, shapeV, &outputV),                         done); if (n == 0 || m == 0) { if (n == m || !compute_uv_ || !full_matrices_) { // S, U, and V are all empty. Nothing to do.",
    "code2": " OP_REQUIRES_OK_ASYNC(context, context->allocate_output(2, shapeV, &outputV),                         done); // If there are zero batches, we are done. if (shapeRaw.num_elements() == 0) { done(); return;    } if (n == 0 || m == 0) { if (n == m || !compute_uv_ || !full_matrices_) { // S, U, and V are all empty. Nothing to do.",
    "method_name": "class SvdOpGpu : public AsyncOpKernel {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57290,
  "title": "r2.9 cherry-pick: c55b476aa0e \"Fix empty batch issue in svd.\"",
  "tags": [],
  "closed_time": "2022-08-19T20:04:55Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/linalg/svd_op_gpu.cu.cc",
    "code1": " OP_REQUIRES_OK_ASYNC(context, context->allocate_output(2, shapeV, &outputV),                         done); if (n == 0 || m == 0) { if (n == m || !compute_uv_ || !full_matrices_) { // S, U, and V are all empty. Nothing to do.",
    "code2": " OP_REQUIRES_OK_ASYNC(context, context->allocate_output(2, shapeV, &outputV),                         done); // If there are zero batches, we are done. if (shapeRaw.num_elements() == 0) { done(); return;    } if (n == 0 || m == 0) { if (n == m || !compute_uv_ || !full_matrices_) { // S, U, and V are all empty. Nothing to do.",
    "method_name": "class SvdOpGpu : public AsyncOpKernel {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57283,
  "title": "r2.8 cherry-pick: 72180be0344 \"Fix tensor shape dtype bug in parameterized_truncated_normal.\"",
  "tags": [],
  "closed_time": "2022-08-19T20:15:40Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/parameterized_truncated_normal_op.cc",
    "code1": "#include \"tensorflow/core/framework/register_types.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/kernels/stateless_random_ops.h\"#include \"tensorflow/core/lib/random/random_distributions.h\"#include \"tensorflow/core/platform/logging.h\"",
    "code2": "#include \"tensorflow/core/framework/register_types.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/framework/tensor_util.h\"#include \"tensorflow/core/kernels/stateless_random_ops.h\"#include \"tensorflow/core/lib/random/random_distributions.h\"#include \"tensorflow/core/platform/logging.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/parameterized_truncated_normal_op.cc",
    "code1": "    OP_REQUIRES(ctx, shape_tensor.NumElements() > 0,                errors::InvalidArgument(\"Shape tensor must not be empty, got \",                                        shape_tensor.DebugString()));    int32_t num_batches = shape_tensor.flat<int32>()(0);    int32_t samples_per_batch = 1;    const int32_t num_dims = shape_tensor.dim_size(0);    for (int32_t i = 1; i < num_dims; i++) {      samples_per_batch *= shape_tensor.flat<int32>()(i);    }    const int32_t num_elements = num_batches * samples_per_batch;    // Allocate the output before fudging num_batches and samples_per_batch.    auto shape_vec = shape_tensor.flat<int32>();    TensorShape tensor_shape;    OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(                            shape_vec.data(), shape_vec.size(), &tensor_shape));    Tensor* samples_tensor;    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, tensor_shape, &samples_tensor));",
    "code2": "    OP_REQUIRES(ctx, shape_tensor.NumElements() > 0,                errors::InvalidArgument(\"Shape tensor must not be empty, got \",                                        shape_tensor.DebugString()));    TensorShape tensor_shape;    OP_REQUIRES_OK(ctx, tensor::MakeShape(shape_tensor, &tensor_shape));    int32_t num_batches = tensor_shape.dim_size(0);    int32_t samples_per_batch = 1;    const int32_t num_dims = tensor_shape.dims();    for (int32_t i = 1; i < num_dims; i++) {      samples_per_batch *= tensor_shape.dim_size(i);    }    const int32_t num_elements = num_batches * samples_per_batch;    // Allocate the output before fudging num_batches and samples_per_batch.    Tensor* samples_tensor;    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, tensor_shape, &samples_tensor));",
    "method_name": "class ParameterizedTruncatedNormalOp : public OpKernel {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57282,
  "title": "r2.7 cherry-pick: 72180be0344 \"Fix tensor shape dtype bug in parameterized_truncated_normal.\"",
  "tags": [],
  "closed_time": "2022-08-19T20:15:38Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/parameterized_truncated_normal_op.cc",
    "code1": "#include \"tensorflow/core/framework/register_types.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/kernels/stateless_random_ops.h\"#include \"tensorflow/core/lib/random/random_distributions.h\"#include \"tensorflow/core/platform/logging.h\"",
    "code2": "#include \"tensorflow/core/framework/register_types.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/framework/tensor_util.h\"#include \"tensorflow/core/kernels/stateless_random_ops.h\"#include \"tensorflow/core/lib/random/random_distributions.h\"#include \"tensorflow/core/platform/logging.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/parameterized_truncated_normal_op.cc",
    "code1": " OP_REQUIRES(ctx, shape_tensor.NumElements() > 0, errors::InvalidArgument(\"Shape tensor must not be empty, got \",                                        shape_tensor.DebugString())); int32_t num_batches = shape_tensor.flat<int32>()(0); int32_t samples_per_batch = 1; const int32_t num_dims = shape_tensor.dim_size(0); for (int32_t i = 1; i < num_dims; i++) {      samples_per_batch *= shape_tensor.flat<int32>()(i);    } const int32_t num_elements = num_batches * samples_per_batch; // Allocate the output before fudging num_batches and samples_per_batch. auto shape_vec = shape_tensor.flat<int32>();    TensorShape tensor_shape; OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(                            shape_vec.data(), shape_vec.size(), &tensor_shape));    Tensor* samples_tensor; OP_REQUIRES_OK(ctx, ctx->allocate_output(0, tensor_shape, &samples_tensor));",
    "code2": " OP_REQUIRES(ctx, shape_tensor.NumElements() > 0, errors::InvalidArgument(\"Shape tensor must not be empty, got \",                                        shape_tensor.DebugString()));    TensorShape tensor_shape; OP_REQUIRES_OK(ctx, tensor::MakeShape(shape_tensor, &tensor_shape)); int32_t num_batches = tensor_shape.dim_size(0); int32_t samples_per_batch = 1; const int32_t num_dims = tensor_shape.dims(); for (int32_t i = 1; i < num_dims; i++) {      samples_per_batch *= tensor_shape.dim_size(i);    } const int32_t num_elements = num_batches * samples_per_batch; // Allocate the output before fudging num_batches and samples_per_batch.    Tensor* samples_tensor; OP_REQUIRES_OK(ctx, ctx->allocate_output(0, tensor_shape, &samples_tensor));",
    "method_name": "class ParameterizedTruncatedNormalOp : public OpKernel {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57281,
  "title": "r2.9 cherry-pick: 72180be0344 \"Fix tensor shape dtype bug in parameterized_truncated_normal.\"",
  "tags": [],
  "closed_time": "2022-08-19T20:15:30Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/parameterized_truncated_normal_op.cc",
    "code1": "#include \"tensorflow/core/framework/register_types.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/kernels/stateless_random_ops.h\"#include \"tensorflow/core/lib/random/random_distributions.h\"#include \"tensorflow/core/platform/logging.h\"",
    "code2": "#include \"tensorflow/core/framework/register_types.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/framework/tensor_util.h\"#include \"tensorflow/core/kernels/stateless_random_ops.h\"#include \"tensorflow/core/lib/random/random_distributions.h\"#include \"tensorflow/core/platform/logging.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/parameterized_truncated_normal_op.cc",
    "code1": "    OP_REQUIRES(ctx, shape_tensor.NumElements() > 0,                errors::InvalidArgument(\"Shape tensor must not be empty, got \",                                        shape_tensor.DebugString()));    int32_t num_batches = shape_tensor.flat<int32>()(0);    int32_t samples_per_batch = 1;    const int32_t num_dims = shape_tensor.dim_size(0);    for (int32_t i = 1; i < num_dims; i++) {      samples_per_batch *= shape_tensor.flat<int32>()(i);    }    const int32_t num_elements = num_batches * samples_per_batch;    // Allocate the output before fudging num_batches and samples_per_batch.    auto shape_vec = shape_tensor.flat<int32>();    TensorShape tensor_shape;    OP_REQUIRES_OK(ctx, TensorShapeUtils::MakeShape(                            shape_vec.data(), shape_vec.size(), &tensor_shape));    Tensor* samples_tensor;    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, tensor_shape, &samples_tensor));",
    "code2": "    OP_REQUIRES(ctx, shape_tensor.NumElements() > 0,                errors::InvalidArgument(\"Shape tensor must not be empty, got \",                                        shape_tensor.DebugString()));    TensorShape tensor_shape;    OP_REQUIRES_OK(ctx, tensor::MakeShape(shape_tensor, &tensor_shape));    int32_t num_batches = tensor_shape.dim_size(0);    int32_t samples_per_batch = 1;    const int32_t num_dims = tensor_shape.dims();    for (int32_t i = 1; i < num_dims; i++) {      samples_per_batch *= tensor_shape.dim_size(i);    }    const int32_t num_elements = num_batches * samples_per_batch;    // Allocate the output before fudging num_batches and samples_per_batch.    Tensor* samples_tensor;    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, tensor_shape, &samples_tensor));",
    "method_name": "class ParameterizedTruncatedNormalOp : public OpKernel {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57267,
  "title": "r2.8 cherry-pick: 785d67a78a1 \"Fix quantize ops input validation issues.\"",
  "tags": [],
  "closed_time": "2022-08-19T19:47:33Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/fake_quant_ops.cc",
    "code1": "// Above is the related header but clang tidy doesn't recognize it.#include \"tensorflow/core/framework/numeric_op.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/lib/core/errors.h\"#include \"tensorflow/core/lib/monitoring/gauge.h\"#include \"tensorflow/core/platform/protobuf.h\"",
    "code2": "// Above is the related header but clang tidy doesn't recognize it.#include \"tensorflow/core/framework/numeric_op.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/lib/core/errors.h\"#include \"tensorflow/core/lib/monitoring/gauge.h\"#include \"tensorflow/core/platform/protobuf.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/fake_quant_ops.cc",
    "code1": "    const Tensor& min = context->input(1);    const Tensor& max = context->input(2);    Tensor* output;    OP_REQUIRES_OK(context,                   context->allocate_output(0, input.shape(), &output));",
    "code2": "    const Tensor& min = context->input(1);    const Tensor& max = context->input(2);    OP_REQUIRES(        context, TensorShapeUtils::IsScalar(min.shape()),        InvalidArgument(\"`min` must be rank 0 but is rank \", min.dims()));    OP_REQUIRES(        context, TensorShapeUtils::IsScalar(max.shape()),        InvalidArgument(\"`max` must be rank 0 but is rank \", max.dims()));    Tensor* output;    OP_REQUIRES_OK(context,                   context->allocate_output(0, input.shape(), &output));",
    "method_name": "class FakeQuantWithMinMaxVarsOp : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/fake_quant_ops.cc",
    "code1": "    const Tensor& input = context->input(0);    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.    const Tensor& min = context->input(1);    OP_REQUIRES(context, min.dim_size(0) == depth,                InvalidArgument(\"min has incorrect size, expected \", depth,                                \" was \", min.dim_size(0)));    const Tensor& max = context->input(2);    OP_REQUIRES(context, max.dim_size(0) == depth,                InvalidArgument(\"max has incorrect size, expected \", depth,                                \" was \", max.dim_size(0)));",
    "code2": "    const Tensor& input = context->input(0);    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.    const Tensor& min = context->input(1);    const Tensor& max = context->input(2);    OP_REQUIRES(        context, TensorShapeUtils::IsVector(min.shape()),        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));    OP_REQUIRES(context, min.dim_size(0) == depth,                InvalidArgument(\"min has incorrect size, expected \", depth,                                \" was \", min.dim_size(0)));    OP_REQUIRES(        context, TensorShapeUtils::IsVector(max.shape()),        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));    OP_REQUIRES(context, max.dim_size(0) == depth,                InvalidArgument(\"max has incorrect size, expected \", depth,                                \" was \", max.dim_size(0)));",
    "method_name": "class FakeQuantWithMinMaxVarsPerChannelOp : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/quantized_bias_add_op.cc",
    "code1": "#include \"tensorflow/core/framework/numeric_op.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/kernels/meta_support.h\"#include \"tensorflow/core/kernels/ops_util.h\"#include \"tensorflow/core/kernels/quantization_utils.h\"",
    "code2": "#include \"tensorflow/core/framework/numeric_op.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/kernels/meta_support.h\"#include \"tensorflow/core/kernels/ops_util.h\"#include \"tensorflow/core/kernels/quantization_utils.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/quantized_bias_add_op.cc",
    "code1": " void Compute(OpKernelContext* context) override { const Tensor& input = context->input(0); const Tensor& bias = context->input(1); const float input_min = context->input(2).flat<float>()(0); const float input_max = context->input(3).flat<float>()(0); const float bias_min = context->input(4).flat<float>()(0); const float bias_max = context->input(5).flat<float>()(0); OP_REQUIRES(context, TensorShapeUtils::IsMatrixOrHigher(input.shape()), errors::InvalidArgument(\"Input tensor must be at least 2D: \",",
    "code2": " void Compute(OpKernelContext* context) override { const Tensor& input = context->input(0); const Tensor& bias = context->input(1); const Tensor& min_input = context->input(2); const Tensor& max_input = context->input(3); const Tensor& min_bias = context->input(4); const Tensor& max_bias = context->input(5); OP_REQUIRES(        context, TensorShapeUtils::IsScalar(min_input.shape()), errors::InvalidArgument(\"`min_input` must be rank 0 but is rank \",                                min_input.dims())); OP_REQUIRES(        context, TensorShapeUtils::IsScalar(max_input.shape()), errors::InvalidArgument(\"`max_input` must be rank 0 but is rank \",                                max_input.dims())); OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_bias.shape()), errors::InvalidArgument( \"`min_bias` must be rank 0 but is rank \", min_bias.dims())); OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_bias.shape()), errors::InvalidArgument( \"`max_bias` must be rank 0 but is rank \", max_bias.dims())); const float input_min = min_input.flat<float>()(0); const float input_max = max_input.flat<float>()(0); const float bias_min = min_bias.flat<float>()(0); const float bias_max = max_bias.flat<float>()(0); OP_REQUIRES(context, TensorShapeUtils::IsMatrixOrHigher(input.shape()), errors::InvalidArgument(\"Input tensor must be at least 2D: \",",
    "method_name": "class QuantizedBiasAddOp : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/quantized_instance_norm.cc",
    "code1": "#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/register_types.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/kernels/quantization_utils.h\"#ifdef USE_NEON",
    "code2": "#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/register_types.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/kernels/quantization_utils.h\"#ifdef USE_NEON",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/quantized_instance_norm.cc",
    "code1": "  void Compute(OpKernelContext* context) override {    const Tensor& input = context->input(0);    float input_min = context->input(1).flat<float>()(0);    float input_max = context->input(2).flat<float>()(0);    float input_scale = (input_max - input_min) / 255.0f;    OP_REQUIRES(context, input_min < input_max,",
    "code2": "  void Compute(OpKernelContext* context) override {    const Tensor& input = context->input(0);    const Tensor& x_min = context->input(1);    const Tensor& x_max = context->input(2);    OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_min.shape()),                errors::InvalidArgument(\"`x_min` must be rank 0 but is rank \",                                        x_min.dims()));    OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_max.shape()),                errors::InvalidArgument(\"`x_max` must be rank 0 but is rank \",                                        x_max.dims()));    float input_min = x_min.scalar<float>()();    float input_max = x_max.scalar<float>()();    float input_scale = (input_max - input_min) / 255.0f;    OP_REQUIRES(context, input_min < input_max,",
    "method_name": "class QuantizedInstanceNorm : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/requantize.cc",
    "code1": "#define EIGEN_USE_THREADS#include <math.h>#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"#include \"tensorflow/core/framework/op.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/type_traits.h\"#include \"tensorflow/core/framework/types.h\"#include \"tensorflow/core/kernels/meta_support.h\"",
    "code2": "#define EIGEN_USE_THREADS#include <math.h>#include \"tensorflow/core/framework/op.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/framework/type_traits.h\"#include \"tensorflow/core/framework/types.h\"#include \"tensorflow/core/kernels/meta_support.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/requantize.cc",
    "code1": " void Compute(OpKernelContext* ctx) override { const Tensor& input = ctx->input(0); const float input_min_float = ctx->input(1).flat<float>()(0); const float input_max_float = ctx->input(2).flat<float>()(0); const float requested_output_min_float = ctx->input(3).flat<float>()(0); const float requested_output_max_float = ctx->input(4).flat<float>()(0);    Tensor* output = nullptr; OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));",
    "code2": " void Compute(OpKernelContext* ctx) override { const Tensor& input = ctx->input(0); const Tensor& input_min = ctx->input(1); const Tensor& input_max = ctx->input(2); const Tensor& requested_output_min = ctx->input(3); const Tensor& requested_output_max = ctx->input(4); OP_REQUIRES(        ctx, TensorShapeUtils::IsScalar(input_min.shape()), errors::InvalidArgument(\"`input_min` must be rank 0 but is rank \",                                input_min.dims())); OP_REQUIRES(        ctx, TensorShapeUtils::IsScalar(input_max.shape()), errors::InvalidArgument(\"`input_max` must be rank 0 but is rank \",                                input_max.dims())); OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_min.shape()), errors::InvalidArgument( \"`requested_output_min` must be rank 0 but is rank \",                    requested_output_min.dims())); OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_max.shape()), errors::InvalidArgument( \"`requested_output_max` must be rank 0 but is rank \",                    requested_output_max.dims())); const float input_min_float = input_min.flat<float>()(0); const float input_max_float = input_max.flat<float>()(0); const float requested_output_min_float =        requested_output_min.flat<float>()(0); const float requested_output_max_float =        requested_output_max.flat<float>()(0);    Tensor* output = nullptr; OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));",
    "method_name": "class RequantizeOp : public OpKernel {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57266,
  "title": "r2.7 cherry-pick: 785d67a78a1 \"Fix quantize ops input validation issues.\"",
  "tags": [],
  "closed_time": "2022-08-19T19:47:39Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/fake_quant_ops.cc",
    "code1": "// Above is the related header but clang tidy doesn't recognize it.#include \"tensorflow/core/framework/numeric_op.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/lib/core/errors.h\"#include \"tensorflow/core/lib/monitoring/gauge.h\"#include \"tensorflow/core/platform/protobuf.h\"",
    "code2": "// Above is the related header but clang tidy doesn't recognize it.#include \"tensorflow/core/framework/numeric_op.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/lib/core/errors.h\"#include \"tensorflow/core/lib/monitoring/gauge.h\"#include \"tensorflow/core/platform/protobuf.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/fake_quant_ops.cc",
    "code1": "    const Tensor& min = context->input(1);    const Tensor& max = context->input(2);    Tensor* output;    OP_REQUIRES_OK(context,                   context->allocate_output(0, input.shape(), &output));",
    "code2": "    const Tensor& min = context->input(1);    const Tensor& max = context->input(2);    OP_REQUIRES(        context, TensorShapeUtils::IsScalar(min.shape()),        InvalidArgument(\"`min` must be rank 0 but is rank \", min.dims()));    OP_REQUIRES(        context, TensorShapeUtils::IsScalar(max.shape()),        InvalidArgument(\"`max` must be rank 0 but is rank \", max.dims()));    Tensor* output;    OP_REQUIRES_OK(context,                   context->allocate_output(0, input.shape(), &output));",
    "method_name": "class FakeQuantWithMinMaxVarsOp : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/fake_quant_ops.cc",
    "code1": "    const Tensor& input = context->input(0);    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.    const Tensor& min = context->input(1);    OP_REQUIRES(context, min.dim_size(0) == depth,                InvalidArgument(\"min has incorrect size, expected \", depth,                                \" was \", min.dim_size(0)));    const Tensor& max = context->input(2);    OP_REQUIRES(context, max.dim_size(0) == depth,                InvalidArgument(\"max has incorrect size, expected \", depth,                                \" was \", max.dim_size(0)));",
    "code2": "    const Tensor& input = context->input(0);    const int depth = input.dim_size(input.dims() - 1);  // last dimension size.    const Tensor& min = context->input(1);    const Tensor& max = context->input(2);    OP_REQUIRES(        context, TensorShapeUtils::IsVector(min.shape()),        InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims()));    OP_REQUIRES(context, min.dim_size(0) == depth,                InvalidArgument(\"min has incorrect size, expected \", depth,                                \" was \", min.dim_size(0)));    OP_REQUIRES(        context, TensorShapeUtils::IsVector(max.shape()),        InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims()));    OP_REQUIRES(context, max.dim_size(0) == depth,                InvalidArgument(\"max has incorrect size, expected \", depth,                                \" was \", max.dim_size(0)));",
    "method_name": "class FakeQuantWithMinMaxVarsPerChannelOp : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/quantized_bias_add_op.cc",
    "code1": "#include \"tensorflow/core/framework/numeric_op.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/kernels/meta_support.h\"#include \"tensorflow/core/kernels/ops_util.h\"#include \"tensorflow/core/kernels/quantization_utils.h\"",
    "code2": "#include \"tensorflow/core/framework/numeric_op.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/kernels/meta_support.h\"#include \"tensorflow/core/kernels/ops_util.h\"#include \"tensorflow/core/kernels/quantization_utils.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/quantized_bias_add_op.cc",
    "code1": "  void Compute(OpKernelContext* context) override {    const Tensor& input = context->input(0);    const Tensor& bias = context->input(1);    const float input_min = context->input(2).flat<float>()(0);    const float input_max = context->input(3).flat<float>()(0);    const float bias_min = context->input(4).flat<float>()(0);    const float bias_max = context->input(5).flat<float>()(0);    OP_REQUIRES(context, TensorShapeUtils::IsMatrixOrHigher(input.shape()),                errors::InvalidArgument(\"Input tensor must be at least 2D: \",",
    "code2": "  void Compute(OpKernelContext* context) override {    const Tensor& input = context->input(0);    const Tensor& bias = context->input(1);    const Tensor& min_input = context->input(2);    const Tensor& max_input = context->input(3);    const Tensor& min_bias = context->input(4);    const Tensor& max_bias = context->input(5);    OP_REQUIRES(        context, TensorShapeUtils::IsScalar(min_input.shape()),        errors::InvalidArgument(\"`min_input` must be rank 0 but is rank \",                                min_input.dims()));    OP_REQUIRES(        context, TensorShapeUtils::IsScalar(max_input.shape()),        errors::InvalidArgument(\"`max_input` must be rank 0 but is rank \",                                max_input.dims()));    OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_bias.shape()),                errors::InvalidArgument(                    \"`min_bias` must be rank 0 but is rank \", min_bias.dims()));    OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_bias.shape()),                errors::InvalidArgument(                    \"`max_bias` must be rank 0 but is rank \", max_bias.dims()));    const float input_min = min_input.flat<float>()(0);    const float input_max = max_input.flat<float>()(0);    const float bias_min = min_bias.flat<float>()(0);    const float bias_max = max_bias.flat<float>()(0);    OP_REQUIRES(context, TensorShapeUtils::IsMatrixOrHigher(input.shape()),                errors::InvalidArgument(\"Input tensor must be at least 2D: \",",
    "method_name": "class QuantizedBiasAddOp : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/quantized_instance_norm.cc",
    "code1": "#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/register_types.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/kernels/quantization_utils.h\"#ifdef USE_NEON",
    "code2": "#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/register_types.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/kernels/quantization_utils.h\"#ifdef USE_NEON",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/quantized_instance_norm.cc",
    "code1": "  void Compute(OpKernelContext* context) override {    const Tensor& input = context->input(0);    float input_min = context->input(1).flat<float>()(0);    float input_max = context->input(2).flat<float>()(0);    float input_scale = (input_max - input_min) / 255.0f;    OP_REQUIRES(context, input_min < input_max,",
    "code2": "  void Compute(OpKernelContext* context) override {    const Tensor& input = context->input(0);    const Tensor& x_min = context->input(1);    const Tensor& x_max = context->input(2);    OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_min.shape()),                errors::InvalidArgument(\"`x_min` must be rank 0 but is rank \",                                        x_min.dims()));    OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_max.shape()),                errors::InvalidArgument(\"`x_max` must be rank 0 but is rank \",                                        x_max.dims()));    float input_min = x_min.scalar<float>()();    float input_max = x_max.scalar<float>()();    float input_scale = (input_max - input_min) / 255.0f;    OP_REQUIRES(context, input_min < input_max,",
    "method_name": "class QuantizedInstanceNorm : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/requantize.cc",
    "code1": "#define EIGEN_USE_THREADS#include <math.h>#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"#include \"tensorflow/core/framework/op.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/type_traits.h\"#include \"tensorflow/core/framework/types.h\"#include \"tensorflow/core/kernels/meta_support.h\"",
    "code2": "#define EIGEN_USE_THREADS#include <math.h>#include \"tensorflow/core/framework/op.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/framework/type_traits.h\"#include \"tensorflow/core/framework/types.h\"#include \"tensorflow/core/kernels/meta_support.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/requantize.cc",
    "code1": "  void Compute(OpKernelContext* ctx) override {    const Tensor& input = ctx->input(0);    const float input_min_float = ctx->input(1).flat<float>()(0);    const float input_max_float = ctx->input(2).flat<float>()(0);    const float requested_output_min_float = ctx->input(3).flat<float>()(0);    const float requested_output_max_float = ctx->input(4).flat<float>()(0);    Tensor* output = nullptr;    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));",
    "code2": "  void Compute(OpKernelContext* ctx) override {    const Tensor& input = ctx->input(0);    const Tensor& input_min = ctx->input(1);    const Tensor& input_max = ctx->input(2);    const Tensor& requested_output_min = ctx->input(3);    const Tensor& requested_output_max = ctx->input(4);    OP_REQUIRES(        ctx, TensorShapeUtils::IsScalar(input_min.shape()),        errors::InvalidArgument(\"`input_min` must be rank 0 but is rank \",                                input_min.dims()));    OP_REQUIRES(        ctx, TensorShapeUtils::IsScalar(input_max.shape()),        errors::InvalidArgument(\"`input_max` must be rank 0 but is rank \",                                input_max.dims()));    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_min.shape()),                errors::InvalidArgument(                    \"`requested_output_min` must be rank 0 but is rank \",                    requested_output_min.dims()));    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_max.shape()),                errors::InvalidArgument(                    \"`requested_output_max` must be rank 0 but is rank \",                    requested_output_max.dims()));    const float input_min_float = input_min.flat<float>()(0);    const float input_max_float = input_max.flat<float>()(0);    const float requested_output_min_float =        requested_output_min.flat<float>()(0);    const float requested_output_max_float =        requested_output_max.flat<float>()(0);    Tensor* output = nullptr;    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));",
    "method_name": "class RequantizeOp : public OpKernel {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57265,
  "title": "r2.9 cherry-pick: 785d67a78a1 \"Fix quantize ops input validation issues.\"",
  "tags": [],
  "closed_time": "2022-08-19T19:47:46Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/fake_quant_ops.cc",
    "code1": "// Above is the related header but clang tidy doesn't recognize it.#include \"tensorflow/core/framework/numeric_op.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/lib/core/errors.h\"#include \"tensorflow/core/lib/monitoring/gauge.h\"#include \"tensorflow/core/platform/protobuf.h\"",
    "code2": "// Above is the related header but clang tidy doesn't recognize it.#include \"tensorflow/core/framework/numeric_op.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/lib/core/errors.h\"#include \"tensorflow/core/lib/monitoring/gauge.h\"#include \"tensorflow/core/platform/protobuf.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/fake_quant_ops.cc",
    "code1": " const Tensor& min = context->input(1); const Tensor& max = context->input(2);    Tensor* output; OP_REQUIRES_OK(context,                   context->allocate_output(0, input.shape(), &output));",
    "code2": " const Tensor& min = context->input(1); const Tensor& max = context->input(2); OP_REQUIRES(        context, TensorShapeUtils::IsScalar(min.shape()), InvalidArgument(\"`min` must be rank 0 but is rank \", min.dims())); OP_REQUIRES(        context, TensorShapeUtils::IsScalar(max.shape()), InvalidArgument(\"`max` must be rank 0 but is rank \", max.dims()));    Tensor* output; OP_REQUIRES_OK(context,                   context->allocate_output(0, input.shape(), &output));",
    "method_name": "class FakeQuantWithMinMaxVarsOp : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/fake_quant_ops.cc",
    "code1": " const Tensor& input = context->input(0); const int depth = input.dim_size(input.dims() - 1);  // last dimension size. const Tensor& min = context->input(1); OP_REQUIRES(context, min.dim_size(0) == depth, InvalidArgument(\"min has incorrect size, expected \", depth, \" was \", min.dim_size(0))); const Tensor& max = context->input(2); OP_REQUIRES(context, max.dim_size(0) == depth, InvalidArgument(\"max has incorrect size, expected \", depth, \" was \", max.dim_size(0)));",
    "code2": " const Tensor& input = context->input(0); const int depth = input.dim_size(input.dims() - 1);  // last dimension size. const Tensor& min = context->input(1); const Tensor& max = context->input(2); OP_REQUIRES(        context, TensorShapeUtils::IsVector(min.shape()), InvalidArgument(\"`min` must be rank 1 but is rank \", min.dims())); OP_REQUIRES(context, min.dim_size(0) == depth, InvalidArgument(\"min has incorrect size, expected \", depth, \" was \", min.dim_size(0))); OP_REQUIRES(        context, TensorShapeUtils::IsVector(max.shape()), InvalidArgument(\"`max` must be rank 1 but is rank \", max.dims())); OP_REQUIRES(context, max.dim_size(0) == depth, InvalidArgument(\"max has incorrect size, expected \", depth, \" was \", max.dim_size(0)));",
    "method_name": "class FakeQuantWithMinMaxVarsPerChannelOp : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/quantized_bias_add_op.cc",
    "code1": "#include \"tensorflow/core/framework/numeric_op.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/kernels/meta_support.h\"#include \"tensorflow/core/kernels/ops_util.h\"#include \"tensorflow/core/kernels/quantization_utils.h\"",
    "code2": "#include \"tensorflow/core/framework/numeric_op.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/kernels/meta_support.h\"#include \"tensorflow/core/kernels/ops_util.h\"#include \"tensorflow/core/kernels/quantization_utils.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/quantized_bias_add_op.cc",
    "code1": " void Compute(OpKernelContext* context) override { const Tensor& input = context->input(0); const Tensor& bias = context->input(1); const float input_min = context->input(2).flat<float>()(0); const float input_max = context->input(3).flat<float>()(0); const float bias_min = context->input(4).flat<float>()(0); const float bias_max = context->input(5).flat<float>()(0); OP_REQUIRES(context, TensorShapeUtils::IsMatrixOrHigher(input.shape()), errors::InvalidArgument(\"Input tensor must be at least 2D: \",",
    "code2": " void Compute(OpKernelContext* context) override { const Tensor& input = context->input(0); const Tensor& bias = context->input(1); const Tensor& min_input = context->input(2); const Tensor& max_input = context->input(3); const Tensor& min_bias = context->input(4); const Tensor& max_bias = context->input(5); OP_REQUIRES(        context, TensorShapeUtils::IsScalar(min_input.shape()), errors::InvalidArgument(\"`min_input` must be rank 0 but is rank \",                                min_input.dims())); OP_REQUIRES(        context, TensorShapeUtils::IsScalar(max_input.shape()), errors::InvalidArgument(\"`max_input` must be rank 0 but is rank \",                                max_input.dims())); OP_REQUIRES(context, TensorShapeUtils::IsScalar(min_bias.shape()), errors::InvalidArgument( \"`min_bias` must be rank 0 but is rank \", min_bias.dims())); OP_REQUIRES(context, TensorShapeUtils::IsScalar(max_bias.shape()), errors::InvalidArgument( \"`max_bias` must be rank 0 but is rank \", max_bias.dims())); const float input_min = min_input.flat<float>()(0); const float input_max = max_input.flat<float>()(0); const float bias_min = min_bias.flat<float>()(0); const float bias_max = max_bias.flat<float>()(0); OP_REQUIRES(context, TensorShapeUtils::IsMatrixOrHigher(input.shape()), errors::InvalidArgument(\"Input tensor must be at least 2D: \",",
    "method_name": "class QuantizedBiasAddOp : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/quantized_instance_norm.cc",
    "code1": "#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/register_types.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/kernels/quantization_utils.h\"#ifdef USE_NEON",
    "code2": "#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/register_types.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/kernels/quantization_utils.h\"#ifdef USE_NEON",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/quantized_instance_norm.cc",
    "code1": " void Compute(OpKernelContext* context) override { const Tensor& input = context->input(0); float input_min = context->input(1).flat<float>()(0); float input_max = context->input(2).flat<float>()(0); float input_scale = (input_max - input_min) / 255.0f; OP_REQUIRES(context, input_min < input_max,",
    "code2": " void Compute(OpKernelContext* context) override { const Tensor& input = context->input(0); const Tensor& x_min = context->input(1); const Tensor& x_max = context->input(2); OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_min.shape()), errors::InvalidArgument(\"`x_min` must be rank 0 but is rank \",                                        x_min.dims())); OP_REQUIRES(context, TensorShapeUtils::IsScalar(x_max.shape()), errors::InvalidArgument(\"`x_max` must be rank 0 but is rank \",                                        x_max.dims())); float input_min = x_min.scalar<float>()(); float input_max = x_max.scalar<float>()(); float input_scale = (input_max - input_min) / 255.0f; OP_REQUIRES(context, input_min < input_max,",
    "method_name": "class QuantizedInstanceNorm : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/requantize.cc",
    "code1": "#define EIGEN_USE_THREADS#include <math.h>#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"#include \"tensorflow/core/framework/op.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/type_traits.h\"#include \"tensorflow/core/framework/types.h\"#include \"tensorflow/core/kernels/meta_support.h\"",
    "code2": "#define EIGEN_USE_THREADS#include <math.h>#include \"tensorflow/core/framework/op.h\"#include \"tensorflow/core/framework/op_kernel.h\"#include \"tensorflow/core/framework/tensor.h\"#include \"tensorflow/core/framework/tensor_shape.h\"#include \"tensorflow/core/framework/type_traits.h\"#include \"tensorflow/core/framework/types.h\"#include \"tensorflow/core/kernels/meta_support.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/requantize.cc",
    "code1": " void Compute(OpKernelContext* ctx) override { const Tensor& input = ctx->input(0); const float input_min_float = ctx->input(1).flat<float>()(0); const float input_max_float = ctx->input(2).flat<float>()(0); const float requested_output_min_float = ctx->input(3).flat<float>()(0); const float requested_output_max_float = ctx->input(4).flat<float>()(0);    Tensor* output = nullptr; OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));",
    "code2": " void Compute(OpKernelContext* ctx) override { const Tensor& input = ctx->input(0); const Tensor& input_min = ctx->input(1); const Tensor& input_max = ctx->input(2); const Tensor& requested_output_min = ctx->input(3); const Tensor& requested_output_max = ctx->input(4); OP_REQUIRES(        ctx, TensorShapeUtils::IsScalar(input_min.shape()), errors::InvalidArgument(\"`input_min` must be rank 0 but is rank \",                                input_min.dims())); OP_REQUIRES(        ctx, TensorShapeUtils::IsScalar(input_max.shape()), errors::InvalidArgument(\"`input_max` must be rank 0 but is rank \",                                input_max.dims())); OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_min.shape()), errors::InvalidArgument( \"`requested_output_min` must be rank 0 but is rank \",                    requested_output_min.dims())); OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(requested_output_max.shape()), errors::InvalidArgument( \"`requested_output_max` must be rank 0 but is rank \",                    requested_output_max.dims())); const float input_min_float = input_min.flat<float>()(0); const float input_max_float = input_max.flat<float>()(0); const float requested_output_min_float =        requested_output_min.flat<float>()(0); const float requested_output_max_float =        requested_output_max.flat<float>()(0);    Tensor* output = nullptr; OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input.shape(), &output));",
    "method_name": "class RequantizeOp : public OpKernel {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57237,
  "title": "r2.7 cherry-pick: 595a65a3e22 \"Return a TFLite error if gather_nd will result in reading invalid memory\"",
  "tags": [],
  "closed_time": "2022-08-19T17:13:41Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/lite/kernels/gather_nd.cc",
    "code1": "==============================================================================*/#include <stdint.h>#include \"tensorflow/lite/c/common.h\"#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"",
    "code2": "==============================================================================*/#include <stdint.h>#include \"tensorflow/lite/c/c_api_types.h\"#include \"tensorflow/lite/c/common.h\"#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/lite/kernels/gather_nd.cc",
    "code1": "}template <typename ParamsT, typename IndicesT>TfLiteStatus GatherNd(const TfLiteTensor* params, const TfLiteTensor* indices,                      TfLiteTensor* output) {  reference_ops::GatherNd(      GetTensorShape(params), GetTensorData<ParamsT>(params),      GetTensorShape(indices), GetTensorData<IndicesT>(indices),      GetTensorShape(output), GetTensorData<ParamsT>(output));  return kTfLiteOk;}template <typename IndicesT>",
    "code2": "}template <typename ParamsT, typename IndicesT>TfLiteStatus GatherNd(TfLiteContext* context, const TfLiteTensor* params, const TfLiteTensor* indices, TfLiteTensor* output) { const TfLiteStatus status = reference_ops::GatherNd(      GetTensorShape(params), GetTensorData<ParamsT>(params),      GetTensorShape(indices), GetTensorData<IndicesT>(indices),      GetTensorShape(output), GetTensorData<ParamsT>(output));  if (status != kTfLiteOk) {    TF_LITE_KERNEL_LOG(context, \"gather_nd index out of bounds\");  }  return status;}template <typename IndicesT>",
    "method_name": "TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/lite/kernels/gather_nd.cc",
    "code1": "  switch (params->type) {    case kTfLiteFloat32:      return GatherNd<float, IndicesT>(params, indices, output);    case kTfLiteUInt8:      return GatherNd<uint8_t, IndicesT>(params, indices, output);    case kTfLiteInt8:      return GatherNd<int8_t, IndicesT>(params, indices, output);    case kTfLiteInt16:      return GatherNd<int16_t, IndicesT>(params, indices, output);    case kTfLiteInt32:      return GatherNd<int32_t, IndicesT>(params, indices, output);    case kTfLiteInt64:      return GatherNd<int64_t, IndicesT>(params, indices, output);    case kTfLiteString:      return GatherNdString<IndicesT>(params, indices, output);    default:",
    "code2": "  switch (params->type) {    case kTfLiteFloat32:      return GatherNd<float, IndicesT>(context, params, indices, output);    case kTfLiteUInt8:      return GatherNd<uint8_t, IndicesT>(context, params, indices, output);    case kTfLiteInt8:      return GatherNd<int8_t, IndicesT>(context, params, indices, output);    case kTfLiteInt16:      return GatherNd<int16_t, IndicesT>(context, params, indices, output);    case kTfLiteInt32:      return GatherNd<int32_t, IndicesT>(context, params, indices, output);    case kTfLiteInt64:      return GatherNd<int64_t, IndicesT>(context, params, indices, output);    case kTfLiteString:      return GatherNdString<IndicesT>(params, indices, output);    default:",
    "method_name": "TfLiteStatus EvalGatherNd(TfLiteContext* context, const TfLiteTensor* params,"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57236,
  "title": "r2.8 cherry-pick: 595a65a3e22 \"Return a TFLite error if gather_nd will result in reading invalid memory\"",
  "tags": [],
  "closed_time": "2022-08-19T17:13:39Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/lite/kernels/gather_nd.cc",
    "code1": "==============================================================================*/#include <stdint.h>#include \"tensorflow/lite/c/common.h\"#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"",
    "code2": "==============================================================================*/#include <stdint.h>#include \"tensorflow/lite/c/c_api_types.h\"#include \"tensorflow/lite/c/common.h\"#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/lite/kernels/gather_nd.cc",
    "code1": "}template <typename ParamsT, typename IndicesT>TfLiteStatus GatherNd(const TfLiteTensor* params, const TfLiteTensor* indices,                      TfLiteTensor* output) {  reference_ops::GatherNd(      GetTensorShape(params), GetTensorData<ParamsT>(params),      GetTensorShape(indices), GetTensorData<IndicesT>(indices),      GetTensorShape(output), GetTensorData<ParamsT>(output));  return kTfLiteOk;}template <typename IndicesT>",
    "code2": "}template <typename ParamsT, typename IndicesT>TfLiteStatus GatherNd(TfLiteContext* context, const TfLiteTensor* params, const TfLiteTensor* indices, TfLiteTensor* output) { const TfLiteStatus status = reference_ops::GatherNd(      GetTensorShape(params), GetTensorData<ParamsT>(params),      GetTensorShape(indices), GetTensorData<IndicesT>(indices),      GetTensorShape(output), GetTensorData<ParamsT>(output));  if (status != kTfLiteOk) {    TF_LITE_KERNEL_LOG(context, \"gather_nd index out of bounds\");  }  return status;}template <typename IndicesT>",
    "method_name": "TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/lite/kernels/gather_nd.cc",
    "code1": "  switch (params->type) {    case kTfLiteFloat32:      return GatherNd<float, IndicesT>(params, indices, output);    case kTfLiteUInt8:      return GatherNd<uint8_t, IndicesT>(params, indices, output);    case kTfLiteInt8:      return GatherNd<int8_t, IndicesT>(params, indices, output);    case kTfLiteInt16:      return GatherNd<int16_t, IndicesT>(params, indices, output);    case kTfLiteInt32:      return GatherNd<int32_t, IndicesT>(params, indices, output);    case kTfLiteInt64:      return GatherNd<int64_t, IndicesT>(params, indices, output);    case kTfLiteString:      return GatherNdString<IndicesT>(params, indices, output);    default:",
    "code2": "  switch (params->type) {    case kTfLiteFloat32:      return GatherNd<float, IndicesT>(context, params, indices, output);    case kTfLiteUInt8:      return GatherNd<uint8_t, IndicesT>(context, params, indices, output);    case kTfLiteInt8:      return GatherNd<int8_t, IndicesT>(context, params, indices, output);    case kTfLiteInt16:      return GatherNd<int16_t, IndicesT>(context, params, indices, output);    case kTfLiteInt32:      return GatherNd<int32_t, IndicesT>(context, params, indices, output);    case kTfLiteInt64:      return GatherNd<int64_t, IndicesT>(context, params, indices, output);    case kTfLiteString:      return GatherNdString<IndicesT>(params, indices, output);    default:",
    "method_name": "TfLiteStatus EvalGatherNd(TfLiteContext* context, const TfLiteTensor* params,"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57235,
  "title": "r2.9 cherry-pick: 595a65a3e22 \"Return a TFLite error if gather_nd will result in reading invalid memory\"",
  "tags": [],
  "closed_time": "2022-08-19T17:13:37Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/lite/kernels/gather_nd.cc",
    "code1": "==============================================================================*/#include <stdint.h>#include \"tensorflow/lite/c/common.h\"#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"",
    "code2": "==============================================================================*/#include <stdint.h>#include \"tensorflow/lite/c/c_api_types.h\"#include \"tensorflow/lite/c/common.h\"#include \"tensorflow/lite/kernels/internal/optimized/optimized_ops.h\"#include \"tensorflow/lite/kernels/internal/reference/reference_ops.h\"",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/lite/kernels/gather_nd.cc",
    "code1": "}template <typename ParamsT, typename IndicesT>TfLiteStatus GatherNd(const TfLiteTensor* params, const TfLiteTensor* indices,                      TfLiteTensor* output) {  reference_ops::GatherNd(      GetTensorShape(params), GetTensorData<ParamsT>(params),      GetTensorShape(indices), GetTensorData<IndicesT>(indices),      GetTensorShape(output), GetTensorData<ParamsT>(output));  return kTfLiteOk;}template <typename IndicesT>",
    "code2": "}template <typename ParamsT, typename IndicesT>TfLiteStatus GatherNd(TfLiteContext* context, const TfLiteTensor* params, const TfLiteTensor* indices, TfLiteTensor* output) { const TfLiteStatus status = reference_ops::GatherNd(      GetTensorShape(params), GetTensorData<ParamsT>(params),      GetTensorShape(indices), GetTensorData<IndicesT>(indices),      GetTensorShape(output), GetTensorData<ParamsT>(output));  if (status != kTfLiteOk) {    TF_LITE_KERNEL_LOG(context, \"gather_nd index out of bounds\");  }  return status;}template <typename IndicesT>",
    "method_name": "TfLiteStatus Prepare(TfLiteContext* context, TfLiteNode* node) {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/lite/kernels/gather_nd.cc",
    "code1": "  switch (params->type) {    case kTfLiteFloat32:      return GatherNd<float, IndicesT>(params, indices, output);    case kTfLiteUInt8:      return GatherNd<uint8_t, IndicesT>(params, indices, output);    case kTfLiteInt8:      return GatherNd<int8_t, IndicesT>(params, indices, output);    case kTfLiteInt16:      return GatherNd<int16_t, IndicesT>(params, indices, output);    case kTfLiteInt32:      return GatherNd<int32_t, IndicesT>(params, indices, output);    case kTfLiteInt64:      return GatherNd<int64_t, IndicesT>(params, indices, output);    case kTfLiteString:      return GatherNdString<IndicesT>(params, indices, output);    default:",
    "code2": "  switch (params->type) {    case kTfLiteFloat32:      return GatherNd<float, IndicesT>(context, params, indices, output);    case kTfLiteUInt8:      return GatherNd<uint8_t, IndicesT>(context, params, indices, output);    case kTfLiteInt8:      return GatherNd<int8_t, IndicesT>(context, params, indices, output);    case kTfLiteInt16:      return GatherNd<int16_t, IndicesT>(context, params, indices, output);    case kTfLiteInt32:      return GatherNd<int32_t, IndicesT>(context, params, indices, output);    case kTfLiteInt64:      return GatherNd<int64_t, IndicesT>(context, params, indices, output);    case kTfLiteString:      return GatherNdString<IndicesT>(params, indices, output);    default:",
    "method_name": "TfLiteStatus EvalGatherNd(TfLiteContext* context, const TfLiteTensor* params,"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57190,
  "title": "Fixing typos",
  "tags": [
   "awaiting review",
   "ready to pull",
   "size:XS",
   "type:docs-bug"
  ],
  "closed_time": "2022-08-18T18:33:43Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/grappler/costs/graph_properties.cc",
    "code1": "    return OkStatus();  }  VLOG(1) << \"Checking any conflics in shapes and dimensions ...\";  int64_t num_incompatible_shapes = 0;  for (const NodeDef& node : graph_def.node()) {    auto ctx = refiner->GetNodeContext(&node);",
    "code2": "    return OkStatus();  }  VLOG(1) << \"Checking any conflicts in shapes and dimensions ...\";  int64_t num_incompatible_shapes = 0;  for (const NodeDef& node : graph_def.node()) {    auto ctx = refiner->GetNodeContext(&node);",
    "method_name": "Status ValidateSymbolicShapeManager(const GraphDef& graph_def,"
   },
   {
    "language": ".py",
    "dir": "tensorflow/python/ops/image_ops_impl.py",
    "code1": "  Raises:    AttributeError: Raises an attribute error when dtype is neither    float nor integer  \"\"\"  image = ops.convert_to_tensor(image, name='image')  dtype = dtypes.as_dtype(dtype)",
    "code2": "  Raises:    AttributeError: Raises an attribute error when dtype is neither    float nor integer.  \"\"\"  image = ops.convert_to_tensor(image, name='image')  dtype = dtypes.as_dtype(dtype)",
    "method_name": "def convert_image_dtype(image, dtype, saturate=False, name=None):"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57178,
  "title": "[TF-TRT] Fix pylint error (line too long)",
  "tags": [
   "awaiting review",
   "comp:gpu:tensorrt",
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-08-18T21:52:37Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57161,
  "title": "Updated the 8-bit SoftmaxOp exponent calculation with 4 16-bit tables to avoid extreme large/small slope errors",
  "tags": [
   "ready to pull",
   "size:M"
  ],
  "closed_time": "2022-08-19T21:01:31Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57096,
  "title": "[TF-TRT] DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES fixed to LLONG_MAX - 512",
  "tags": [
   "comp:gpu:tensorrt",
   "ready to pull",
   "size:S"
  ],
  "closed_time": "2022-08-15T15:39:09Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tensorflow/python/compiler/tensorrt/trt_convert.py",
    "code1": "from functools import partial  # pylint: disable=g-importing-memberimport osimport platformimport tempfileimport numpy as np",
    "code2": "from functools import partial  # pylint: disable=g-importing-memberimport osimport platformimport sysimport tempfileimport numpy as np",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "tensorflow/python/compiler/tensorrt/trt_convert.py",
    "code1": "# For TRT >= 8.4, the recommendation is MAX_INT.if (_pywrap_py_utils.is_tensorrt_enabled() and    trt_utils.is_loaded_tensorrt_version_greater_equal(8, 4, 0)):  DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = np.iinfo(np.int32).maxelse:  DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = 1 << 30PROFILE_STRATEGY_RANGE = \"Range\"PROFILE_STRATEGY_OPTIMAL = \"Optimal\"",
    "code2": "# For TRT >= 8.4, the recommendation is MAX_INT.if (_pywrap_py_utils.is_tensorrt_enabled() and    trt_utils.is_loaded_tensorrt_version_greater_equal(8, 4, 0)):  # We must use `sys.maxsize - 512` to avoid overflow during casting.  DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = sys.maxsize - 512else:  DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = 1 << 30  # 1,073,741,824PROFILE_STRATEGY_RANGE = \"Range\"PROFILE_STRATEGY_OPTIMAL = \"Optimal\"",
    "method_name": "def supported_precision_modes():"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57071,
  "title": "r2.10 cherry-pick: [oneDNN] Fix memory corruption issues",
  "tags": [],
  "closed_time": "2022-08-10T18:48:35Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57062,
  "title": "r2.10 cherry-pick: 741777a876c \"Fix out of range index error.\"",
  "tags": [],
  "closed_time": "2022-08-09T18:59:38Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/c/ops.cc",
    "code1": "                                      TF_Status* status) {  TF_SetStatus(status, TF_OK, \"\");  auto* cc_ctx = reinterpret_cast<InferenceContext*>(ctx);  if (0 < i || i >= cc_ctx->num_inputs()) {    TF_SetStatus(status, TF_INVALID_ARGUMENT, \"input index out of range\");  }  if (TF_GetCode(status) == TF_OK) {",
    "code2": "                                      TF_Status* status) {  TF_SetStatus(status, TF_OK, \"\");  auto* cc_ctx = reinterpret_cast<InferenceContext*>(ctx);  if (i < 0 || i >= cc_ctx->num_inputs()) {    TF_SetStatus(status, TF_INVALID_ARGUMENT, \"input index out of range\");  }  if (TF_GetCode(status) == TF_OK) {",
    "method_name": "void TF_ShapeInferenceContextGetInput(TF_ShapeInferenceContext* ctx, int i,"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/c/ops.cc",
    "code1": "                                       TF_Status* status) {  TF_SetStatus(status, TF_OK, \"\");  auto* cc_ctx = reinterpret_cast<InferenceContext*>(ctx);  if (0 < i || i >= cc_ctx->num_outputs()) {    TF_SetStatus(status, TF_INVALID_ARGUMENT, \"output index out of range\");  }  if (TF_GetCode(status) == TF_OK) {",
    "code2": "                                       TF_Status* status) {  TF_SetStatus(status, TF_OK, \"\");  auto* cc_ctx = reinterpret_cast<InferenceContext*>(ctx);  if (i < 0 || i >= cc_ctx->num_outputs()) {    TF_SetStatus(status, TF_INVALID_ARGUMENT, \"output index out of range\");  }  if (TF_GetCode(status) == TF_OK) {",
    "method_name": "void TF_ShapeInferenceContextSetOutput(TF_ShapeInferenceContext* ctx, int i,"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57042,
  "title": "[XLA] Add debug print to all tests",
  "tags": [
   "awaiting review",
   "comp:xla",
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-08-10T10:32:26Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57028,
  "title": "fix build of tensorflow/tensorflow/lite/c with cmake",
  "tags": [
   "comp:lite",
   "size:XS"
  ],
  "closed_time": "2022-08-08T09:25:00Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57024,
  "title": "r2.10 cherry-pick: [PluggableDevice] Minor Grappler C API fixes",
  "tags": [],
  "closed_time": "2022-08-09T14:58:20Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/c/experimental/grappler/grappler.cc",
    "code1": "}TF_FunctionLibraryDefinition* TF_NewFunctionLibraryDefinition(    TF_Buffer* graph_buf, TF_Status* status) {  TF_SetStatus(status, TF_OK, \"\");  tensorflow::GraphDef graph_def;  tensorflow::Status s = tensorflow::BufferToMessage(graph_buf, &graph_def);",
    "code2": "}TF_FunctionLibraryDefinition* TF_NewFunctionLibraryDefinition( const TF_Buffer* graph_buf, TF_Status* status) {  TF_SetStatus(status, TF_OK, \"\");  tensorflow::GraphDef graph_def;  tensorflow::Status s = tensorflow::BufferToMessage(graph_buf, &graph_def);",
    "method_name": "void TF_GetOutputPropertiesList(TF_GraphProperties* graph_properties,"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56993,
  "title": "[TF-TRT] Fail gracefully if calibration error occurs",
  "tags": [
   "awaiting review",
   "comp:gpu:tensorrt",
   "ready to pull",
   "size:S"
  ],
  "closed_time": "2022-08-11T22:42:10Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc",
    "code1": "  //  // In both of the above cases, setBatch here returns a boolean value to  // indicate the result of the calibration process.  OP_REQUIRES_ASYNC(ctx, calib_ctx->calibrator_->setBatch(input_data, *stream),                    errors::Internal(\"Failed to feed calibration data\"),                    dummy_async_helper);  VLOG(2) << \"Passed calibration data\";  ExecuteNativeSegment(ctx, async_helper);}",
    "code2": "  //  // In both of the above cases, setBatch here returns a boolean value to  // indicate the result of the calibration process.  if(!calib_ctx->calibrator_->setBatch(input_data, *stream)) {    VLOG(2) << \"Failed to feed calibration data\";  } else {    VLOG(2) << \"Passed calibration data\";  }  ExecuteNativeSegment(ctx, async_helper);}",
    "method_name": "void TRTEngineOp::ExecuteCalibration(OpKernelContext* ctx,"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc",
    "code1": "    if (!s.ok()) {      LOG(ERROR) << \"Calibration failed: \" << s;      cres->calibrator_->setDone();  // Ignore further pushes    } else {      // Transfer the ownership of the engine to the engine cache, so we can      // dump it out during conversion for TF 2.0.",
    "code2": "    if (!s.ok()) {      LOG(ERROR) << \"Calibration failed: \" << s;      cres->calibrator_->setDone();  // Ignore further pushes      cache_res->cache_.emplace(shapes, std::make_unique<EngineContext>());    } else {      // Transfer the ownership of the engine to the engine cache, so we can      // dump it out during conversion for TF 2.0.",
    "method_name": "Status TRTEngineOp::AllocateCalibrationResources("
   },
   {
    "language": ".py",
    "dir": "tensorflow/python/compiler/tensorrt/trt_convert.py",
    "code1": "def _save_calibration_table(node):  calibration_table = gen_trt_ops.get_calibration_data_op(      _get_canonical_engine_name(node.name))  node.attr[\"calibration_data\"].s = calibration_table.numpy()def _convert_to_tensor(inp):",
    "code2": "def _save_calibration_table(node):  try:    calibration_table = gen_trt_ops.get_calibration_data_op(        _get_canonical_engine_name(node.name))    node.attr[\"calibration_data\"].s = calibration_table.numpy()  except errors.UnknownError:    logging.warning(\"Warning calibration error for %s\", node.name)def _convert_to_tensor(inp):",
    "method_name": "def _apply_inlining(func):"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56956,
  "title": "[XLA] [Docs] Add a known issue: must-be-constant inputs as functions of induction variables",
  "tags": [
   "awaiting review",
   "comp:xla",
   "ready to pull",
   "size:S"
  ],
  "closed_time": "2022-08-01T18:23:23Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56939,
  "title": "Fix out of range index error.",
  "tags": [
   "comp:runtime",
   "prtype:bugfix",
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-08-05T18:55:42Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/c/ops.cc",
    "code1": "                                      TF_Status* status) {  TF_SetStatus(status, TF_OK, \"\");  auto* cc_ctx = reinterpret_cast<InferenceContext*>(ctx);  if (0 < i || i >= cc_ctx->num_inputs()) {    TF_SetStatus(status, TF_INVALID_ARGUMENT, \"input index out of range\");  }  if (TF_GetCode(status) == TF_OK) {",
    "code2": "                                      TF_Status* status) {  TF_SetStatus(status, TF_OK, \"\");  auto* cc_ctx = reinterpret_cast<InferenceContext*>(ctx);  if (i < 0 || i >= cc_ctx->num_inputs()) {    TF_SetStatus(status, TF_INVALID_ARGUMENT, \"input index out of range\");  }  if (TF_GetCode(status) == TF_OK) {",
    "method_name": "void TF_ShapeInferenceContextGetInput(TF_ShapeInferenceContext* ctx, int i,"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/c/ops.cc",
    "code1": "                                       TF_Status* status) {  TF_SetStatus(status, TF_OK, \"\");  auto* cc_ctx = reinterpret_cast<InferenceContext*>(ctx);  if (0 < i || i >= cc_ctx->num_outputs()) {    TF_SetStatus(status, TF_INVALID_ARGUMENT, \"output index out of range\");  }  if (TF_GetCode(status) == TF_OK) {",
    "code2": "                                       TF_Status* status) {  TF_SetStatus(status, TF_OK, \"\");  auto* cc_ctx = reinterpret_cast<InferenceContext*>(ctx);  if (i < 0 || i >= cc_ctx->num_outputs()) {    TF_SetStatus(status, TF_INVALID_ARGUMENT, \"output index out of range\");  }  if (TF_GetCode(status) == TF_OK) {",
    "method_name": "void TF_ShapeInferenceContextSetOutput(TF_ShapeInferenceContext* ctx, int i,"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56933,
  "title": "Some unit tests are fixed and should be removed from exclude list",
  "tags": [
   "ready to pull",
   "size:S"
  ],
  "closed_time": "2022-07-29T13:37:02Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56914,
  "title": "Fix failing unit tests quantization_ops:quantization_ops_test",
  "tags": [
   "comp:mkl",
   "ready to pull",
   "size:S",
   "type:docs-bug"
  ],
  "closed_time": "2022-07-27T23:30:51Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/mkl/mkl_avgpooling_op.cc",
    "code1": "      if (int8_forward_inference) {        const Tensor& min_input_t = MklGetInput(context, 1);        const Tensor& max_input_t = MklGetInput(context, 2);        const float min_input = min_input_t.flat<float>()(0);        const float max_input = max_input_t.flat<float>()(0);        Tensor* output_min = nullptr;        Tensor* output_max = nullptr;",
    "code2": "      if (int8_forward_inference) {        const Tensor& min_input_t = MklGetInput(context, 1);        const Tensor& max_input_t = MklGetInput(context, 2);        OP_REQUIRES(            context, TensorShapeUtils::IsScalar(min_input_t.shape()),            errors::InvalidArgument(                \"min_input shape must be rank 0 but is rank \",                min_input_t.dims(), \", received shape: \", min_input_t.shape()));        OP_REQUIRES(            context, TensorShapeUtils::IsScalar(max_input_t.shape()),            errors::InvalidArgument(                \"max_input shape must be rank 0 but is rank \",                max_input_t.dims(), \", received shape: \", max_input_t.shape()));        const float min_input = min_input_t.scalar<float>()();        const float max_input = max_input_t.scalar<float>()();        Tensor* output_min = nullptr;        Tensor* output_max = nullptr;",
    "method_name": "class MklAvgPoolingOp : public MklPoolingForwardOpBase<T> {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/mkl/mkl_maxpooling_op.cc",
    "code1": "        // Pass min, max from input to output.        const Tensor& min_input_t = MklGetInput(context, 1);        const Tensor& max_input_t = MklGetInput(context, 2);        const float min_input = min_input_t.flat<float>()(0);        const float max_input = max_input_t.flat<float>()(0);        Tensor* output_min = nullptr;        Tensor* output_max = nullptr;",
    "code2": "        // Pass min, max from input to output.        const Tensor& min_input_t = MklGetInput(context, 1);        const Tensor& max_input_t = MklGetInput(context, 2);        OP_REQUIRES(            context, TensorShapeUtils::IsScalar(min_input_t.shape()),            errors::InvalidArgument(                \"min_input shape must be rank 0 but is rank \",                min_input_t.dims(), \", received shape: \", min_input_t.shape()));        OP_REQUIRES(            context, TensorShapeUtils::IsScalar(max_input_t.shape()),            errors::InvalidArgument(                \"max_input shape must be rank 0 but is rank \",                max_input_t.dims(), \", received shape: \", max_input_t.shape()));        const float min_input = min_input_t.scalar<float>()();        const float max_input = max_input_t.scalar<float>()();        Tensor* output_min = nullptr;        Tensor* output_max = nullptr;",
    "method_name": "class MklMaxPoolingOp : public MklPoolingForwardOpBase<T> {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56911,
  "title": "Correction of some minor errors in the Tile Converter testing procedure.",
  "tags": [
   "awaiting review",
   "comp:gpu:tensorrt",
   "ready to pull",
   "size:M"
  ],
  "closed_time": "2022-07-28T18:43:48Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56902,
  "title": "Case error",
  "tags": [
   "size:XL"
  ],
  "closed_time": "2022-07-26T10:36:02Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/compiler/tf2xla/kernels/spacetobatch_op.cc",
    "code1": "#include \"tensorflow/compiler/tf2xla/xla_op_kernel.h\"#include \"tensorflow/compiler/tf2xla/xla_op_registry.h\"#include \"tensorflow/compiler/xla/client/xla_builder.h\"namespace tensorflow {namespace {",
    "code2": "#include \"tensorflow/compiler/tf2xla/xla_op_kernel.h\"#include \"tensorflow/compiler/tf2xla/xla_op_registry.h\"#include \"tensorflow/compiler/xla/client/xla_builder.h\"#include \"tensorflow/core/util/overflow.h\"namespace tensorflow {namespace {",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/compiler/tf2xla/kernels/spacetobatch_op.cc",
    "code1": "    int64_t pad_end = paddings.Get<int64_t>({i, 1});    OP_REQUIRES(ctx, pad_start >= 0 && pad_end >= 0,                errors::InvalidArgument(\"Paddings must be non-negative\"));    dim->set_edge_padding_low(pad_start);    dim->set_edge_padding_high(pad_end);    padded_shape[1 + i] += pad_start + pad_end;    block_num_elems *= block_shape[i];  }  // Don't pad the remainder dimensions.  for (int i = 0; i < remainder_shape.size(); ++i) {",
    "code2": "    int64_t pad_end = paddings.Get<int64_t>({i, 1});    OP_REQUIRES(ctx, pad_start >= 0 && pad_end >= 0,                errors::InvalidArgument(\"Paddings must be non-negative\"));    OP_REQUIRES(ctx, block_shape[i] >= 1,                errors::InvalidArgument(                    \"All values in block_shape must be positive, got value, \",                    block_shape[i], \" at index \", i, \".\"));    dim->set_edge_padding_low(pad_start);    dim->set_edge_padding_high(pad_end);    padded_shape[1 + i] += pad_start + pad_end;    block_num_elems = MultiplyWithoutOverflow(block_num_elems, block_shape[i]);  }  // Don't pad the remainder dimensions.  for (int i = 0; i < remainder_shape.size(); ++i) {",
    "method_name": "void SpaceToBatch(XlaOpKernelContext* ctx, const xla::XlaOp& input,"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/compiler/tf2xla/kernels/spacetobatch_op.cc",
    "code1": "  OP_REQUIRES(ctx, block_num_elems > 0,              errors::InvalidArgument(                  \"The product of the block dimensions must be positive\"));  xla::XlaOp padded =      xla::Pad(input, XlaHelpers::Zero(b, input_dtype), padding_config);",
    "code2": "  OP_REQUIRES(ctx, block_num_elems > 0,              errors::InvalidArgument(                  \"The product of the block dimensions must be positive\"));  const int64_t batch_size = input_shape[0];  const int64_t output_dim =      MultiplyWithoutOverflow(batch_size, block_num_elems);  if (output_dim < 0) {    OP_REQUIRES(        ctx, output_dim >= 0,        errors::InvalidArgument(\"Negative output dimension size caused by \"                                \"overflow when multiplying \",                                batch_size, \" and \", block_num_elems));  }  xla::XlaOp padded =      xla::Pad(input, XlaHelpers::Zero(b, input_dtype), padding_config);",
    "method_name": "void SpaceToBatch(XlaOpKernelContext* ctx, const xla::XlaOp& input,"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/compiler/tf2xla/kernels/spacetobatch_op.cc",
    "code1": "  //       padded_shape[M] / block_shape[M-1],  //       block_shape[M-1]] +  //      remaining_shape  const int64_t batch_size = input_shape[0];  std::vector<int64_t> reshaped_padded_shape(input_rank + block_rank);  reshaped_padded_shape[0] = batch_size;  for (int i = 0; i < block_rank; ++i) {",
    "code2": "  //       padded_shape[M] / block_shape[M-1],  //       block_shape[M-1]] +  //      remaining_shape  std::vector<int64_t> reshaped_padded_shape(input_rank + block_rank);  reshaped_padded_shape[0] = batch_size;  for (int i = 0; i < block_rank; ++i) {",
    "method_name": "void SpaceToBatch(XlaOpKernelContext* ctx, const xla::XlaOp& input,"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/compiler/tf2xla/kernels/spacetobatch_op.cc",
    "code1": "  // Determine the length of the prefix of block dims that can be combined  // into the batch dimension due to having no padding and block_shape=1.  std::vector<int64_t> output_shape(input_rank);  output_shape[0] = batch_size * block_num_elems;  for (int i = 0; i < block_rank; ++i) {    output_shape[1 + i] = padded_shape[1 + i] / block_shape[i];  }",
    "code2": "  // Determine the length of the prefix of block dims that can be combined  // into the batch dimension due to having no padding and block_shape=1.  std::vector<int64_t> output_shape(input_rank);  output_shape[0] = output_dim;  for (int i = 0; i < block_rank; ++i) {    output_shape[1 + i] = padded_shape[1 + i] / block_shape[i];  }",
    "method_name": "void SpaceToBatch(XlaOpKernelContext* ctx, const xla::XlaOp& input,"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/common_runtime/eager/execute.cc",
    "code1": "    const Tensor* tensor;    // TODO(fishx): Avoid blocking here.    TF_RETURN_IF_ERROR(tensor_handle->Tensor(&tensor));    const ResourceHandle& handle = tensor->flat<ResourceHandle>()(0);    device_name = handle.device();",
    "code2": "    const Tensor* tensor;    // TODO(fishx): Avoid blocking here.    TF_RETURN_IF_ERROR(tensor_handle->Tensor(&tensor));    if (tensor->NumElements() == 0) {      return errors::InvalidArgument(\"Empty resource handle\");    }    const ResourceHandle& handle = tensor->flat<ResourceHandle>()(0);    device_name = handle.device();",
    "method_name": "Status GetDeviceForInput(const EagerContext& ctx, TensorHandle* tensor_handle,"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/common_runtime/eager/execute.cc",
    "code1": "  auto status = GetOrCreateKernelAndDevice(op, retvals, num_retvals, &kernel);#ifdef INTEL_MKL  if (IsMKLEnabled() && kernel != nullptr && !ctx.RunEagerOpAsFunction() &&      op->Device() == kVariantDeviceNull) {    // oneDNN optimization pass relies on the op's assigned device to determine    // whether it can be rewritten.",
    "code2": "  auto status = GetOrCreateKernelAndDevice(op, retvals, num_retvals, &kernel);#ifdef INTEL_MKL  if (IsMKLEnabled() && kernel != nullptr &&      op->Device() == kVariantDeviceNull) {    // oneDNN optimization pass relies on the op's assigned device to determine    // whether it can be rewritten.",
    "method_name": "Status EagerLocalExecute(EagerOperation* op, TensorHandle** retvals,"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc",
    "code1": "  if (!IsMKLEnabled()) {    return false;  }  // Don't rewrite the op if it should run as a function.  if (op->EagerContext().RunEagerOpAsFunction()) {    return false;  }  DataType data_type;  if (op->Attrs().Get(\"T\", &data_type) != Status::OK()) {    return false;",
    "code2": "  if (!IsMKLEnabled()) {    return false;  }  DataType data_type;  if (op->Attrs().Get(\"T\", &data_type) != Status::OK()) {    return false;",
    "method_name": "bool MklEagerOpRewrite::ShouldRewriteOp(EagerOperation* op) {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/framework/shape_inference.cc",
    "code1": "#include \"tensorflow/core/lib/strings/numbers.h\"#include \"tensorflow/core/lib/strings/scanner.h\"#include \"tensorflow/core/lib/strings/str_util.h\"namespace tensorflow {namespace shape_inference {",
    "code2": "#include \"tensorflow/core/lib/strings/numbers.h\"#include \"tensorflow/core/lib/strings/scanner.h\"#include \"tensorflow/core/lib/strings/str_util.h\"#include \"tensorflow/core/util/overflow.h\"namespace tensorflow {namespace shape_inference {",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/framework/shape_inference.cc",
    "code1": "    *out = UnknownDim();  } else {    // Invariant: Both values are known and greater than 1.    const int64_t product = first_value * second_value;    if (product < 0) {      return errors::InvalidArgument(          \"Negative dimension size caused by overflow when multiplying \",",
    "code2": "    *out = UnknownDim();  } else {    // Invariant: Both values are known and greater than 1.    const int64_t product = MultiplyWithoutOverflow(first_value, second_value);    if (product < 0) {      return errors::InvalidArgument(          \"Negative dimension size caused by overflow when multiplying \",",
    "method_name": "Status InferenceContext::Multiply(DimensionHandle first,"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/grappler/optimizers/remapper.cc",
    "code1": "    return true;  };  if (ShapesSymbolicallyEqual(prot0_shape, prot1_shape) ||      !ShapesBroadcastable(prot0_shape, prot1_shape))    return false;  if (IsConvOrMatMul(*node_def_0)) {    bias_port = 1;    return (is_supported_shape(prot0_shape, prot1_shape));  } else if (IsConvOrMatMul(*node_def_1)) {    bias_port = 0;    return (is_supported_shape(prot1_shape, prot0_shape));  }  return false;}",
    "code2": "    return true;  };  // This is used only for MatMul+Add fusion.  const auto is_matmul_supported_shape =      [](const TensorShapeProto& shape,         const TensorShapeProto& bcast_shape) -> bool {    if (shape.dim_size() < 2 || bcast_shape.dim_size() != 1) return false;    int channel_dim = shape.dim(shape.dim_size() - 1).size();    return (channel_dim == bcast_shape.dim(0).size());  };  if (ShapesSymbolicallyEqual(prot0_shape, prot1_shape) ||      !ShapesBroadcastable(prot0_shape, prot1_shape))    return false;  // For now block MatMul+Add fusion if Bias dims are more than one.  // TODO(intel-tf): Enable this fusion once it is properly tested.  if (IsConvOrMatMul(*node_def_0)) {    bias_port = 1;    if (IsMatMul(*node_def_0)) {      return (is_matmul_supported_shape(prot0_shape, prot1_shape));    } else {      return (is_supported_shape(prot0_shape, prot1_shape));    }  } else if (IsConvOrMatMul(*node_def_1)) {    bias_port = 0;    if (IsMatMul(*node_def_1)) {      return (is_matmul_supported_shape(prot1_shape, prot0_shape));    } else {      return (is_supported_shape(prot1_shape, prot0_shape));    }  }  return false;}",
    "method_name": "bool IsBiasSemanticAdd(const RemapperContext& ctx,"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/conv_grad_ops_3d.cc",
    "code1": "    TensorShape filter_shape;    if (takes_shape_) {      const Tensor& filter_sizes = context->input(1);      OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(                                  filter_sizes.vec<int32>(), &filter_shape));    } else {",
    "code2": "    TensorShape filter_shape;    if (takes_shape_) {      const Tensor& filter_sizes = context->input(1);      OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),                  errors::InvalidArgument(                      \"filter_sizes shape must be rank 1 but is rank \",                      filter_sizes.shape().dims()));      OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(                                  filter_sizes.vec<int32>(), &filter_shape));    } else {",
    "method_name": "class Conv3DBackpropFilterOp : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/conv_grad_ops_3d.cc",
    "code1": "    TensorShape filter_shape;    if (takes_shape_) {      const Tensor& filter_sizes = context->input(1);      OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(                                  filter_sizes.vec<int32>(), &filter_shape));    } else {",
    "code2": "    TensorShape filter_shape;    if (takes_shape_) {      const Tensor& filter_sizes = context->input(1);      OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),                  errors::InvalidArgument(                      \"filter_sizes shape must be rank 1 but is rank \",                      filter_sizes.shape().dims()));      OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(                                  filter_sizes.vec<int32>(), &filter_shape));    } else {",
    "method_name": "class Conv3DCustomBackpropFilterOp : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/conv_grad_ops_3d.cc",
    "code1": "    TensorShape filter_shape;    if (takes_shape_) {      const Tensor& filter_sizes = context->input(1);      OP_REQUIRES_OK(context, tensor::MakeShape(filter_sizes, &filter_shape));    } else {      filter_shape = context->input(1).shape();",
    "code2": "    TensorShape filter_shape;    if (takes_shape_) {      const Tensor& filter_sizes = context->input(1);      OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()),                  errors::InvalidArgument(                      \"filter_sizes shape must be rank 1 but is rank \",                      filter_sizes.shape().dims()));      OP_REQUIRES_OK(context, tensor::MakeShape(filter_sizes, &filter_shape));    } else {      filter_shape = context->input(1).shape();",
    "method_name": "class Conv3DBackpropFilterOp<GPUDevice, T> : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/depthwise_conv_grad_op.cc",
    "code1": "      OP_REQUIRES(context, in_sizes_data[i] >= 0,                  errors::InvalidArgument(\"Dimension \", i,                                          \" of input_sizes must be >= 0\"));      input_shape.AddDim(in_sizes_data[i]);    }    const TensorShape& filter_shape = filter.shape();    EXTRACT_AND_VERIFY_DIMENSIONS(\"DepthwiseConv2DBackpropInput\");",
    "code2": "      OP_REQUIRES(context, in_sizes_data[i] >= 0,                  errors::InvalidArgument(\"Dimension \", i,                                          \" of input_sizes must be >= 0\")); OP_REQUIRES_OK(context, input_shape.AddDimWithStatus(in_sizes_data[i]));    }    const TensorShape& filter_shape = filter.shape();    EXTRACT_AND_VERIFY_DIMENSIONS(\"DepthwiseConv2DBackpropInput\");",
    "method_name": "class DepthwiseConv2dNativeBackpropInputOp : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/depthwise_conv_grad_op.cc",
    "code1": "      OP_REQUIRES(context, filter_sizes_data[i] >= 0,                  errors::InvalidArgument(\"Dimension \", i,                                          \" of filter_sizes must be >= 0\"));      filter_shape.AddDim(filter_sizes_data[i]);    }    const TensorShape& input_shape = input.shape();",
    "code2": "      OP_REQUIRES(context, filter_sizes_data[i] >= 0,                  errors::InvalidArgument(\"Dimension \", i,                                          \" of filter_sizes must be >= 0\"));      OP_REQUIRES_OK(context,                     filter_shape.AddDimWithStatus(filter_sizes_data[i]));    }    const TensorShape& input_shape = input.shape();",
    "method_name": "class DepthwiseConv2dNativeBackpropFilterOp : public OpKernel {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56853,
  "title": "fixed some typos, formatted document",
  "tags": [
   "size:XL"
  ],
  "closed_time": "2022-08-19T05:46:34Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56841,
  "title": "Cache size is too small for machines with more than 49 CPU cores",
  "tags": [
   "awaiting review",
   "comp:data",
   "ready to pull",
   "size:XS",
   "type:bug"
  ],
  "closed_time": "2022-07-22T19:57:02Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56831,
  "title": "Fix compilation issue when building mhlo as an external project.",
  "tags": [
   "awaiting review",
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-07-21T10:25:27Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56811,
  "title": "Remove incorrect test tags so that all expected pip tests are run",
  "tags": [
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-08-02T18:56:52Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56764,
  "title": "[ROCm] Add generic layout optimizer fix for ROCm.",
  "tags": [
   "comp:gpu",
   "size:XS"
  ],
  "closed_time": "2022-07-29T22:30:34Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56730,
  "title": "Fix PyLint error where line is too long",
  "tags": [
   "comp:ops",
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-07-29T23:49:48Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tensorflow/python/ops/array_ops.py",
    "code1": "  arranged along the last axis of `indices`.  This is similar to `tf.gather`, in which `indices` defines slices into the  first dimension of `params`. In `tf.gather_nd`, `indices` defines slices into the  first `N` dimensions of `params`, where `N = indices.shape[-1]`.  Caution: On CPU, if an out of bound index is found, an error is returned.  On GPU, if an out of bound index is found, a 0 is stored in the",
    "code2": "  arranged along the last axis of `indices`.  This is similar to `tf.gather`, in which `indices` defines slices into the  first dimension of `params`. In `tf.gather_nd`, `indices` defines slices into the first `N` dimensions of `params`, where `N = indices.shape[-1]`.  Caution: On CPU, if an out of bound index is found, an error is returned.  On GPU, if an out of bound index is found, a 0 is stored in the",
    "method_name": "def gather_nd(params, indices, name=None, batch_dims=0):"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56728,
  "title": "Fixing the incorrect link in backend.py",
  "tags": [
   "comp:keras",
   "size:XS"
  ],
  "closed_time": "2022-07-13T06:52:03Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tensorflow/python/keras/backend.py",
    "code1": "      ragged: Boolean, whether the placeholder should have a ragged type.          In this case, values of 'None' in the 'shape' argument represent          ragged dimensions. For more information about RaggedTensors, see this          [guide](https://www.tensorflow.org/guide/ragged_tensors).  Raises:      ValueError: If called with sparse = True and ragged = True.",
    "code2": "      ragged: Boolean, whether the placeholder should have a ragged type.          In this case, values of 'None' in the 'shape' argument represent          ragged dimensions. For more information about RaggedTensors, see this          [guide](https://www.tensorflow.org/guide/ragged_tensor).  Raises:      ValueError: If called with sparse = True and ragged = True.",
    "method_name": "def placeholder(shape=None,"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56714,
  "title": "Fix unit tests that rely on unsupported behaviour",
  "tags": [
   "awaiting review",
   "ready to pull",
   "size:S",
   "type:bug"
  ],
  "closed_time": "2022-07-20T23:06:15Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tensorflow/python/ops/array_ops.py",
    "code1": "  if mode == \"CONSTANT\":    # TODO(rjryan): Once the forward compatibility period (3 weeks) have passed    # remove the \"Pad\" fallback here.    if not tensor_util.is_tf_type(constant_values) and constant_values == 0:      result = gen_array_ops.pad(tensor, paddings, name=name)    else:      result = gen_array_ops.pad_v2(",
    "code2": "  if mode == \"CONSTANT\":    # TODO(rjryan): Once the forward compatibility period (3 weeks) have passed    # remove the \"Pad\" fallback here.    if not tensor_util.is_tf_type(constant_values) \\        and np.isscalar(constant_values) and constant_values == 0:      result = gen_array_ops.pad(tensor, paddings, name=name)    else:      result = gen_array_ops.pad_v2(",
    "method_name": "def pad(tensor, paddings, mode=\"CONSTANT\", name=None, constant_values=0):  # pyl"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56691,
  "title": "[NVIDIA TF] Fix cublas build issues and update stub",
  "tags": [
   "comp:xla",
   "ready to pull",
   "size:M"
  ],
  "closed_time": "2022-08-10T21:33:22Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83808,
  "title": "Debug baseline CI in #83317",
  "tags": [
   "cla signed",
   "open source"
  ],
  "closed_time": "2022-08-20T23:39:46Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83754,
  "title": "Minifier fixes",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-22T05:22:42Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "functorch/functorch/_src/fx_minifier.py",
    "code1": "        return    node.op = 'placeholder'    node.args = ()    node.target = node.name    concrete_val = node.meta.get('concrete_value', None)    if isinstance(concrete_val, torch.Tensor):",
    "code2": "        return    node.op = 'placeholder'    node.args = ()    node.kwargs = {}    node.target = node.name    concrete_val = node.meta.get('concrete_value', None)    if isinstance(concrete_val, torch.Tensor):",
    "method_name": "def _convert_node_to_placeholder(node, inps):"
   },
   {
    "language": ".py",
    "dir": "functorch/functorch/_src/fx_minifier.py",
    "code1": "    num_queries = 0    def graph_fails(graph, inps):        nonlocal num_queries        num_queries += 1        mod = fx.GraphModule(fail_f, graph)        mod.graph.lint()",
    "code2": "    num_queries = 0    def deepcopy_fx_graph(fx_graph):        return fx.GraphModule(fail_f, copy.deepcopy(fx_graph)).graph    def graph_fails(graph, inps):        nonlocal num_queries        graph = copy.deepcopy(graph)        num_queries += 1        mod = fx.GraphModule(fail_f, graph)        mod.graph.lint()",
    "method_name": "def minifier(fail_f: fx.GraphModule, inps, module_fails, dump_state: Callable ="
   },
   {
    "language": ".py",
    "dir": "functorch/functorch/_src/fx_minifier.py",
    "code1": "        def new_func(old_state: ReproState, granularity=1):            print()            print(f\"Strategy: {name} (G: {granularity}) ({len(old_state.graph.nodes)} nodes, {len(old_state.inps)} inputs)\")            new_state = strategy(copy.deepcopy(old_state.graph), list(old_state.inps), granularity)            if new_state is not None:                new_nodes = len(new_state.graph.nodes)                old_nodes = len(old_state.graph.nodes)",
    "code2": "        def new_func(old_state: ReproState, granularity=1):            print()            print(f\"Strategy: {name} (G: {granularity}) ({len(old_state.graph.nodes)} nodes, {len(old_state.inps)} inputs)\")            new_state = strategy(deepcopy_fx_graph(old_state.graph), list(old_state.inps), granularity)            if new_state is not None:                new_nodes = len(new_state.graph.nodes)                old_nodes = len(old_state.graph.nodes)",
    "method_name": "def _register_strategy(strategy: Callable, name: str):"
   },
   {
    "language": ".py",
    "dir": "functorch/functorch/_src/fx_minifier.py",
    "code1": "        num_nodes = len(cur_graph.nodes)        for start_range in range(0, num_nodes, granularity):            is_removing = False            new_graph = copy.deepcopy(cur_graph)            new_inps = cur_inps[:]            end_range = min(num_nodes, start_range + granularity)            for idx in range(start_range, end_range):",
    "code2": "        num_nodes = len(cur_graph.nodes)        for start_range in range(0, num_nodes, granularity):            is_removing = False            new_graph = deepcopy_fx_graph(cur_graph)            new_inps = cur_inps[:]            end_range = min(num_nodes, start_range + granularity)            for idx in range(start_range, end_range):",
    "method_name": "def delta_debugging(cur_graph: fx.Graph, cur_inps, granularity):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83722,
  "title": "[Vulkan] Fix issues in GRU and LSTM",
  "tags": [
   "cla signed",
   "fb-exported",
   "Merged",
   "oncall: jit"
  ],
  "closed_time": "2022-08-19T16:29:13Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/ops/Gru.cpp",
    "code1": "namespace ops {namespace {//// input_vk: input tensor of shape (L, N, H_in) when batch_first=False//                                 (N, L, H_in) when batch_first=True containing//                                 the features of the input sequence// hx_vk: initial hidden state for each element in the batch. tensor of shape (D// * num_layers, N, H_out) output: tensor of shape (N, L, D * H_out)) when// batch_first=True h_n: tensor of shape (D * num_layers, N, H_out)////  where//    L = sequence length//    N = batch size//    D = 2 if bidirectional=True otherwise 1",
    "code2": "namespace ops {namespace {//// input_vk: input tensor containing the features of the input sequence//           tensor of shape (N, L, H_in) when batch_first=True//                           (L, N, H_in) when batch_first=False//// hx_vk: initial hidden state for each element in the batch.//        tensor of shape (D * num_layers, N, H_out)//// output: tensor of shape (N, L, D * H_out) when batch_first=True//                         (L, N, D * H_out) when batch_first=False//// h_n: tensor of shape (D * num_layers, N, H_out)//// where//    L = sequence length//    N = batch size//    D = 2 if bidirectional=True otherwise 1",
    "method_name": "namespace vulkan {"
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/ops/Gru.cpp",
    "code1": " TORCH_INTERNAL_ASSERT(!train, \"Vulkan gru expects 'train' to be false.\"); TORCH_INTERNAL_ASSERT(      !bidirectional, \"Vulkan gru expects 'bidirectional' to be false.\"); TORCH_INTERNAL_ASSERT(      batch_first, \"Vulkan gru expects 'batch_first' to be true.\"); TORCH_INTERNAL_ASSERT(      dropout < std::numeric_limits<double>::epsilon() * 1000, \"Vulkan gru expects 'dropout' to be 0.0.\"); const auto hidden_size = hx_vk.size(2);  std::vector<at::Tensor> h_n_list; // hidden output // reshape to 2D due to Vulkan at::mm op accepts only 2D auto x =      input_vk.reshape({input_vk.size(0) * input_vk.size(1), input_vk.size(2)}); for (int64_t i = 0; i < num_layers; ++i) { // extract each hidden state and squeeze into 2D dim",
    "code2": " TORCH_INTERNAL_ASSERT(!train, \"Vulkan gru expects 'train' to be false.\"); TORCH_INTERNAL_ASSERT(      !bidirectional, \"Vulkan gru expects 'bidirectional' to be false.\"); TORCH_INTERNAL_ASSERT(      dropout < std::numeric_limits<double>::epsilon() * 1000, \"Vulkan gru expects 'dropout' to be 0.0.\"); const auto batch_size = input_vk.size(0); const auto seq_length = input_vk.size(1); TORCH_INTERNAL_ASSERT(      (batch_size == 1 && seq_length == 1) || batch_first, \"Vulkan gru expects batch-first input\"); const auto hidden_size = hx_vk.size(2);  std::vector<at::Tensor> h_n_list; // hidden output // reshape to 2D due to Vulkan at::mm op accepts only 2D auto x = input_vk.reshape({batch_size * seq_length, input_vk.size(2)}); for (int64_t i = 0; i < num_layers; ++i) { // extract each hidden state and squeeze into 2D dim",
    "method_name": "std::tuple<Tensor, Tensor> gru_input("
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/ops/Gru.cpp",
    "code1": "  } auto h_n = at::cat(h_n_list, 1);  h_n = h_n.reshape({h_n.size(0) * h_n.size(1), h_n.size(2), h_n.size(3)}); return std::tuple<Tensor, Tensor>(x, h_n);}",
    "code2": "  } auto h_n = at::cat(h_n_list, 1);  x = x.reshape({batch_size, seq_length, x.size(1)});  h_n = h_n.reshape({h_n.size(0) * h_n.size(1), h_n.size(2), h_n.size(3)}); return std::tuple<Tensor, Tensor>(x, h_n);}",
    "method_name": "std::tuple<Tensor, Tensor> gru_input("
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/ops/Gru.cpp",
    "code1": " int64_t num_layers) { TORCH_CHECK( static_cast<int64_t>(params_cpu.size()) == 4 * num_layers, \"Vulkan gru expects 'params_cpu' size to be 4 * 'num_layers'.\");  std::vector<c10::intrusive_ptr<LinearPackedContext>> linear_op_contexts;  linear_op_contexts.reserve(num_layers * 6);",
    "code2": " int64_t num_layers) { TORCH_CHECK( static_cast<int64_t>(params_cpu.size()) == 4 * num_layers, \"Vulkan gru expects 'params_cpu' size to be 4 * 'num_layers'.\" \" But 'params_cpu' has size: \",      params_cpu.size(), \" and 'num_layers' is: \",      num_layers);  std::vector<c10::intrusive_ptr<LinearPackedContext>> linear_op_contexts;  linear_op_contexts.reserve(num_layers * 6);",
    "method_name": "std::vector<c10::intrusive_ptr<LinearPackedContext>> pack_linear_op_contexts("
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/ops/Gru.cpp",
    "code1": " TORCH_INTERNAL_ASSERT(!train, \"Vulkan gru expects 'train' to be false.\"); TORCH_INTERNAL_ASSERT(      !bidirectional, \"Vulkan gru expects 'bidirectional' to be false.\"); TORCH_INTERNAL_ASSERT(      batch_first, \"Vulkan gru expects 'batch_first' to be true.\"); TORCH_INTERNAL_ASSERT(      dropout < std::numeric_limits<double>::epsilon() * 1000, \"Vulkan gru expects 'dropout' to be 0.0.\");",
    "code2": " TORCH_INTERNAL_ASSERT(!train, \"Vulkan gru expects 'train' to be false.\"); TORCH_INTERNAL_ASSERT(      !bidirectional, \"Vulkan gru expects 'bidirectional' to be false.\"); TORCH_INTERNAL_ASSERT(      dropout < std::numeric_limits<double>::epsilon() * 1000, \"Vulkan gru expects 'dropout' to be 0.0.\");",
    "method_name": "GruPackedContext::GruPackedContext("
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/ops/Gru.cpp",
    "code1": " TORCH_INTERNAL_ASSERT(      hx_vk.sizes().size() == 3, \"Vulkan gru expects 'hx_vk' dims to be 3.\"); const c10::List<c10::IValue> packed_linear_contexts =      gru_context->get_val(GruPackedContext::Packed::LinearContexts).toList(); const int64_t num_layers =      gru_context->get_val(GruPackedContext::Packed::NumLayers).toInt(); const int64_t linear_contexts_per_layer = 6; // (b_ir, w_ir), (b_hr, w_hr), (b_iz, w_iz), // (b_hz, w_hz), (b_in,cw_in), (b_hn, w_hn)  std::vector<at::Tensor> h_n_list; // hidden output // reshape to 2D due to Vulkan at::mm op accepts only 2D auto x =      input_vk.reshape({input_vk.size(0) * input_vk.size(1), input_vk.size(2)}); for (int64_t i = 0; i < num_layers; ++i) { // extract each hidden state and squeeze into 2D dim",
    "code2": " TORCH_INTERNAL_ASSERT(      hx_vk.sizes().size() == 3, \"Vulkan gru expects 'hx_vk' dims to be 3.\"); const int64_t num_layers =      gru_context->get_val(GruPackedContext::Packed::NumLayers).toInt(); const bool batch_first =      gru_context->get_val(GruPackedContext::Packed::BatchFirst).toBool(); const auto batch_size = input_vk.size(0); const auto seq_length = input_vk.size(1); TORCH_INTERNAL_ASSERT(      (batch_size == 1 && seq_length == 1) || batch_first, \"Vulkan gru expects batch-first input\"); const c10::List<c10::IValue> packed_linear_contexts =      gru_context->get_val(GruPackedContext::Packed::LinearContexts).toList(); const int64_t linear_contexts_per_layer = 6; // (b_ir, w_ir), (b_hr, w_hr), (b_iz, w_iz), // (b_hz, w_hz), (b_in,cw_in), (b_hn, w_hn)  std::vector<at::Tensor> h_n_list; // hidden output // reshape to 2D due to Vulkan at::mm op accepts only 2D auto x = input_vk.reshape({batch_size * seq_length, input_vk.size(2)}); for (int64_t i = 0; i < num_layers; ++i) { // extract each hidden state and squeeze into 2D dim",
    "method_name": "std::tuple<Tensor, Tensor> run_gru_context("
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/ops/Gru.cpp",
    "code1": "  } auto h_n = at::cat(h_n_list, 1);  h_n = h_n.reshape({h_n.size(0) * h_n.size(1), h_n.size(2), h_n.size(3)}); return std::tuple<Tensor, Tensor>(x, h_n);}",
    "code2": "  } auto h_n = at::cat(h_n_list, 1);  x = x.reshape({batch_size, seq_length, x.size(1)});  h_n = h_n.reshape({h_n.size(0) * h_n.size(1), h_n.size(2), h_n.size(3)}); return std::tuple<Tensor, Tensor>(x, h_n);}",
    "method_name": "std::tuple<Tensor, Tensor> run_gru_context("
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/ops/Lstm.cpp",
    "code1": "namespace ops {namespace {//// input_vk: input tensor of shape (L, N, H_in) when batch_first=False or (N, L,// H_in) when batch_first=True//           containing the features of the input sequence// hx_vk: tensor of shape (D * num_layers, N, H_out) containing the initial// hidden state for each element in the input sequence. cx_vk: tensor of shape// (D * num_layers, N, H_cell) containing the initial cell state for each// element in the input sequence. output: tensor of shape (L, N, D * H_out) when// batch_first=False or (N, L, D * H_out) when batch_first=True//         containing the output features (h_t) from the last layer of the LSTM,//         for each t// h_n: tensor of shape (D * num_layers, N, H_out) containing the final hidden// state for each element in the sequence. c_n: tensor of shape (D * num_layers,// N, H_cell) containing the final cell state for each element in the sequence.////  where//    L = sequence length",
    "code2": "namespace ops {namespace {//// input_vk: input tensor of shape (L, N, H_in) when batch_first=False or// (N, L, H_in) when batch_first=True containing the features of the input// sequence//// hx_vk: tensor of shape (D * num_layers, N, H_out) containing the initial// hidden state for each element in the input sequence.//// cx_vk: tensor of shape (D * num_layers, N, H_cell) containing the initial// cell state for each element in the input sequence.//// output: tensor of shape (L, N, D * H_out) when batch_first=False or// (N, L, D * H_out) when batch_first=True, containing the output features// (h_t) from the last layer of the LSTM, for each t//// h_n: tensor of shape (D * num_layers, N, H_out) containing the final hidden// state for each element in the sequence.//// c_n: tensor of shape (D * num_layers, N, H_cell) containing the final cell// state for each element in the sequence.////  where//    L = sequence length",
    "method_name": "namespace vulkan {"
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/ops/Lstm.cpp",
    "code1": " TORCH_INTERNAL_ASSERT(!train, \"Vulkan LSTM expects 'train' to be false.\"); TORCH_INTERNAL_ASSERT(      !bidirectional, \"Vulkan LSTM expects 'bidirectional' to be false.\"); TORCH_INTERNAL_ASSERT(      batch_first, \"Vulkan LSTM expects 'batch_first' to be true.\"); TORCH_INTERNAL_ASSERT(      dropout < std::numeric_limits<double>::epsilon() * 1000, \"Vulkan LSTM expects 'dropout' to be 0.0.\"); const Tensor& hx_vk = hx[0]; const Tensor& cx_vk = hx[1];",
    "code2": " TORCH_INTERNAL_ASSERT(!train, \"Vulkan LSTM expects 'train' to be false.\"); TORCH_INTERNAL_ASSERT(      !bidirectional, \"Vulkan LSTM expects 'bidirectional' to be false.\"); TORCH_INTERNAL_ASSERT(      dropout < std::numeric_limits<double>::epsilon() * 1000, \"Vulkan LSTM expects 'dropout' to be 0.0.\"); const auto batch_size = input_vk.size(0); const auto seq_length = input_vk.size(1); TORCH_INTERNAL_ASSERT(      (batch_size == 1 && seq_length == 1) || batch_first, \"Vulkan gru expects batch-first input\"); const Tensor& hx_vk = hx[0]; const Tensor& cx_vk = hx[1];",
    "method_name": "std::tuple<Tensor, Tensor, Tensor> lstm_input("
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/ops/Lstm.cpp",
    "code1": "  std::vector<at::Tensor> c_n_list; // cell state output // reshape to 2D due to Vulkan at::mm op accepts only 2D auto x =      input_vk.reshape({input_vk.size(0) * input_vk.size(1), input_vk.size(2)});  h_n_list.reserve(num_layers);  c_n_list.reserve(num_layers);",
    "code2": "  std::vector<at::Tensor> c_n_list; // cell state output // reshape to 2D due to Vulkan at::mm op accepts only 2D auto x = input_vk.reshape({batch_size * seq_length, input_vk.size(2)});  h_n_list.reserve(num_layers);  c_n_list.reserve(num_layers);",
    "method_name": "std::tuple<Tensor, Tensor, Tensor> lstm_input("
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/ops/Lstm.cpp",
    "code1": " auto h_n = at::cat(h_n_list, 1); auto c_n = at::cat(c_n_list, 1);  h_n = h_n.reshape({h_n.size(0) * h_n.size(1), h_n.size(2), h_n.size(3)});  c_n = c_n.reshape({c_n.size(0) * c_n.size(1), c_n.size(2), c_n.size(3)}); return std::tuple<Tensor, Tensor, Tensor>(x, h_n, c_n);",
    "code2": " auto h_n = at::cat(h_n_list, 1); auto c_n = at::cat(c_n_list, 1);  x = x.reshape({batch_size, seq_length, x.size(1)});  h_n = h_n.reshape({h_n.size(0) * h_n.size(1), h_n.size(2), h_n.size(3)});  c_n = c_n.reshape({c_n.size(0) * c_n.size(1), c_n.size(2), c_n.size(3)}); return std::tuple<Tensor, Tensor, Tensor>(x, h_n, c_n);",
    "method_name": "std::tuple<Tensor, Tensor, Tensor> lstm_input("
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/ops/Lstm.cpp",
    "code1": " int64_t num_layers) { TORCH_CHECK( static_cast<int64_t>(params_cpu.size()) == 4 * num_layers, \"Vulkan LSTM expects 'params_cpu' size to be 4 * 'num_layers'.\");  std::vector<c10::intrusive_ptr<LinearPackedContext>> linear_op_contexts;  linear_op_contexts.reserve(num_layers * 8);",
    "code2": " int64_t num_layers) { TORCH_CHECK( static_cast<int64_t>(params_cpu.size()) == 4 * num_layers, \"Vulkan LSTM expects 'params_cpu' size to be 4 * 'num_layers'.\" \" But 'params_cpu' has size: \",      params_cpu.size(), \" and 'num_layers' is: \",      num_layers);  std::vector<c10::intrusive_ptr<LinearPackedContext>> linear_op_contexts;  linear_op_contexts.reserve(num_layers * 8);",
    "method_name": "pack_lstm_linear_op_contexts("
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/ops/Lstm.cpp",
    "code1": " TORCH_INTERNAL_ASSERT(!train, \"Vulkan LSTM expects 'train' to be false.\"); TORCH_INTERNAL_ASSERT(      !bidirectional, \"Vulkan LSTM expects 'bidirectional' to be false.\"); TORCH_INTERNAL_ASSERT(      batch_first, \"Vulkan LSTM expects 'batch_first' to be true.\"); TORCH_INTERNAL_ASSERT(      dropout < std::numeric_limits<double>::epsilon() * 1000, \"Vulkan LSTM expects 'dropout' to be 0.0.\");",
    "code2": " TORCH_INTERNAL_ASSERT(!train, \"Vulkan LSTM expects 'train' to be false.\"); TORCH_INTERNAL_ASSERT(      !bidirectional, \"Vulkan LSTM expects 'bidirectional' to be false.\"); TORCH_INTERNAL_ASSERT(      dropout < std::numeric_limits<double>::epsilon() * 1000, \"Vulkan LSTM expects 'dropout' to be 0.0.\");",
    "method_name": "LstmPackedContext::LstmPackedContext("
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/ops/Lstm.cpp",
    "code1": "      cx_vk.sizes().size() == 3, \"Vulkan LSTM expects cell state dims to be 3.\"); const c10::List<c10::IValue> packed_linear_op_contexts =      lstm_context->get_val(LstmPackedContext::Packed::LinearContexts).toList(); const int64_t packed_num_layers =      lstm_context->get_val(LstmPackedContext::Packed::NumLayers).toInt(); const int64_t linear_op_contexts_per_layer = 8; // (b_ii, w_ii), (b_hi, w_hi), (b_if, w_if), (b_hf, w_hf), (b_ig, // w_ig), (b_hg, w_hg), (b_io, w_io), (b_ho, w_ho)  std::vector<at::Tensor> h_n_list; // hidden state output  std::vector<at::Tensor> c_n_list; // cell state output // reshape to 2D due to Vulkan at::mm op accepts only 2D auto x =      input_vk.reshape({input_vk.size(0) * input_vk.size(1), input_vk.size(2)});  h_n_list.reserve(packed_num_layers);  c_n_list.reserve(packed_num_layers); for (int64_t l = 0; l < packed_num_layers; ++l) { // extract each hidden state and squeeze into 2D dim auto h = at::slice(hx_vk, 0, l, l + 1, 1);    h = h.reshape({h.size(0) * h.size(1), h.size(2)});",
    "code2": "      cx_vk.sizes().size() == 3, \"Vulkan LSTM expects cell state dims to be 3.\"); const int64_t num_layers =      lstm_context->get_val(LstmPackedContext::Packed::NumLayers).toInt(); const bool batch_first =      lstm_context->get_val(LstmPackedContext::Packed::BatchFirst).toBool(); const auto batch_size = input_vk.size(0); const auto seq_length = input_vk.size(1); TORCH_INTERNAL_ASSERT(      (batch_size == 1 && seq_length == 1) || batch_first, \"Vulkan gru expects batch-first input\"); const c10::List<c10::IValue> packed_linear_op_contexts =      lstm_context->get_val(LstmPackedContext::Packed::LinearContexts).toList(); const int64_t linear_op_contexts_per_layer = 8; // (b_ii, w_ii), (b_hi, w_hi), (b_if, w_if), (b_hf, w_hf), // (b_ig, w_ig), (b_hg, w_hg), (b_io, w_io), (b_ho, w_ho)  std::vector<at::Tensor> h_n_list; // hidden state output  std::vector<at::Tensor> c_n_list; // cell state output // reshape to 2D due to Vulkan at::mm op accepts only 2D auto x = input_vk.reshape({batch_size * seq_length, input_vk.size(2)});  h_n_list.reserve(num_layers);  c_n_list.reserve(num_layers); for (int64_t l = 0; l < num_layers; ++l) { // extract each hidden state and squeeze into 2D dim auto h = at::slice(hx_vk, 0, l, l + 1, 1);    h = h.reshape({h.size(0) * h.size(1), h.size(2)});",
    "method_name": "std::tuple<Tensor, Tensor, Tensor> run_lstm_context("
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/ops/Lstm.cpp",
    "code1": " auto h_n = at::cat(h_n_list, 1); auto c_n = at::cat(c_n_list, 1);  h_n = h_n.reshape({h_n.size(0) * h_n.size(1), h_n.size(2), h_n.size(3)});  c_n = c_n.reshape({c_n.size(0) * c_n.size(1), c_n.size(2), c_n.size(3)}); return std::tuple<Tensor, Tensor, Tensor>(x, h_n, c_n);",
    "code2": " auto h_n = at::cat(h_n_list, 1); auto c_n = at::cat(c_n_list, 1);  x = x.reshape({batch_size, seq_length, x.size(1)});  h_n = h_n.reshape({h_n.size(0) * h_n.size(1), h_n.size(2), h_n.size(3)});  c_n = c_n.reshape({c_n.size(0) * c_n.size(1), c_n.size(2), c_n.size(3)}); return std::tuple<Tensor, Tensor, Tensor>(x, h_n, c_n);",
    "method_name": "std::tuple<Tensor, Tensor, Tensor> run_lstm_context("
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83701,
  "title": "fix functionalization <> fake tensor mode",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-19T23:30:58Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "c10/core/TensorImpl.cpp",
    "code1": " const auto& maybe_torch_dispatch_mode_state = c10::impl::TorchDispatchModeTLS::get_state(); // TODO: do we have to exclude after Python dispatch key set? if (maybe_torch_dispatch_mode_state) {    r = maybe_torch_dispatch_mode_state->pyinterpreter()->detach(this);  } else if (      key_set_.has(DispatchKey::Python) &&",
    "code2": " const auto& maybe_torch_dispatch_mode_state = c10::impl::TorchDispatchModeTLS::get_state(); // TODO: do we have to exclude after Python dispatch key set? if (maybe_torch_dispatch_mode_state &&      !c10::impl::tls_is_dispatch_key_excluded(DispatchKey::Python)) {    r = maybe_torch_dispatch_mode_state->pyinterpreter()->detach(this);  } else if (      key_set_.has(DispatchKey::Python) &&",
    "method_name": "c10::intrusive_ptr<TensorImpl> TensorImpl::shallow_copy_and_detach_core("
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83697,
  "title": "[MPS] Fix torch.full for uint8",
  "tags": [
   "ciflow/mps",
   "cla signed",
   "Merged",
   "release notes: mps",
   "topic: bug fixes"
  ],
  "closed_time": "2022-08-18T21:59:26Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83682,
  "title": "fix torch._C._nn.linear bug",
  "tags": [
   "cla signed",
   "fx",
   "Merged"
  ],
  "closed_time": "2022-08-22T02:16:26Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "torch/fx/experimental/migrate_gradual_types/constraint_generator.py",
    "code1": " assert isinstance(n.args[0], Node) weight_dims, counter = gen_tensor_dims(2, counter) equality_constraint = BinConstraintT(symbols[n.args[1]], TensorType(weight_dims), op_eq) constraints, counter = linear_constraints(n, weight_dims[0], weight_dims[1], symbols, counter) return [equality_constraint] + constraints, counter",
    "code2": " assert isinstance(n.args[0], Node) weight_dims, counter = gen_tensor_dims(2, counter) equality_constraint = BinConstraintT(symbols[n.args[1]], TensorType(weight_dims), op_eq) constraints, counter = linear_constraints(n, weight_dims[1], weight_dims[0], symbols, counter) return [equality_constraint] + constraints, counter",
    "method_name": "def torch_linear_inference_rule(n: Node, symbols, constraints, counter):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83645,
  "title": "fixing define_constant pybind signature to match std::complex scalar",
  "tags": [
   "cla signed",
   "Merged",
   "oncall: jit",
   "open source",
   "Reverted"
  ],
  "closed_time": "2022-08-18T04:52:37Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "torch/csrc/jit/codegen/cuda/python_frontend/python_bindings.cpp",
    "code1": "      .def( \"define_constant\",          [](nvfuser::FusionDefinition& self, c10::complex<double> val) -> nvfuser::Scalar* {            nvfuser::Scalar* out = self.defineScalar();            self.defineRecord(new nvfuser::ConstantRecord<                              torch::jit::fuser::cuda::ComplexDouble,                              c10::complex<double>>({out->index}, val)); return out;          },          py::return_value_policy::reference)",
    "code2": "      .def( \"define_constant\",          [](nvfuser::FusionDefinition& self, std::complex<double> val) -> nvfuser::Scalar* {            nvfuser::Scalar* out = self.defineScalar();            self.defineRecord(new nvfuser::ConstantRecord<                              torch::jit::fuser::cuda::ComplexDouble,                              c10::complex<double>>(                {out->index}, static_cast<c10::complex<double>>(val))); return out;          },          py::return_value_policy::reference)",
    "method_name": "void initNvFuserPythonBindings(PyObject* module) {"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83644,
  "title": "Minifier fix for non tensor inputs",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-19T00:39:33Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "functorch/functorch/_src/fx_minifier.py",
    "code1": "# inplace modifies node/inpsdef _convert_node_to_placeholder(node, inps): if node.op == 'output': return node.op = 'placeholder' node.args = () node.target = node.name concrete_val = node.meta['concrete_value'] if isinstance(concrete_val, torch.Tensor): inps.append(concrete_val) else:",
    "code2": "# inplace modifies node/inpsdef _convert_node_to_placeholder(node, inps): if node.op == 'output' or node.op == \"placeholder\": return node.op = 'placeholder' node.args = () node.target = node.name concrete_val = node.meta.get('concrete_value', None) if isinstance(concrete_val, torch.Tensor): inps.append(concrete_val) else:",
    "method_name": "def propagate(self, *args):"
   },
   {
    "language": ".py",
    "dir": "functorch/functorch/_src/fx_minifier.py",
    "code1": " if node.op == 'output': output = node break output_args = sorted(output.args[0], key=lambda x: x.idx if isinstance(x, fx.Node) else int(1e9)) if len(output_args) == 1: return None for idx in range(0, len(output_args), granularity): output.args = (output_args[:idx] + output_args[idx + granularity:],) if graph_fails(cur_graph, cur_inps):",
    "code2": " if node.op == 'output': output = node break output_args = sorted(output.args[0], key=lambda x: x.idx if isinstance(x, fx.Node) else int(1e9)) if len(output_args) == 1: return None for idx in range(0, len(output_args), granularity): output.args = (output_args[:idx] + output_args[idx + granularity:],) if graph_fails(cur_graph, cur_inps):",
    "method_name": "def remove_outputs(cur_graph, cur_inps, granularity):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83636,
  "title": "Bugfix4",
  "tags": [],
  "closed_time": "2022-08-17T23:50:31Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83633,
  "title": "[TESTING] Debug API rate limit",
  "tags": [
   "cla signed"
  ],
  "closed_time": "2022-08-18T07:13:23Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": ".github/scripts/get_workflow_job_id.py",
    "code1": "    \"Authorization\": \"token \" + GITHUB_TOKEN,}response = requests.get(    f\"{PYTORCH_GITHUB_API}/actions/runs/{args.workflow_run_id}/jobs?per_page=100\",    headers=REQUEST_HEADERS,",
    "code2": "    \"Authorization\": \"token \" + GITHUB_TOKEN,}print(f\"=== HEADERS {REQUEST_HEADERS}\")debug = requests.get(\"https://api.github.com/rate_limit\", headers=REQUEST_HEADERS)print(f\"==== RESPONSE {debug.json()}\")response = requests.get(    f\"{PYTORCH_GITHUB_API}/actions/runs/{args.workflow_run_id}/jobs?per_page=100\",    headers=REQUEST_HEADERS,",
    "method_name": ""
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83627,
  "title": "[vulkan] Throw std::runtime_error instead of using TORCH_CHECK when creating Vulkan context/runtime fails",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-19T16:20:23Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/api/Context.cpp",
    "code1": "      };      return new Context(runtime()->default_adapter_i(), config);    } catch (const std::exception& e) {      TORCH_CHECK(          false, \"Vulkan: Failed to initialize context! Error: \", e.what());    } catch (...) { TORCH_CHECK( false, \"Vulkan: Failed to initialize context! Error: Unknown\");    }    return nullptr;  }());  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(context, \"Invalid Vulkan context!\");  return context.get();}",
    "code2": "      };      return new Context(runtime()->default_adapter_i(), config);    } catch (const c10::Error& e) {      TORCH_WARN(          \"Pytorch Vulkan Context: Failed to initialize global vulkan context: \",          e.what());    } catch (const std::exception& e) {      TORCH_WARN(          \"Pytorch Vulkan Context: Failed to initialize global vulkan context: \",          e.what());    } catch (...) { TORCH_WARN( \"Pytorch Vulkan Context: Failed to initialize global vulkan context!\");    }    return nullptr;  }());  TORCH_CHECK(      context,      \"Pytorch Vulkan Context: The global context could not be retrieved \"      \"because it failed to initialize.\");  return context.get();}",
    "method_name": "Context* context() {"
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/api/Resource.cpp",
    "code1": "  if (access_ & MemoryAccessType::WRITE) {    // Call will be ignored by implementation if the memory type this allocation    // belongs to is not HOST_VISIBLE or is HOST_COHERENT, which is the behavior    // we want. VK_CHECK(vmaFlushAllocation(allocator_, allocation_, 0u, VK_WHOLE_SIZE));  }  vmaUnmapMemory(allocator_, allocation_);",
    "code2": "  if (access_ & MemoryAccessType::WRITE) {    // Call will be ignored by implementation if the memory type this allocation    // belongs to is not HOST_VISIBLE or is HOST_COHERENT, which is the behavior    // we want. Don't check the result here as the destructor cannot throw.    vmaFlushAllocation(allocator_, allocation_, 0u, VK_WHOLE_SIZE);  }  vmaUnmapMemory(allocator_, allocation_);",
    "method_name": "MemoryMap::~MemoryMap() {"
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/api/Runtime.cpp",
    "code1": " try { return std::make_unique<Runtime>(Runtime(default_config));  } catch (const std::exception& e) { TORCH_WARN( \"Pytorch Vulkan Runtime: Failed to initialize the global vulkan runtime! \"",
    "code2": " try { return std::make_unique<Runtime>(Runtime(default_config));  } catch (const c10::Error& e) { TORCH_WARN( \"Pytorch Vulkan Runtime: Failed to initialize the global vulkan runtime! \" \"The global vulkan runtime is invalid. Error: \",        e.what());  } catch (const std::exception& e) { TORCH_WARN( \"Pytorch Vulkan Runtime: Failed to initialize the global vulkan runtime! \"",
    "method_name": "std::unique_ptr<Runtime> init_global_vulkan_runtime() {"
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/api/Runtime.cpp",
    "code1": " case AdapterSelector::First:          default_adapter_i_ = create_adapter(select_first);      }    } catch (const std::exception& e) { TORCH_WARN( \"Pytorch Vulkan Runtime: Could not initialize default device! Error: \",",
    "code2": " case AdapterSelector::First:          default_adapter_i_ = create_adapter(select_first);      }    } catch (const c10::Error& e) { TORCH_WARN( \"Pytorch Vulkan Runtime: Could not initialize default device! Error: \",          e.what());    } catch (const std::exception& e) { TORCH_WARN( \"Pytorch Vulkan Runtime: Could not initialize default device! Error: \",",
    "method_name": "Runtime::Runtime(const RuntimeConfiguration config)"
   },
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/api/Runtime.cpp",
    "code1": " // Runtime.h as it would have internal linkage. static const std::unique_ptr<Runtime> p_runtime = init_global_vulkan_runtime(); TORCH_CHECK(      p_runtime, \"Pytorch Vulkan Runtime: The global runtime could not be retrieved \" \"because it failed to initialize.\"); return p_runtime.get();}",
    "code2": " // Runtime.h as it would have internal linkage. static const std::unique_ptr<Runtime> p_runtime = init_global_vulkan_runtime(); TORCH_CHECK(      p_runtime, \"Pytorch Vulkan Runtime: The global runtime could not be retrieved \" \"because it failed to initialize.\"); return p_runtime.get();}",
    "method_name": "Runtime* runtime() {"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83626,
  "title": "reinplacing pass fixes for torchbench + huggingface",
  "tags": [
   "cla signed",
   "fx"
  ],
  "closed_time": "2022-08-19T23:30:57Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "torch/_subclasses/meta_utils.py",
    "code1": " def __call__(self, t): # TODO: zero tensors?  We appear to have eliminated them by # excluding complex for now if type(t) is torch.Tensor or type(t) is torch.nn.Parameter: if any(                [ t.is_sparse_csr,",
    "code2": " def __call__(self, t): # TODO: zero tensors?  We appear to have eliminated them by # excluding complex for now from torch._subclasses.fake_tensor import FakeTensor if ( type(t) is torch.Tensor or type(t) is torch.nn.Parameter or isinstance(t, FakeTensor)        ): if any(                [ t.is_sparse_csr,",
    "method_name": "def is_c_of_r(complex_dtype, real_dtype):"
   },
   {
    "language": ".py",
    "dir": "torch/fx/passes/reinplace.py",
    "code1": "from torch.fx import Nodefrom torch.fx._compatibility import compatibilityfrom torch._subclasses.fake_tensor import FakeTensorMode, FakeTensorfrom torch.utils._pytree import tree_mapfrom torch.multiprocessing.reductions import StorageWeakRefimport _operator",
    "code2": "from torch.fx import Nodefrom torch.fx._compatibility import compatibilityfrom torch._subclasses.fake_tensor import FakeTensorMode, FakeTensorfrom torch.utils._pytree import tree_map, tree_flattenfrom torch.multiprocessing.reductions import StorageWeakRefimport _operator",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "torch/fx/passes/reinplace.py",
    "code1": " def propagate(self, *args): self.multi_output_view_nodes = {} self.node_counter = -1 with FakeTensorMode.push() as mode: fake_args = [mode.from_tensor(a) for a in args] return super().run(*fake_args)",
    "code2": " def propagate(self, *args): self.multi_output_view_nodes = {} self.node_counter = -1 with FakeTensorMode(allow_meta=True) as mode: fake_args = [mode.from_tensor(a) for a in args] return super().run(*fake_args)",
    "method_name": "def run_node(self, node: Node):"
   },
   {
    "language": ".py",
    "dir": "torch/fx/passes/reinplace.py",
    "code1": " for f in inplace_overloads if _schemas_match(op._schema, f._schema)    ] # This is for sanity: if foo() and foo_() are both operators, # we expect them to have compatible schemas. # (This is asserted by codegen for ATen, but might not be true # for other arbitrary operators). assert len(inplace_overloads_with_matching_schemas) == 1 inplace_op = inplace_overloads_with_matching_schemas[0] return inplace_op",
    "code2": " for f in inplace_overloads if _schemas_match(op._schema, f._schema)    ] # Just becuase foo() and foo_() are both existing operators, # They aren't guaranteed to have compatible schemas. # For example, pow.Scalar(Scalar self, Tensor exponent) has no valid inplace variant, # Even though several overloads of pow_ exist. if len(inplace_overloads_with_matching_schemas) == 0: return None assert len(inplace_overloads_with_matching_schemas) == 1 inplace_op = inplace_overloads_with_matching_schemas[0] return inplace_op",
    "method_name": "def _maybe_get_inplace_op(op):"
   },
   {
    "language": ".py",
    "dir": "torch/fx/passes/reinplace.py",
    "code1": "    Given a node \"b = foo(a, ...)\", the algorithm for re-inplacing is as follows:    (1) Check if foo has a mutating variant. If not, move to the next node.        Note that we ignore view ops (we don't bother to turn `as_strided()`        into `as_strided_()`), as it complicates the algorithm and doesn't",
    "code2": "    Given a node \"b = foo(a, ...)\", the algorithm for re-inplacing is as follows:    (1a) Check if foo has a mutating variant. If not, move to the next node.        Note that we ignore view ops (we don't bother to turn `as_strided()`        into `as_strided_()`), as it complicates the algorithm and doesn't",
    "method_name": "def reinplace(gm, *sample_args):"
   },
   {
    "language": ".py",
    "dir": "torch/fx/passes/reinplace.py",
    "code1": "        Currently, we also only check for an inplace op, `foo_`.        Later, we should beef this up to check for out= or mutable ops.    (2) Check if \"a\" is an alias of any of the program inputs.        If it is, skip and move to the next node.",
    "code2": "        Currently, we also only check for an inplace op, `foo_`.        Later, we should beef this up to check for out= or mutable ops.    (1b) Check that the self argument we're attempting to reinplace         has acceptable metadata to reinplace with.         For example, if we have:           a = torch.ones(1)           b = torch.ones(10)           out = torch.add(a, b)         We can't turn that into           a.add_(b)         Because that would require resizing \"a\".         Similarly, we can't convert torch.ge(a, b) into a.ge_(b),         beause that would require changing a's dtype (from e.g. float32 to bool).         Note that in this specific example, we could technically do better..         If we see the pattern:           a_1 = a.ge(b)           a_2 = aten._to_copy(a_1, a.dtype)         Then we this should be valid to completely re-inplace         (this is exactly what functionalization will emit when it sees a.ge_(b)).         This optimization is only really important for user programs         that directly use inplace comparison ops though.         We also cannot re-inplace on tensors that have overlapping memory,         e.g. torch.ones(1).expand(4, 4).add_(1)    (2) Check if \"a\" is an alias of any of the program inputs.        If it is, skip and move to the next node.",
    "method_name": "def reinplace(gm, *sample_args):"
   },
   {
    "language": ".py",
    "dir": "torch/fx/passes/reinplace.py",
    "code1": " assert len(node.target._schema.arguments) > 0 assert 'Tensor' in str(node.target._schema.arguments[0].type) # Step 2: ensure that the op we're trying to re-inplace isn't a program input. self_arg = node.args[0] self_arg_name = self_arg.name self_arg_storage = StorageWeakRef(self_arg.meta['fake_result'].storage())",
    "code2": " assert len(node.target._schema.arguments) > 0 assert 'Tensor' in str(node.target._schema.arguments[0].type) # Step 1b: Check that the self argument we're attempting to reinplace # has the same size/stride as the output. # For example, we shouldn't try to reinplace torch.add(scalar_tensor, larger_tensor) # As it would require resizing scalar_tensor. # (We could potentially swizzle this into larger_tensor.add_(scalar_tensor), # this is probably an optimization to revisit later). self_arg = node.args[0] self_flattened, _ = tree_flatten(self_arg.meta['fake_result']) node_flattened, _ = tree_flatten(node.meta['fake_result']) assert len(self_flattened) == len(node_flattened) self_has_wrong_metadata = False for self_meta, node_meta in zip(self_flattened, node_flattened): if self_meta.numel() != node_meta.numel(): self_has_wrong_metadata = True if self_meta.dtype != node_meta.dtype: self_has_wrong_metadata = True # We also cannot re-inplace on tensors that have internal memory overlap. # e.g. torch.ones(1).expand(4, 4).add_(1) if torch._debug_has_internal_overlap(self_meta) == 1: self_has_wrong_metadata = True # Here, we (optimistically) assume that a.resize(b) is valid to re-inplace, # Since users should never really be calling the functional \"torch.ops.aten.resize\" # op directly in their programs. if self_has_wrong_metadata and node.target != torch.ops.aten.resize.default: continue # Step 2: ensure that the op we're trying to re-inplace isn't a program i self_arg = node.args[0] self_arg_name = self_arg.name self_arg_storage = StorageWeakRef(self_arg.meta['fake_result'].storage())",
    "method_name": "def _add_to_map(x):"
   },
   {
    "language": ".py",
    "dir": "torch/fx/passes/reinplace.py",
    "code1": " node_to_update.args = tuple(new_args) node_to_update.kwargs = new_kwargs old_ref = StorageWeakRef(old.meta['fake_result'].storage()) node_ref = StorageWeakRef(node_to_update.meta['fake_result'].storage()) if old_ref == node_ref: # This will happen if we're updating a view op, e.g. # e.g. replacing #     x = view(old) #     x = view(new) # When that happens, we need to make sure to keep our # storage mapping up to date. new_ref = StorageWeakRef(new.meta['fake_result'].storage()) # Technically, \"old_ref\" and all its aliases will remain # in our mapping. # That should be fine though, since we deleted \"old\"",
    "code2": " node_to_update.args = tuple(new_args) node_to_update.kwargs = new_kwargs old_flattened_res, _ = tree_flatten(old.meta['fake_result']) node_flattened_res, _ = tree_flatten(node_to_update.meta['fake_result']) old_res_storage = set(StorageWeakRef(x.storage()) for x in old_flattened_res if isinstance(x, FakeTensor)) node_res_storage = set(StorageWeakRef(x.storage()) for x in node_flattened_res if isinstance(x, FakeTensor)) # This will happen if we're updating a view op, e.g. # e.g. replacing #     x = view(old) #     x = view(new) # When that happens, we need to make sure to keep our # storage mapping up to date. # # We're checking for len(...) == 1 here because all view ops are guaranteed to return either a single tensor, # or multiple tensors that all share the same storage. # We can't just check equality because we might encounter FX nodes that return zero tensor outputs. if len(old_res_storage) == 1 and len(node_res_storage) == 1 and old_res_storage == node_res_storage: new_flattened_res, _ = tree_flatten(new.meta['fake_result']) new_res_storage = set(StorageWeakRef(x.storage()) for x in new_flattened_res if isinstance(x, FakeTensor)) assert len(new_res_storage) == 1                        (old_ref,) = old_res_storage                        (new_ref,) = new_res_storage                        (node_ref,) = node_res_storage # Technically, \"old_ref\" and all its aliases will remain # in our mapping. # That should be fine though, since we deleted \"old\"",
    "method_name": "def _add_to_map(x):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83623,
  "title": "Release the GIL when munmap'ing tensors - fixes #77139",
  "tags": [
   "cla signed",
   "Merged",
   "open source"
  ],
  "closed_time": "2022-08-18T15:24:25Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "torch/csrc/autograd/python_variable.cpp",
    "code1": "    }  } TORCH_INTERNAL_ASSERT(!isResurrectable((THPVariable*)self));  self->cdata = MaybeOwned<Variable>(); return 0;}",
    "code2": "    }  } TORCH_INTERNAL_ASSERT(!isResurrectable((THPVariable*)self));  { // MapAllocator can take significant time to release large tensors; // release the GIL here to avoid impacting main thread perf.    pybind11::gil_scoped_release no_gil;    self->cdata = MaybeOwned<Variable>();  } return 0;}",
    "method_name": "static int THPVariable_clear(THPVariable* self) {"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83608,
  "title": "Reset compile cache to fix flaky test",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-17T20:12:23Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83489,
  "title": "S3 third-party deps sync workflow: specify correct secrets",
  "tags": [
   "cla signed",
   "Merged",
   "Reverted",
   "topic: not user facing"
  ],
  "closed_time": "2022-08-16T02:16:19Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83484,
  "title": "[BE] Better test stats errors",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-16T07:51:16Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83463,
  "title": "[FuncTorch] Fix compilation with -Werror",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-16T07:50:25Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/CompileCache.cpp",
    "code1": "    cache_.emplace(cacheKey, compileFn);  } const int64_t size() const { return cache_.size(); }  /// Clear the cache.  void clear() { cache_.clear(); }",
    "code2": "    cache_.emplace(cacheKey, compileFn);  }  int64_t size() const { return cache_.size(); }  /// Clear the cache.  void clear() { cache_.clear(); }",
    "method_name": "struct CompileCache {"
   },
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/DynamicLayer.cpp",
    "code1": "  return VmapInterpreterPtr(&interpreter_).randomness();}constexpr DispatchKeySet kFrontBackKeys({kDynamicLayerBackModeKey, kDynamicLayerFrontModeKey});using DynmetaData = std::unordered_map<int64_t, std::shared_ptr<bool>>;DynmetaData kDynMetaDataSingleton;",
    "code2": "  return VmapInterpreterPtr(&interpreter_).randomness();}using DynmetaData = std::unordered_map<int64_t, std::shared_ptr<bool>>;DynmetaData kDynMetaDataSingleton;",
    "method_name": "RandomnessType DynamicLayer::randomness() const {"
   },
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/dim/dim.cpp",
    "code1": "    0,                              /* tp_descr_get */    0,                              /* tp_descr_set */    0,                              /* tp_dictoffset */    (initproc) Dim_init,      /* tp_init */    0,                              /* tp_alloc */    Dim::new_stub,                      /* tp_new */};",
    "code2": "    0,                              /* tp_descr_get */    0,                              /* tp_descr_set */    0,                              /* tp_dictoffset */    (initproc)(void*) Dim_init,     /* tp_init */    0,                              /* tp_alloc */    Dim::new_stub,                      /* tp_new */};",
    "method_name": "PyTypeObject Dim::Type = {"
   },
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/dim/dim.cpp",
    "code1": "        } else {            bound_ = true;            dims_.resize(size);            for (ssize_t i = 0; i < size; ++i) {                dims_[i] = Dim::create(py::unicode_from_format(\"%S%i\", name_.ptr(), (int)i));            }        }",
    "code2": "        } else {            bound_ = true;            dims_.resize(size);            for (Py_ssize_t i = 0; i < size; ++i) {                dims_[i] = Dim::create(py::unicode_from_format(\"%S%i\", name_.ptr(), (int)i));            }        }",
    "method_name": "struct DimList : public py::base<DimList> {"
   },
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/dim/dim.cpp",
    "code1": "    py::sequence_view seq = sizes;    auto size = seq.size();    self->bind_len(size);    for (ssize_t i = 0; i < size; ++i) {        self->dims_[i]->set_size(py::to_int(seq[i]));    }    Py_RETURN_NONE;",
    "code2": "    py::sequence_view seq = sizes;    auto size = seq.size();    self->bind_len(size);    for (Py_ssize_t i = 0; i < size; ++i) {        self->dims_[i]->set_size(py::to_int(seq[i]));    }    Py_RETURN_NONE;",
    "method_name": "static PyObject* DimList_bind(DimList *self,"
   },
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/dim/dim.cpp",
    "code1": "}static PyMethodDef DimList_methods[] = {    {\"bind\", (PyCFunction) DimList_bind, METH_FASTCALL | METH_KEYWORDS},    {\"bind_len\", (PyCFunction) DimList_bind_len, METH_FASTCALL | METH_KEYWORDS},    {NULL, NULL, 0, NULL}        /* Sentinel */};",
    "code2": "}static PyMethodDef DimList_methods[] = {    {\"bind\", (PyCFunction)(void*) DimList_bind, METH_FASTCALL | METH_KEYWORDS},    {\"bind_len\", (PyCFunction)(void*) DimList_bind_len, METH_FASTCALL | METH_KEYWORDS},    {NULL, NULL, 0, NULL}        /* Sentinel */};",
    "method_name": "static PyObject* DimList_bind_len(DimList *self,"
   },
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/dim/dim.cpp",
    "code1": "    if (!self->is_bound()) {        py::raise_error(DimensionBindError(), \"DimList not bound\");    }    if (idx >= self->dims_.size()) {        py::raise_error(PyExc_IndexError, \"index out of bounds\");    }    py::object r = self->dims_[idx];",
    "code2": "    if (!self->is_bound()) {        py::raise_error(DimensionBindError(), \"DimList not bound\");    }    if (idx < 0 || (size_t) idx >= self->dims_.size()) {        py::raise_error(PyExc_IndexError, \"index out of bounds\");    }    py::object r = self->dims_[idx];",
    "method_name": "PyObject * DimList_item(DimList* self, Py_ssize_t idx) {"
   },
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/dim/dim.cpp",
    "code1": "PyMappingMethods DimList_mapping = {    0, //lenfunc mp_length;    (binaryfunc) DimList_subscript, //binaryfunc mp_subscript;    0, //objobjargproc mp_ass_subscript;};",
    "code2": "PyMappingMethods DimList_mapping = {    0, //lenfunc mp_length;    (binaryfunc)(void*) DimList_subscript, //binaryfunc mp_subscript;    0, //objobjargproc mp_ass_subscript;};",
    "method_name": "static PyObject* DimList_subscript(DimList* self, py::handle idx) {"
   },
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/dim/dim.cpp",
    "code1": "            std::vector<py::obj<Dim>> dims;            size_t size = s.size();            dims.reserve(size);            for (ssize_t i = 0; i < size; ++i) {                auto r = s[i];                if (py::is_int(r)) {                    dims.emplace_back(Dim::create(py::unicode_from_format(\"%S%i\", self->name_.ptr(), (int)i),  py::to_int(r)));",
    "code2": "            std::vector<py::obj<Dim>> dims;            size_t size = s.size();            dims.reserve(size);            for (size_t i = 0; i < size; ++i) {                auto r = s[i];                if (py::is_int(r)) {                    dims.emplace_back(Dim::create(py::unicode_from_format(\"%S%i\", self->name_.ptr(), (int)i),  py::to_int(r)));",
    "method_name": "static int DimList_init(DimList *self, PyObject *args, PyObject *kwds) {"
   },
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/dim/dim.cpp",
    "code1": "    PY_END(nullptr)}PyMethodDef py_unflatten_def = {\"unflatten\", (PyCFunction) py_unflatten, METH_FASTCALL | METH_KEYWORDS};void free_unflatten_arena(PyObject * pc) {    delete (UnflattenArena*) PyCapsule_GetPointer(pc, \"arena\");",
    "code2": "    PY_END(nullptr)}PyMethodDef py_unflatten_def = {\"unflatten\", (PyCFunction)(void*) py_unflatten, METH_FASTCALL | METH_KEYWORDS};void free_unflatten_arena(PyObject * pc) {    delete (UnflattenArena*) PyCapsule_GetPointer(pc, \"arena\");",
    "method_name": "static PyObject* py_unflatten(PyObject *self,"
   },
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/dim/dim.cpp",
    "code1": "    if (!py::is_none(size)) {        d->set_size(py::to_int(size));    }    return d;}py::object create_dimlist(py::object name, py::handle size) {",
    "code2": "    if (!py::is_none(size)) {        d->set_size(py::to_int(size));    }    return std::move(d);}py::object create_dimlist(py::object name, py::handle size) {",
    "method_name": "py::object create_dim(py::object name, py::handle size) {"
   },
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/dim/dim.cpp",
    "code1": "            }        }    }    return d;}template<py::object (*create_object)(py::object, py::handle)>",
    "code2": "            }        }    }    return std::move(d);}template<py::object (*create_object)(py::object, py::handle)>",
    "method_name": "py::object create_dimlist(py::object name, py::handle size) {"
   },
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/dim/dim.cpp",
    "code1": "    if (lhs_list && rhs_list) {        py::sequence_view dv(dims);        py::sequence_view ind(indices); size_t N = dv.size();        if (N != ind.size()) {            py::raise_error(PyExc_TypeError, \"dims (%d) and indices (%d) must have the same length\", int(N), int(ind.size()));        }",
    "code2": "    if (lhs_list && rhs_list) {        py::sequence_view dv(dims);        py::sequence_view ind(indices); Py_ssize_t N = dv.size();        if (N != ind.size()) {            py::raise_error(PyExc_TypeError, \"dims (%d) and indices (%d) must have the same length\", int(N), int(ind.size()));        }",
    "method_name": "static py::object index(Arena& A, py::handle self, py::handle dims, py::handle i"
   },
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/dim/dim.cpp",
    "code1": "    } else {        dim_name_str = \"dim\";    }    auto info = WrappedOperator::create(py::object::borrow(orig), (PyCFunction) patched_dim_method, std::move(dim_name_str));    if (dim_offset.ptr()) {        info->dim_offset = py::to_int(dim_offset);    }",
    "code2": "    } else {        dim_name_str = \"dim\";    }    auto info = WrappedOperator::create(py::object::borrow(orig), (PyCFunction)(void*) patched_dim_method, std::move(dim_name_str));    if (dim_offset.ptr()) {        info->dim_offset = py::to_int(dim_offset);    }",
    "method_name": "static PyObject* _wrap(PyObject * self_,"
   },
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/dim/dim.cpp",
    "code1": "        auto dim = py::import(\"functorch.dim\");        pointwise = dim.attr(\"pointwise\");    }    auto info = WrappedOperator::create(py::object::borrow(orig), (PyCFunction) call_torch_function);    info->is_pointwise = pointwise.contains(orig);    return PyInstanceMethod_New(info->function().release());    PY_END(nullptr);",
    "code2": "        auto dim = py::import(\"functorch.dim\");        pointwise = dim.attr(\"pointwise\");    }    auto info = WrappedOperator::create(py::object::borrow(orig), (PyCFunction)(void*) call_torch_function);    info->is_pointwise = pointwise.contains(orig);    return PyInstanceMethod_New(info->function().release());    PY_END(nullptr);",
    "method_name": "static PyObject* _wrap_method(PyObject *self,"
   },
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/dim/dim.cpp",
    "code1": ")\"\"\";static PyMethodDef methods[] = {    {\"dims\", (PyCFunction) _dims<create_dim>, METH_FASTCALL | METH_KEYWORDS, dims_doc},    {\"dimlists\", (PyCFunction) _dims<create_dimlist>, METH_FASTCALL | METH_KEYWORDS},    {\"_test_c\", (PyCFunction) test_c, METH_FASTCALL | METH_KEYWORDS},    {\"_wrap_method\", (PyCFunction) _wrap_method, METH_FASTCALL | METH_KEYWORDS},    {\"Tensor_from_positional\", (PyCFunction) py_Tensor_from_positional, METH_FASTCALL | METH_KEYWORDS},    {\"__torch_function__\", (PyCFunction) py___torch_function__, METH_FASTCALL | METH_KEYWORDS},    {\"tree_flatten\", (PyCFunction) py_tree_flatten, METH_FASTCALL | METH_KEYWORDS},    {\"order\", (PyCFunction) order, METH_FASTCALL | METH_KEYWORDS},    {\"index\", (PyCFunction) py_index, METH_FASTCALL | METH_KEYWORDS},    {\"stack\", (PyCFunction) py_stack, METH_FASTCALL | METH_KEYWORDS},    {\"split\", (PyCFunction) py_split, METH_FASTCALL | METH_KEYWORDS},    {\"expand\", (PyCFunction) expand, METH_FASTCALL | METH_KEYWORDS},    {\"__getitem__\", (PyCFunction) py___getitem__, METH_FASTCALL | METH_KEYWORDS},    {\"__setitem__\", (PyCFunction) py___setitem__, METH_FASTCALL | METH_KEYWORDS},    {\"_wrap\", (PyCFunction) _wrap, METH_FASTCALL | METH_KEYWORDS},    {\"Tensor_sum\", (PyCFunction) Tensor_sum, METH_FASTCALL | METH_KEYWORDS},    {\"_parse_test\", (PyCFunction) _parse_test, METH_FASTCALL | METH_KEYWORDS},    {\"_set_pointwise_optimize\", (PyCFunction) _set_pointwise_optimize, METH_FASTCALL | METH_KEYWORDS},    {\"_patch_tensor_class\", (PyCFunction) _patch_tensor_class, METH_FASTCALL | METH_KEYWORDS},    {NULL, NULL, 0, NULL}        /* Sentinel */};",
    "code2": ")\"\"\";static PyMethodDef methods[] = {    {\"dims\", (PyCFunction)(void*) _dims<create_dim>, METH_FASTCALL | METH_KEYWORDS, dims_doc},    {\"dimlists\", (PyCFunction)(void*) _dims<create_dimlist>, METH_FASTCALL | METH_KEYWORDS},    {\"_test_c\", (PyCFunction)(void*) test_c, METH_FASTCALL | METH_KEYWORDS},    {\"_wrap_method\", (PyCFunction)(void*) _wrap_method, METH_FASTCALL | METH_KEYWORDS},    {\"Tensor_from_positional\", (PyCFunction)(void*) py_Tensor_from_positional, METH_FASTCALL | METH_KEYWORDS},    {\"__torch_function__\", (PyCFunction)(void*) py___torch_function__, METH_FASTCALL | METH_KEYWORDS},    {\"tree_flatten\", (PyCFunction)(void*) py_tree_flatten, METH_FASTCALL | METH_KEYWORDS},    {\"order\", (PyCFunction)(void*) order, METH_FASTCALL | METH_KEYWORDS},    {\"index\", (PyCFunction)(void*) py_index, METH_FASTCALL | METH_KEYWORDS},    {\"stack\", (PyCFunction)(void*) py_stack, METH_FASTCALL | METH_KEYWORDS},    {\"split\", (PyCFunction)(void*) py_split, METH_FASTCALL | METH_KEYWORDS},    {\"expand\", (PyCFunction)(void*) expand, METH_FASTCALL | METH_KEYWORDS},    {\"__getitem__\", (PyCFunction)(void*) py___getitem__, METH_FASTCALL | METH_KEYWORDS},    {\"__setitem__\", (PyCFunction)(void*) py___setitem__, METH_FASTCALL | METH_KEYWORDS},    {\"_wrap\", (PyCFunction)(void*) _wrap, METH_FASTCALL | METH_KEYWORDS},    {\"Tensor_sum\", (PyCFunction)(void*) Tensor_sum, METH_FASTCALL | METH_KEYWORDS},    {\"_parse_test\", (PyCFunction)(void*) _parse_test, METH_FASTCALL | METH_KEYWORDS},    {\"_set_pointwise_optimize\", (PyCFunction)(void*) _set_pointwise_optimize, METH_FASTCALL | METH_KEYWORDS},    {\"_patch_tensor_class\", (PyCFunction)(void*) _patch_tensor_class, METH_FASTCALL | METH_KEYWORDS},    {NULL, NULL, 0, NULL}        /* Sentinel */};",
    "method_name": "Example::"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83457,
  "title": "fix native_layer_norm meta kernel parity w cuda",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-16T23:38:07Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/layer_norm.cpp",
    "code1": "      at::empty_like(input, c10::TensorOptions().dtype(result_type))    );  }  at::Tensor input_reshaped = input.view({1, M, -1});  // Unlike Batch Normalization, which applies scalar scale and bias for each  // entire channel/plane with the affine option, Layer Normalization applies  // per-element scale and bias. E.g. For input {N, C, H, W}, weight for",
    "code2": "      at::empty_like(input, c10::TensorOptions().dtype(result_type))    );  }  at::Tensor input_reshaped = input.reshape({1, M, -1});  // Unlike Batch Normalization, which applies scalar scale and bias for each  // entire channel/plane with the affine option, Layer Normalization applies  // per-element scale and bias. E.g. For input {N, C, H, W}, weight for",
    "method_name": "std::tuple<Tensor, Tensor, Tensor> math_native_layer_norm("
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83432,
  "title": "[vulkan][fix] Fix unsafe direct array access",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-16T17:30:57Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/ops/Utils.cpp",
    "code1": "      },  };  api::UniformParamsBuffer params(context, block);  api::PipelineBarrier pipeline_barrier{};  bool is_quantized = v_self.is_quantized();  api::utils::uvec3 copy_extents;  copy_extents.data[0u] = 1;  copy_extents.data[1u] = 1;  copy_extents.data[2u] =      ((v_self.sizes()[1] * v_self.sizes()[2] * v_self.sizes()[3]) / 4);  api::ShaderSource kernel = is_quantized ? VK_KERNEL(image_to_nchw_quantized)                                          : VK_KERNEL(image_to_nchw);  api::utils::uvec3 extents_to_use = is_quantized ? copy_extents : extents;  context->submit_compute_job(      // shader descriptor      kernel,      // pipeline barrier      pipeline_barrier,      // global work group size extents_to_use,      // local work group size      adaptive_work_group_size(extents_to_use),      // fence handle      fence_handle,      // shader arguments",
    "code2": "      },  };  bool is_quantized = v_self.is_quantized();  api::utils::uvec3 pack_extents = extents;  if (is_quantized) {    pack_extents.data[0u] = 1;    pack_extents.data[1u] = 1;    pack_extents.data[2u] =        api::utils::safe_downcast<uint32_t>(v_self.numtexels());  }  api::UniformParamsBuffer params(context, block);  api::PipelineBarrier pipeline_barrier{};  api::ShaderSource kernel = is_quantized ? VK_KERNEL(image_to_nchw_quantized)                                          : VK_KERNEL(image_to_nchw);  context->submit_compute_job(      // shader descriptor      kernel,      // pipeline barrier      pipeline_barrier,      // global work group size pack_extents,      // local work group size      adaptive_work_group_size(pack_extents),      // fence handle      fence_handle,      // shader arguments",
    "method_name": "void pack_vtensor_to_staging("
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83424,
  "title": "shard trunk / linux-bionic-cuda10.2-py3.9-gcc7 / test (default from 2 -> 4",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-15T20:03:13Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83395,
  "title": "Fix issue with compiling under with_grad",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-15T16:08:50Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "functorch/functorch/_src/aot_autograd.py",
    "code1": " if config.debug_graphs: print(fw_module.code, bw_module.code) with track_graph_compiling(\"forward\"): compiled_fw = aot_config.fw_compiler(fw_module, flat_args) # TODO: Delay this backwards compilation until the backwards pass with torch.no_grad(): fw_outs = call_func_with_args(compiled_fw, flat_args) if config.debug_partitioner: activation_sizes = 0 for out in fw_outs[num_outs:]: if isinstance(out, torch.Tensor): activation_sizes += out.storage().nbytes() print(f\"Real Activations Stored(GB): {activation_sizes/1e9}\") bw_args = fw_outs[num_outs:] + fw_outs[0:num_outs] with track_graph_compiling(\"backward\", True): compiled_bw = aot_config.bw_compiler(bw_module, bw_args) class CompiledFunction(torch.autograd.Function): @staticmethod",
    "code2": " if config.debug_graphs: print(fw_module.code, bw_module.code) with torch.no_grad(): with track_graph_compiling(\"forward\"): compiled_fw = aot_config.fw_compiler(fw_module, flat_args) # TODO: Delay this backwards compilation until the backwards pass with torch.no_grad(): fw_outs = call_func_with_args(compiled_fw, flat_args) if config.debug_partitioner: activation_sizes = 0 for out in fw_outs[num_outs:]: if isinstance(out, torch.Tensor): activation_sizes += out.storage().nbytes() print(f\"Real Activations Stored(GB): {activation_sizes/1e9}\") bw_args = fw_outs[num_outs:] + fw_outs[0:num_outs] with track_graph_compiling(\"backward\", True): compiled_bw = aot_config.bw_compiler(bw_module, bw_args) class CompiledFunction(torch.autograd.Function): @staticmethod",
    "method_name": "def fake_fn(primals, tangents):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83391,
  "title": "[fix] cat : support different dtype tensor with 0-dim like before",
  "tags": [
   "cla signed",
   "Merged",
   "open source",
   "triaged"
  ],
  "closed_time": "2022-08-15T14:32:01Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/TensorShape.cpp",
    "code1": "    size_t size_at_dim = 0;    for (const auto i : c10::irange(materialized.size())) {      const Tensor& t = materialized[i];      if (!at::native::cat_should_skip_tensor(t)) {        at::native::check_cat_shape_except_dim(materialized[valid], t, dim, i);        size_at_dim += t.size(dim);        all_contiguous = all_contiguous && t.is_contiguous(memory_format);        all_same_dtype = all_same_dtype && out_dtype == t.scalar_type();        all_same_sizes_and_stride = all_same_sizes_and_stride &&            t.sizes() == materialized[valid].get().sizes() &&            t.strides() == materialized[valid].get().strides();",
    "code2": "    size_t size_at_dim = 0;    for (const auto i : c10::irange(materialized.size())) {      const Tensor& t = materialized[i];      all_same_dtype = all_same_dtype && out_dtype == t.scalar_type();      if (!at::native::cat_should_skip_tensor(t)) {        at::native::check_cat_shape_except_dim(materialized[valid], t, dim, i);        size_at_dim += t.size(dim);        all_contiguous = all_contiguous && t.is_contiguous(memory_format);        all_same_sizes_and_stride = all_same_sizes_and_stride &&            t.sizes() == materialized[valid].get().sizes() &&            t.strides() == materialized[valid].get().strides();",
    "method_name": "TORCH_PRECOMPUTE_META_FUNC(cat)(ITensorListRef tensors, int64_t dim) {"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83377,
  "title": "Grammatically docs fix",
  "tags": [
   "cla signed",
   "open source"
  ],
  "closed_time": "2022-08-14T05:04:38Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "torch/_lobpcg.py",
    "code1": "def _symeig_backward_complete_eigenspace(D_grad, U_grad, A, D, U):    # compute F, such that F_ij = (d_j - d_i)^{-1} for i != j, F_ii = 0    F = D.unsqueeze(-2) - D.unsqueeze(-1)    F.diagonal(dim1=-2, dim2=-1).fill_(float(\"inf\"))    F.pow_(-1)",
    "code2": "def _symeig_backward_complete_eigenspace(D_grad, U_grad, A, D, U):    # Compute F, such that F_ij = (d_j - d_i)^{-1} for i != j, F_ii = 0    F = D.unsqueeze(-2) - D.unsqueeze(-1)    F.diagonal(dim1=-2, dim2=-1).fill_(float(\"inf\"))    F.pow_(-1)",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "torch/_lobpcg.py",
    "code1": "    proj_U_ortho = -U.matmul(Ut)    proj_U_ortho.diagonal(dim1=-2, dim2=-1).add_(1)    # compute U_ortho, a basis for the orthogonal complement to the span(U),    # by projecting a random [..., m, m - k] matrix onto the subspace spanned    # by the columns of U.    #",
    "code2": "    proj_U_ortho = -U.matmul(Ut)    proj_U_ortho.diagonal(dim1=-2, dim2=-1).add_(1)    # Compute U_ortho, a basis for the orthogonal complement to the span(U),    # by projecting a random [..., m, m - k] matrix onto the subspace spanned    # by the columns of U.    #",
    "method_name": "def _symeig_backward_partial_eigenspace(D_grad, U_grad, A, D, U, largest):"
   },
   {
    "language": ".py",
    "dir": "torch/_lobpcg.py",
    "code1": "        series_acc += U_grad_projected * poly_D.unsqueeze(-2)        U_grad_projected = A.matmul(U_grad_projected)    # compute chr_poly_D(A) which essentially is:    #    # chr_poly_D_at_A = A.new_zeros(A.shape)    # for k in range(chr_poly_D.size(-1)):",
    "code2": "        series_acc += U_grad_projected * poly_D.unsqueeze(-2)        U_grad_projected = A.matmul(U_grad_projected)    # Compute chr_poly_D(A) which essentially is:    #    # chr_poly_D_at_A = A.new_zeros(A.shape)    # for k in range(chr_poly_D.size(-1)):",
    "method_name": "def _symeig_backward_partial_eigenspace(D_grad, U_grad, A, D, U, largest):"
   },
   {
    "language": ".py",
    "dir": "torch/_lobpcg.py",
    "code1": "def _symeig_backward(D_grad, U_grad, A, D, U, largest):    # if `U` is square, then the columns of `U` is a complete eigenspace    if U.size(-1) == U.size(-2):        return _symeig_backward_complete_eigenspace(D_grad, U_grad, A, D, U)    else:",
    "code2": "def _symeig_backward(D_grad, U_grad, A, D, U, largest):    # If `U` is square, then the columns of `U` is a complete eigenspace    if U.size(-1) == U.size(-2):        return _symeig_backward_complete_eigenspace(D_grad, U_grad, A, D, U)    else:",
    "method_name": "def _symeig_backward_partial_eigenspace(D_grad, U_grad, A, D, U, largest):"
   },
   {
    "language": ".py",
    "dir": "torch/_lobpcg.py",
    "code1": "        if largest is None:            largest = True        # symeig backward        if B is None:            A_grad = _symeig_backward(D_grad, U_grad, A, D, U, largest)",
    "code2": "        if largest is None:            largest = True        # Symeig backward        if B is None:            A_grad = _symeig_backward(D_grad, U_grad, A, D, U, largest)",
    "method_name": "def backward(ctx, D_grad, U_grad):"
   },
   {
    "language": ".py",
    "dir": "torch/_lobpcg.py",
    "code1": "      not recommended but there exist cases where the usage of the      basic method may be preferred.    .. warning:: The backward method does not support sparse and complex inputs.      It works only when `B` is not provided (i.e. `B == None`).      We are actively working on extensions, and the details of      the algorithms are going to be published promptly.",
    "code2": "      not recommended but there exist cases where the usage of the      basic method may be preferred.    .. arning:: The backward method does not support sparse and complex inputs.      It works only when `B` is not provided (i.e. `B == None`).      We are actively working on extensions, and the details of      the algorithms are going to be published promptly.",
    "method_name": "def lobpcg("
   },
   {
    "language": ".py",
    "dir": "torch/_torch_docs.py",
    "code1": "common_args = parse_kwargs(    \"\"\" input (Tensor): the input tensor.    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling    out (Tensor, optional): the output tensor.    memory_format (:class:`torch.memory_format`, optional): the desired memory format of",
    "code2": "common_args = parse_kwargs(    \"\"\" Input (Tensor): the input tensor.    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling    out (Tensor, optional): the output tensor.    memory_format (:class:`torch.memory_format`, optional): the desired memory format of",
    "method_name": "def merge_dicts(*dicts):"
   },
   {
    "language": ".py",
    "dir": "torch/_utils_internal.py",
    "code1": "import tempfile# this arbitrary-looking assortment of functionality is provided here# to have a central place for overrideable behavior. The motivating# use is the FB build environment, where this source file is replaced# by an equivalent.",
    "code2": "import tempfile# This arbitrary-looking assortment of functionality is provided here# to have a central place for overrideable behavior. The motivating# use is the FB build environment, where this source file is replaced# by an equivalent.",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "torch/hub.py",
    "code1": "from urllib.parse import urlparse  # noqa: F401try:    from tqdm.auto import tqdm  # automatically select proper tqdm submodule if availableexcept ImportError:    try:        from tqdm import tqdm    except ImportError:        # fake tqdm if it's not installed        class tqdm(object):  # type: ignore[no-redef]            def __init__(self, total=None, disable=False,                         unit=None, unit_scale=None, unit_divisor=None):                self.total = total                self.disable = disable                self.n = 0                # ignore unit, unit_scale, unit_divisor; they're just for real tqdm            def update(self, n):                if self.disable:",
    "code2": "from urllib.parse import urlparse  # noqa: F401try:    from tqdm.auto import tqdm  # Automatically select proper tqdm submodule if availableexcept ImportError:    try:        from tqdm import tqdm    except ImportError:        # Fake tqdm if it's not installed        class tqdm(object):  # type: ignore[no-redef]            def __init__(self, total=None, disable=False,                         unit=None, unit_scale=None, unit_divisor=None):                self.total = total                self.disable = disable                self.n = 0                # Ignore unit, unit_scale, unit_divisor; they're just for real tqdm            def update(self, n):                if self.disable:",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "torch/hub.py",
    "code1": "    'set_dir',]# matches bfd8deac from resnet18-bfd8deac.pthHASH_REGEX = re.compile(r'-([a-f0-9]*)\\.')_TRUSTED_REPO_OWNERS = (\"facebookresearch\", \"facebookincubator\", \"pytorch\", \"fairinternal\")",
    "code2": "    'set_dir',]# Matches bfd8deac from resnet18-bfd8deac.pthHASH_REGEX = re.compile(r'-([a-f0-9]*)\\.')_TRUSTED_REPO_OWNERS = (\"facebookresearch\", \"facebookincubator\", \"pytorch\", \"fairinternal\")",
    "method_name": "def __exit__(self, exc_type, exc_val, exc_tb):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83341,
  "title": "fix quantization/core/test_docs for Buck2",
  "tags": [
   "cla signed",
   "fb-exported",
   "Merged"
  ],
  "closed_time": "2022-08-18T13:03:05Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83339,
  "title": "fix resnet50_quantized_qat and mobilenet_v2_quantized_qat <> functionalization",
  "tags": [
   "cla signed"
  ],
  "closed_time": "2022-08-16T23:38:06Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "functorch/functorch/_src/aot_autograd.py",
    "code1": "    with preserve_rng_state(), fake_mode as mode:        def process_inputs(flat_args):            flat_args = pytree.tree_map(                lambda x: x.detach().requires_grad_(x.requires_grad)                if isinstance(x, Tensor)                else x,                flat_args,            )            fake_flat_tensor_args = pytree.tree_map(                lambda x: mode.from_tensor(x)                if mode                else x                if isinstance(x, Tensor)                else x,                flat_args,            )            return fake_flat_tensor_args        fake_flat_tensor_args = process_inputs(flat_args)",
    "code2": "    with preserve_rng_state(), fake_mode as mode:        def process_inputs(flat_args):            if mode:                fake_flat_tensor_args = pytree.tree_map_only(                    Tensor, mode.from_tensor, flat_args                )            else:                # The detach().requires_grad_() pattern can cause some subtle bugs.                # These will be fixed once FakeTensor is always-on for AOTAutograd.                #                # For models that might resize their inputs, the input tensors                # must have allow_tensor_metadata_change() set to true.                # detach() returns a view tensor, but with that field set to false.                #                # Specifically, this breaks quantized models                # (resnet50_quantized_qat and mobilenet_v2_quantized_qat)                # because they use a \"running-mean\" style op that requires                # resizing the running counter buffers stored on the module.                fake_flat_tensor_args = pytree.tree_map_only(                    Tensor,                    lambda x: x.detach().requires_grad_(x.requires_grad),                    flat_args,                )            return fake_flat_tensor_args        fake_flat_tensor_args = process_inputs(flat_args)",
    "method_name": "def create_aot_dispatcher_function("
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83337,
  "title": "fix a comment since the options in arg parser no longer require Declarations.yaml",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-12T21:10:44Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tools/autograd/gen_autograd.py",
    "code1": "repository, run:python -m tools.autograd.gen_autograd \\       build/aten/src/ATen/Declarations.yaml \\       aten/src/ATen/native/native_functions.yaml \\       aten/src/ATen/native/tags.yaml \\       $OUTPUT_DIR \\",
    "code2": "repository, run:python -m tools.autograd.gen_autograd \\       aten/src/ATen/native/native_functions.yaml \\       aten/src/ATen/native/tags.yaml \\       $OUTPUT_DIR \\",
    "method_name": ""
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83323,
  "title": "fix quantization/core/test_docs for Buck2",
  "tags": [
   "cla signed"
  ],
  "closed_time": "2022-08-13T15:35:36Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83284,
  "title": "[ONNX] Update the default opset to version 14",
  "tags": [
   "cla signed",
   "Merged",
   "module: onnx",
   "onnx-needs-import",
   "open source",
   "release notes: onnx",
   "topic: bc_breaking",
   "topic: improvements"
  ],
  "closed_time": "2022-08-18T19:13:41Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83275,
  "title": "Fix building with Werror",
  "tags": [
   "cla signed",
   "Merged",
   "module: build warnings",
   "open source",
   "topic: not user facing"
  ],
  "closed_time": "2022-08-12T22:36:22Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "c10/cuda/CUDACachingAllocator.cpp",
    "code1": "    BlockPool& pool = *p.pool;    // because of std::unique_ptr, block cannot be trivially copied    Block key(0, 0, 0);    std::memcpy(&key, &p.search_key, sizeof(Block));    key.history.release();    key.size = (key.size < CachingAllocatorConfig::max_split_size())        ? CachingAllocatorConfig::max_split_size()        : key.size;",
    "code2": "    BlockPool& pool = *p.pool;    // because of std::unique_ptr, block cannot be trivially copied    Block key(        p.search_key.device,        p.search_key.stream,        p.search_key.size,        p.search_key.pool,        p.search_key.ptr);    key.size = (key.size < CachingAllocatorConfig::max_split_size())        ? CachingAllocatorConfig::max_split_size()        : key.size;",
    "method_name": "class DeviceCachingAllocator {"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83273,
  "title": "Fix issues with Werror=range-loop-construct",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-11T17:45:21Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83269,
  "title": "[JIT Test] Add more debugging information for JIT opinfo tests",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-12T00:29:10Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83217,
  "title": "Fix lint errors",
  "tags": [
   "cla signed",
   "fx"
  ],
  "closed_time": "2022-08-10T23:06:04Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "torch/ao/quantization/backend_config/_common_operator_config_utils.py",
    "code1": "    linear_configs.append(        BackendPatternConfig(torch.nn.functional.linear)            .set_observation_type(observation_type)  # noqa: E131            .set_dtype_configs(dtype_configs))            ._set_input_type_to_index({\"weight\": 1, \"bias\": 2})    # (2) Linear + relu    # -------------------",
    "code2": "    linear_configs.append(        BackendPatternConfig(torch.nn.functional.linear)            .set_observation_type(observation_type)  # noqa: E131            .set_dtype_configs(dtype_configs)            ._set_input_type_to_index({\"weight\": 1, \"bias\": 2}))    # (2) Linear + relu    # -------------------",
    "method_name": "def _get_linear_configs(dtype_configs: List[DTypeConfig]) -> List[BackendPattern"
   },
   {
    "language": ".py",
    "dir": "torch/ao/quantization/backend_config/_common_operator_config_utils.py",
    "code1": "        conv_configs.append(            BackendPatternConfig(convs.func)                .set_observation_type(observation_type)  # noqa: E131                .set_dtype_configs(dtype_configs))                ._set_input_type_to_index()                ._set_input_type_to_index({\"weight\": 1, \"bias\": 2})        # (2) Conv + relu        # -----------------",
    "code2": "        conv_configs.append(            BackendPatternConfig(convs.func)                .set_observation_type(observation_type)  # noqa: E131                .set_dtype_configs(dtype_configs)                ._set_input_type_to_index({\"weight\": 1, \"bias\": 2}))        # (2) Conv + relu        # -----------------",
    "method_name": "def _get_conv_configs(dtype_configs):"
   },
   {
    "language": ".py",
    "dir": "torch/ao/quantization/fx/prepare.py",
    "code1": "    pattern_to_input_type_to_index = get_pattern_to_input_type_to_index(backend_config)    for pattern, input_type_to_index in pattern_to_input_type_to_index.items():        for input_type, index in input_type_to_index.items():            index_dicts = {                \"weight\": {},                \"bias\": {},                \"input\": {}  # not used right now",
    "code2": "    pattern_to_input_type_to_index = get_pattern_to_input_type_to_index(backend_config)    for pattern, input_type_to_index in pattern_to_input_type_to_index.items():        for input_type, index in input_type_to_index.items():            index_dicts = {                \"weight\": {},                \"bias\": {},                \"input\": {}  # not used right now",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "torch/ao/quantization/fx/prepare.py",
    "code1": "    if isinstance(node, Node) and node.op == 'call_function' and \\            node.target in weight_index_dict:        for i, node_arg in enumerate(node.args):            if arg is node_arg and i in \\                    weight_index_dict[node.target]:  # type: ignore[index]                return True        for kwarg_name, kwarg_value in node.kwargs.items():",
    "code2": "    if isinstance(node, Node) and node.op == 'call_function' and \\            node.target in weight_index_dict:        for i, node_arg in enumerate(node.args):            if arg is node_arg and i in \\                    weight_index_dict[node.target]:  # type: ignore[index]                return True        for kwarg_name, kwarg_value in node.kwargs.items():",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "torch/ao/quantization/fx/prepare.py",
    "code1": "        return False    for i, node_arg in enumerate(node.args):        if arg is node_arg and i in \\           bias_index_dict[node.target]:  # type: ignore[index]            return True",
    "code2": "        return False    for i, node_arg in enumerate(node.args):        if arg is node_arg and i in \\           bias_index_dict[node.target]:  # type: ignore[index]            return True",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "torch/ao/quantization/fx/prepare.py",
    "code1": "    is supported by the backend or not    \"\"\"    if isinstance(arg, (list, tuple)):        return all(map(lambda a: is_input_arg_dtype_supported_by_backend(a, node, node_name_to_target_dtype, dtype_config, weight_index_dict, bias_index_dict), arg))    if not isinstance(arg, Node):        return True    # TODO: support check for standalone module",
    "code2": "    is supported by the backend or not    \"\"\"    if isinstance(arg, (list, tuple)):        return all(map(lambda a: is_input_arg_dtype_supported_by_backend(a, node, node_name_to_target_dtype, dtype_config, weight_index_dict, bias_index_dict), arg))    if not isinstance(arg, Node):        return True    # TODO: support check for standalone module",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "torch/ao/quantization/fx/prepare.py",
    "code1": "        for arg in input_node.args:            supported = supported and \\                is_input_arg_dtype_supported_by_backend(                    arg, input_node, node_name_to_target_dtype, dtype_config, weight_index_dict, bias_index_dict)        for k, arg in input_node.kwargs.items():            supported = supported and \\                is_input_arg_dtype_supported_by_backend(                    arg, input_node, node_name_to_target_dtype, dtype_config)        # check if output dtype is supported        supported = supported and is_output_dtype_supported_by_backend(",
    "code2": "        for arg in input_node.args:            supported = supported and \\                is_input_arg_dtype_supported_by_backend(                    arg, input_node, node_name_to_target_dtype, dtype_config, weight_index_dict, bias_index_dict)        for k, arg in input_node.kwargs.items():            supported = supported and \\                is_input_arg_dtype_supported_by_backend(                    arg, input_node, node_name_to_target_dtype, dtype_config)        # check if output dtype is supported        supported = supported and is_output_dtype_supported_by_backend(",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "torch/ao/quantization/fx/prepare.py",
    "code1": "    if isinstance(arg, (list, tuple)):        new_arg_to_return = []        for inner_arg in arg:            new_inner_arg = maybe_insert_input_observer_for_arg_or_kwarg(                node, inner_arg, qconfig, model, modules,                graph, node_name_to_target_dtype,                qhandler,",
    "code2": "    if isinstance(arg, (list, tuple)):        new_arg_to_return = []        for inner_arg in arg:            new_inner_arg = maybe_insert_input_observer_for_arg_or_kwarg(                node, inner_arg, qconfig, model, modules,                graph, node_name_to_target_dtype,                qhandler,",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "torch/ao/quantization/fx/prepare.py",
    "code1": "            qconfig.activation        arg_as_output_target_dtype = get_arg_target_dtype_as_output(arg, modules, node_name_to_target_dtype)        arg_as_input_target_dtype = get_arg_target_dtype_as_input_to_node(arg, node, modules, node_name_to_target_dtype, weight_index_dict, bias_index_dict)        arg_as_input_target_compute_dtype = \\            get_arg_target_compute_dtype_as_input_to_node(                arg, node, modules, node_name_to_target_dtype, weight_index_dict, bias_index_dict)",
    "code2": "            qconfig.activation        arg_as_output_target_dtype = get_arg_target_dtype_as_output(arg, modules, node_name_to_target_dtype)        arg_as_input_target_dtype = get_arg_target_dtype_as_input_to_node(arg, node, modules, node_name_to_target_dtype, weight_index_dict, bias_index_dict)        arg_as_input_target_compute_dtype = \\            get_arg_target_compute_dtype_as_input_to_node(                arg, node, modules, node_name_to_target_dtype, weight_index_dict, bias_index_dict)",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "torch/ao/quantization/fx/prepare.py",
    "code1": "            node_name_to_target_dtype,            qhandler,            prepare_custom_config,            backend_config            weight_index_dict,            bias_index_dict,)        new_kwargs[k] = new_kwarg",
    "code2": "            node_name_to_target_dtype,            qhandler,            prepare_custom_config,            backend_config,            weight_index_dict,            bias_index_dict,)        new_kwargs[k] = new_kwarg",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "torch/ao/quantization/fx/prepare.py",
    "code1": "    custom_module_classes = get_custom_module_class_keys(prepare_custom_config.float_to_observed_mapping)    matches_without_qconfig = find_matches(        model.graph, modules, patterns, root_node_getter_mapping,        standalone_module_names, standalone_module_classes, custom_module_classes)    # map qconfig instances to matches",
    "code2": "    custom_module_classes = get_custom_module_class_keys(prepare_custom_config.float_to_observed_mapping)    matches_without_qconfig = find_matches(        model.graph, modules, patterns, root_node_getter_mapping,        standalone_module_names, standalone_module_classes, custom_module_classes)    # map qconfig instances to matches",
    "method_name": ""
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83207,
  "title": "fix functionalization <> resnet18, make ProxyTensor work with tensor-less decomps",
  "tags": [
   "cla signed",
   "fx",
   "Merged"
  ],
  "closed_time": "2022-08-12T01:07:35Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "torch/fx/experimental/proxy_tensor.py",
    "code1": "    func = func_overload.overloadpacket    if func_overload in CURRENT_DECOMPOSITION_TABLE:        return CURRENT_DECOMPOSITION_TABLE[func_overload](*args, **kwargs)    with proxy_mode.restore():        r = func_overload.decompose(*args, **kwargs)        if r is not NotImplemented:",
    "code2": "    func = func_overload.overloadpacket    if func_overload in CURRENT_DECOMPOSITION_TABLE:        with proxy_mode.restore():            return CURRENT_DECOMPOSITION_TABLE[func_overload](*args, **kwargs)    with proxy_mode.restore():        r = func_overload.decompose(*args, **kwargs)        if r is not NotImplemented:",
    "method_name": "def proxy_call(proxy_mode, func_overload, args, kwargs=None):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83206,
  "title": "fix resnet18 <> functionalization, remove problematic decomp",
  "tags": [
   "cla signed"
  ],
  "closed_time": "2022-08-10T22:13:06Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "functorch/functorch/_src/aot_autograd.py",
    "code1": "    return aten.view(x, shape)@register_decomposition(aten.new_zeros, aot_autograd_decompositions)def new_zeros(inp, size, dtype=None, layout=None, device=None, pin_memory=None):    return torch.zeros(size, dtype=inp.dtype, device=inp.device)@register_decomposition(aten.new_full, aot_autograd_decompositions)def new_full(inp, size, value, dtype=None, layout=None, device=None, pin_memory=None):    return torch.full(size, value, dtype=inp.dtype, device=inp.device)",
    "code2": "    return aten.view(x, shape)@register_decomposition(aten.new_full, aot_autograd_decompositions)def new_full(inp, size, value, dtype=None, layout=None, device=None, pin_memory=None):    return torch.full(size, value, dtype=inp.dtype, device=inp.device)",
    "method_name": "def _reshape_alias(x, shape, strides):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83198,
  "title": "[primTorch] Fix off by 1 in canonicalize_dim",
  "tags": [
   "bug",
   "cla signed",
   "Merged",
   "module: primTorch",
   "open source",
   "topic: bug fixes"
  ],
  "closed_time": "2022-08-16T17:57:14Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "c10/core/WrapDimMinimal.cpp",
    "code1": "    int64_t dim,    int64_t dim_post_expr,    bool wrap_scalar) {  if (dim_post_expr <= 0) {    TORCH_CHECK_INDEX(        wrap_scalar,        \"dimension specified as \",        dim,        \" but tensor has no dimensions\");    return c10::maybe_wrap_dim(dim, /*dim_post_expr=*/1, /*wrap_scalar=*/false);",
    "code2": "    int64_t dim,    int64_t dim_post_expr,    bool wrap_scalar) {  TORCH_CHECK_INDEX(      dim_post_expr >= 0, \"Rank cannot be negative but got \", dim_post_expr);  if (dim_post_expr == 0) {    TORCH_CHECK_INDEX(        wrap_scalar,        \"Dimension specified as \",        dim,        \" but tensor has no dimensions\");    return c10::maybe_wrap_dim(dim, /*dim_post_expr=*/1, /*wrap_scalar=*/false);",
    "method_name": "int64_t maybe_wrap_dim_slow("
   },
   {
    "language": ".py",
    "dir": "torch/_prims/__init__.py",
    "code1": ")def expand_dims(a: TensorLikeType, dimensions: DimsSequenceType) -> TensorLikeType:    \"\"\"    Creates a view of a with a.ndim + len(dimensions) dimensions, with new    dimensions of length one at the dimensions specified by dimensions.    \"\"\"    dims = sorted(utils.canonicalize_dims(a.ndim, dimensions))  # type: ignore[arg-type]    if len(set(dims)) != len(dims):        msg = \"Received duplicate dimensions to expand in {0}\".format(str(dimensions))        raise ValueError(msg)",
    "code2": ")def expand_dims(    a: TensorLikeType, dimensions: DimsSequenceType, ndim=None) -> TensorLikeType:    \"\"\"    Creates a view of a with a.ndim + len(dimensions) dimensions, with new    dimensions of length one at the dimensions specified by dimensions.    \"\"\"    if ndim is not None:        # TODO: this is only here to support the unsqueeze ref        dims = sorted(utils.canonicalize_dims(ndim, dimensions))  # type: ignore[arg-type]    else:        dims = sorted(utils.canonicalize_dims(a.ndim, dimensions))  # type: ignore[arg-type]    if len(set(dims)) != len(dims):        msg = \"Received duplicate dimensions to expand in {0}\".format(str(dimensions))        raise ValueError(msg)",
    "method_name": "def _conj_meta(a: TensorLikeType) -> TensorLikeType:"
   },
   {
    "language": ".py",
    "dir": "torch/_prims_common/__init__.py",
    "code1": "# \"Wraps\" a dim (up to one time) for the given rank, allowing# dims to be specified using negative indicesdef canonicalize_dim(rank: int, idx: int) -> int:    # TODO: add a comment for why this is    _rank = rank if rank != 0 else 1    if idx >= 0 and idx < _rank:        return idx    if idx < 0:        _idx = idx + _rank    else:        _idx = idx    if _idx < 0 or _idx > _rank:        # Same error message as in aten/src/ATen/WrapDimUtils.h:49        msg = \"Dimension out of range (expected to be in range of [{0}, {1}], but got {2})\".format(            -rank, rank - 1, idx",
    "code2": "# \"Wraps\" a dim (up to one time) for the given rank, allowing# dims to be specified using negative indicesdef canonicalize_dim(rank: int, idx: int, wrap_scalar: bool = True) -> int:    if rank < 0:        msg = f\"Rank cannot be negative but got {rank}\"        raise IndexError(msg)    if rank == 0:        if not wrap_scalar:            msg = f\"Dimension specified as {idx} but tensor has no dimensions\"            raise IndexError(msg)        rank = 1    if idx >= 0 and idx < rank:        return idx    if idx < 0:        _idx = idx + rank    else:        _idx = idx    if _idx < 0 or _idx >= rank:        # Same error message as in aten/src/ATen/WrapDimUtils.h:49        msg = \"Dimension out of range (expected to be in range of [{0}, {1}], but got {2})\".format(            -rank, rank - 1, idx",
    "method_name": "def validate_exclusive_idx(rank: int, ex_idx: int):"
   },
   {
    "language": ".py",
    "dir": "torch/_refs/__init__.py",
    "code1": "@out_wrapper()def column_stack(tensors: TensorSequenceType) -> TensorLikeType:    aligned_tensors = tuple(        x if x.ndim > 1 else prims.expand_dims(x, list(range(x.ndim, 2)))        for x in tensors    )    return cat(aligned_tensors, 1)",
    "code2": "@out_wrapper()def column_stack(tensors: TensorSequenceType) -> TensorLikeType:    aligned_tensors = tuple(        x if x.ndim > 1 else x.reshape((x.numel(), 1)) for x in tensors    )    return cat(aligned_tensors, 1)",
    "method_name": "def cat(tensors: TensorSequenceType, dim: int = 0) -> TensorLikeType:"
   },
   {
    "language": ".py",
    "dir": "torch/_refs/__init__.py",
    "code1": "    a: TensorLikeType, k: int = 1, dims: DimsSequenceType = (0, 1)) -> TensorLikeType:    \"\"\"Reference implementation of :func:`torch.rot90`.\"\"\"    dims_ = utils.canonicalize_dims(a.ndim, dims)    # Required to silence MyPy errors    assert isinstance(dims_, (tuple, list))    dims = dims_    if len(dims) != 2:        raise RuntimeError(            f\"expected total rotation dims == 2, but got dims = {len(dims)}\"        )    if a.ndim < 2:        raise RuntimeError(f\"expected total dims >= 2, but got total dims = {a.ndim}\")    if dims[0] == dims[1]:        raise RuntimeError(            f\"expected rotation dims to be different, but got dim0 = {dims[0]} and dim1 = {dims[1]}\"",
    "code2": "    a: TensorLikeType, k: int = 1, dims: DimsSequenceType = (0, 1)) -> TensorLikeType:    \"\"\"Reference implementation of :func:`torch.rot90`.\"\"\"    if len(dims) != 2:        raise RuntimeError(            f\"expected total rotation dims == 2, but got dims = {len(dims)}\"        )    if a.ndim < 2:        raise RuntimeError(f\"expected total dims >= 2, but got total dims = {a.ndim}\")    # Do this after the initial checks to be compatible with the behavior in    # core.    dims = utils.canonicalize_dims(a.ndim, dims)    if dims[0] == dims[1]:        raise RuntimeError(            f\"expected rotation dims to be different, but got dim0 = {dims[0]} and dim1 = {dims[1]}\"",
    "method_name": "def rot90("
   },
   {
    "language": ".py",
    "dir": "torch/_refs/__init__.py",
    "code1": "def unsqueeze(a: TensorLikeType, dim: int) -> TensorLikeType:    # Note that unsqueeze canonicalizes with rank + 1 because it allows    # a new innermost dimension to be specified    dim = utils.canonicalize_dim(a.ndim + 1, dim)    return prims.expand_dims(a, (dim,))# NOTE: shape is a vararg because Tensor.reshape can be called with as",
    "code2": "def unsqueeze(a: TensorLikeType, dim: int) -> TensorLikeType:    # Note that unsqueeze canonicalizes with rank + 1 because it allows    # a new innermost dimension to be specified    ndim = a.ndim + 1    dim = utils.canonicalize_dim(ndim, dim)    return prims.expand_dims(a, (dim,), ndim=ndim)# NOTE: shape is a vararg because Tensor.reshape can be called with as",
    "method_name": "def transpose(a: TensorLikeType, dim0: int, dim1: int) -> TensorLikeType:"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83190,
  "title": "properly raise an error in torch.ops.aten.foo.decompose when foo doesnt have a composite",
  "tags": [
   "cla signed"
  ],
  "closed_time": "2022-08-10T18:39:39Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "torch/_ops.py",
    "code1": "        if torch._C._dispatch_has_kernel_for_dispatch_key(self.name, dk):            return self._op_dk(dk, *args, **kwargs)        else: return NotImplemented    @property    def overloadpacket(self):",
    "code2": "        if torch._C._dispatch_has_kernel_for_dispatch_key(self.name, dk):            return self._op_dk(dk, *args, **kwargs)        else: raise NotImplementedError    @property    def overloadpacket(self):",
    "method_name": "def decompose(self, *args, **kwargs):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83174,
  "title": "Fix hash for Tensor subclasses",
  "tags": [
   "cla signed",
   "Merged",
   "release notes: python_frontend",
   "topic: bug fixes"
  ],
  "closed_time": "2022-08-10T19:23:59Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "torch/_tensor.py",
    "code1": "        return iter(self.unbind(0))    def __hash__(self):        if has_torch_function_unary(self):            return handle_torch_function(Tensor.__hash__, (self,), self)        return id(self)    def __dir__(self):",
    "code2": "        return iter(self.unbind(0))    def __hash__(self):        # Do NOT handle __torch_function__ here as user's default        # implementation that handle most functions will most likely do it wrong.        # It can be easily overridden by defining this method on the user        # subclass if needed.        return id(self)    def __dir__(self):",
    "method_name": "def __iter__(self):"
   },
   {
    "language": ".py",
    "dir": "torch/overrides.py",
    "code1": "        Tensor.__new__,        Tensor.__class__,        Tensor.__subclasshook__,        Tensor.as_subclass,        Tensor.reinforce,        Tensor.new,",
    "code2": "        Tensor.__new__,        Tensor.__class__,        Tensor.__subclasshook__,        Tensor.__hash__,        Tensor.as_subclass,        Tensor.reinforce,        Tensor.new,",
    "method_name": "def get_ignored_functions() -> Set[Callable]:"
   },
   {
    "language": ".py",
    "dir": "torch/overrides.py",
    "code1": "        Tensor.__deepcopy__: lambda self, memo: -1,        Tensor.__int__: lambda self: -1,        Tensor.__long__: lambda self: -1,        Tensor.__hash__: lambda self: -1,        Tensor.__index__: lambda self: -1,        Tensor.__len__: lambda self: -1,        Tensor.__format__: lambda self, format_spec: -1,",
    "code2": "        Tensor.__deepcopy__: lambda self, memo: -1,        Tensor.__int__: lambda self: -1,        Tensor.__long__: lambda self: -1,        Tensor.__index__: lambda self: -1,        Tensor.__len__: lambda self: -1,        Tensor.__format__: lambda self, format_spec: -1,",
    "method_name": "def get_testing_overrides() -> Dict[Callable, Callable]:"
   }
  ]
 }
]