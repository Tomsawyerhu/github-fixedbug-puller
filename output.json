[
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57190,
  "title": "Fixing typos",
  "tags": [
   "awaiting review",
   "ready to pull",
   "size:XS",
   "type:docs-bug"
  ],
  "closed_time": "2022-08-18T18:33:43Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/grappler/costs/graph_properties.cc",
    "code1": " return OkStatus();  } VLOG(1) << \"Checking any conflics in shapes and dimensions ...\"; int64_t num_incompatible_shapes = 0; for (const NodeDef& node : graph_def.node()) { auto ctx = refiner->GetNodeContext(&node);",
    "code2": " return OkStatus();  } VLOG(1) << \"Checking any conflicts in shapes and dimensions ...\"; int64_t num_incompatible_shapes = 0; for (const NodeDef& node : graph_def.node()) { auto ctx = refiner->GetNodeContext(&node);",
    "method_name": "Status ValidateSymbolicShapeManager(const GraphDef& graph_def,"
   },
   {
    "language": ".py",
    "dir": "tensorflow/python/ops/image_ops_impl.py",
    "code1": "  Raises:    AttributeError: Raises an attribute error when dtype is neither    float nor integer  \"\"\"  image = ops.convert_to_tensor(image, name='image')  dtype = dtypes.as_dtype(dtype)",
    "code2": "  Raises:    AttributeError: Raises an attribute error when dtype is neither    float nor integer.  \"\"\"  image = ops.convert_to_tensor(image, name='image')  dtype = dtypes.as_dtype(dtype)",
    "method_name": "def convert_image_dtype(image, dtype, saturate=False, name=None):"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57178,
  "title": "[TF-TRT] Fix pylint error (line too long)",
  "tags": [
   "awaiting review",
   "comp:gpu:tensorrt",
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-08-18T21:52:37Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tensorflow/python/compiler/tensorrt/trt_convert_test.py",
    "code1": " # Use a small max_workspace_size for tests so they don't consume too much GPU # memory. _TRT_MAX_WORKSPACE_SIZE_BYTES = trt_convert.DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES def mkdtemp(self): return tempfile.mkdtemp(dir=self.get_temp_dir())",
    "code2": " # Use a small max_workspace_size for tests so they don't consume too much GPU # memory. _TRT_MAX_WORKSPACE_SIZE_BYTES = ( trt_convert.DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES) def mkdtemp(self): return tempfile.mkdtemp(dir=self.get_temp_dir())",
    "method_name": "class TrtConvertTest(test_util.TensorFlowTestCase, parameterized.TestCase):"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57096,
  "title": "[TF-TRT] DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES fixed to LLONG_MAX - 512",
  "tags": [
   "comp:gpu:tensorrt",
   "ready to pull",
   "size:S"
  ],
  "closed_time": "2022-08-15T15:39:09Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tensorflow/python/compiler/tensorrt/test/quantization_mnist_test.py",
    "code1": "          nodes_denylist=[OUTPUT_NODE_NAME],          max_batch_size=max_batch_size,          precision_mode='INT8',          # There is a 2GB GPU memory limit for each test, so we set          # max_workspace_size_bytes to 256MB to leave enough room for TF          # runtime to allocate GPU memory.          max_workspace_size_bytes=1 << 28,          minimum_segment_size=2,          use_calibration=False)      graph_def = converter.convert()        conv_params = trt_convert.TrtConversionParams(            precision_mode='FP16',            minimum_segment_size=2,            max_workspace_size_bytes=1 << 28,            maximum_cached_engines=1)        converter = trt_convert.TrtGraphConverterV2(            input_saved_model_dir=saved_model_dir,",
    "code2": "          nodes_denylist=[OUTPUT_NODE_NAME],          max_batch_size=max_batch_size,          precision_mode='INT8',          max_workspace_size_bytes=(            trt_convert.DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES          ),          minimum_segment_size=2,          use_calibration=False)      graph_def = converter.convert()        conv_params = trt_convert.TrtConversionParams(            precision_mode='FP16',            minimum_segment_size=2,            max_workspace_size_bytes=(              trt_convert.DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES            ),            maximum_cached_engines=1)        converter = trt_convert.TrtGraphConverterV2(            input_saved_model_dir=saved_model_dir,",
    "method_name": "def _GetGraphDef(self, use_trt, max_batch_size, model_dir):"
   },
   {
    "language": ".py",
    "dir": "tensorflow/python/compiler/tensorrt/test/tf_function_test.py",
    "code1": "from tensorflow.core.framework import attr_value_pb2from tensorflow.core.protobuf import saved_model_pb2from tensorflow.python.compiler.tensorrt.test import tf_trt_integration_test_base as trt_testfrom tensorflow.python.compiler.tensorrt.test.tf_trt_integration_test_base import GraphStatefrom tensorflow.python.compiler.tensorrt.test.tf_trt_integration_test_base import IsQuantizationWithCalibration        \"_tftrt_convert_function\": True,        \"_tftrt_trt_logger_name\": \"DefaultLogger\",        \"_tftrt_max_batch_size\": 10,        \"_tftrt_max_workspace_size_bytes\": 1 << 25,        \"_tftrt_precision_mode\": \"FP16\",        \"_tftrt_minimum_segment_size\": 2,        \"_tftrt_is_dyn_op\": True,",
    "code2": "from tensorflow.core.framework import attr_value_pb2from tensorflow.core.protobuf import saved_model_pb2from tensorflow.python.compiler.tensorrt import trt_convertfrom tensorflow.python.compiler.tensorrt.test import tf_trt_integration_test_base as trt_testfrom tensorflow.python.compiler.tensorrt.test.tf_trt_integration_test_base import GraphStatefrom tensorflow.python.compiler.tensorrt.test.tf_trt_integration_test_base import IsQuantizationWithCalibration        \"_tftrt_convert_function\": True,        \"_tftrt_trt_logger_name\": \"DefaultLogger\",        \"_tftrt_max_batch_size\": 10,        \"_tftrt_max_workspace_size_bytes\": (          trt_convert.DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES        ),        \"_tftrt_precision_mode\": \"FP16\",        \"_tftrt_minimum_segment_size\": 2,        \"_tftrt_is_dyn_op\": True,",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "tensorflow/python/compiler/tensorrt/test/tf_trt_integration_test_base.py",
    "code1": "        # We use the minimum of all the batch sizes, so when multiple different        # input shapes are provided it'll always create new engines in the        # cache, and we can therefore test the cache behavior.        max_workspace_size_bytes=1 << 25,        precision_mode=run_params.precision_mode,        minimum_segment_size=2,        maximum_cached_engines=1,  def _Test(self):    logging.info(        \"Running test %s with parameters: convert_online=%s, \"        \"precision_mode=%s, dynamic_engine=%s, dynamic_shape_mode%s\",        run_params.test_name, run_params.convert_online,        run_params.precision_mode, run_params.dynamic_engine,        run_params.dynamic_shape)    self.RunTest(run_params)  return _Test",
    "code2": "        # We use the minimum of all the batch sizes, so when multiple different        # input shapes are provided it'll always create new engines in the        # cache, and we can therefore test the cache behavior.        max_workspace_size_bytes=(          trt_convert.DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES        ),        precision_mode=run_params.precision_mode,        minimum_segment_size=2,        maximum_cached_engines=1,  def _Test(self):    logging.info(        f\"Running test `{run_params.test_name}` with parameters: \"        f\"convert_online={run_params.convert_online}, \"        f\"precision_mode={run_params.precision_mode}, \"        f\"dynamic_engine={run_params.dynamic_engine}, \"        f\"dynamic_shape={run_params.dynamic_shape}\"    )    self.RunTest(run_params)  return _Test",
    "method_name": "def GetConversionParams(self, run_params):"
   },
   {
    "language": ".py",
    "dir": "tensorflow/python/compiler/tensorrt/trt_convert.py",
    "code1": "from functools import partial  # pylint: disable=g-importing-memberimport osimport platformimport tempfileimport numpy as np# For TRT >= 8.4, the recommendation is MAX_INT.if (_pywrap_py_utils.is_tensorrt_enabled() and    trt_utils.is_loaded_tensorrt_version_greater_equal(8, 4, 0)):  DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = np.iinfo(np.int32).maxelse:  DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = 1 << 30PROFILE_STRATEGY_RANGE = \"Range\"PROFILE_STRATEGY_OPTIMAL = \"Optimal\"",
    "code2": "from functools import partial  # pylint: disable=g-importing-memberimport osimport platformimport sysimport tempfileimport numpy as np# For TRT >= 8.4, the recommendation is MAX_INT.if (_pywrap_py_utils.is_tensorrt_enabled() and    trt_utils.is_loaded_tensorrt_version_greater_equal(8, 4, 0)):  # We must use `sys.maxsize - 512` to avoid overflow during casting.  DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = sys.maxsize - 512else:  DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES = 1 << 30  # 1,073,741,824PROFILE_STRATEGY_RANGE = \"Range\"PROFILE_STRATEGY_OPTIMAL = \"Optimal\"",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "tensorflow/python/compiler/tensorrt/trt_convert_test.py",
    "code1": "  # Use a small max_workspace_size for tests so they don't consume too much GPU  # memory.  _TRT_MAX_WORKSPACE_SIZE_BYTES = 2 << 20  def mkdtemp(self):    return tempfile.mkdtemp(dir=self.get_temp_dir())",
    "code2": "  # Use a small max_workspace_size for tests so they don't consume too much GPU  # memory.  _TRT_MAX_WORKSPACE_SIZE_BYTES = trt_convert.DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES  def mkdtemp(self):    return tempfile.mkdtemp(dir=self.get_temp_dir())",
    "method_name": "class TrtConvertTest(test_util.TensorFlowTestCase, parameterized.TestCase):"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57071,
  "title": "r2.10 cherry-pick: [oneDNN] Fix memory corruption issues",
  "tags": [],
  "closed_time": "2022-08-10T18:48:35Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57062,
  "title": "r2.10 cherry-pick: 741777a876c \"Fix out of range index error.\"",
  "tags": [],
  "closed_time": "2022-08-09T18:59:38Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/c/ops.cc",
    "code1": "                                      TF_Status* status) {  TF_SetStatus(status, TF_OK, \"\");  auto* cc_ctx = reinterpret_cast<InferenceContext*>(ctx);  if (0 < i || i >= cc_ctx->num_inputs()) {    TF_SetStatus(status, TF_INVALID_ARGUMENT, \"input index out of range\");  }  if (TF_GetCode(status) == TF_OK) {                                       TF_Status* status) {  TF_SetStatus(status, TF_OK, \"\");  auto* cc_ctx = reinterpret_cast<InferenceContext*>(ctx);  if (0 < i || i >= cc_ctx->num_outputs()) {    TF_SetStatus(status, TF_INVALID_ARGUMENT, \"output index out of range\");  }  if (TF_GetCode(status) == TF_OK) {",
    "code2": "                                      TF_Status* status) {  TF_SetStatus(status, TF_OK, \"\");  auto* cc_ctx = reinterpret_cast<InferenceContext*>(ctx);  if (i < 0 || i >= cc_ctx->num_inputs()) {    TF_SetStatus(status, TF_INVALID_ARGUMENT, \"input index out of range\");  }  if (TF_GetCode(status) == TF_OK) {                                       TF_Status* status) {  TF_SetStatus(status, TF_OK, \"\");  auto* cc_ctx = reinterpret_cast<InferenceContext*>(ctx);  if (i < 0 || i >= cc_ctx->num_outputs()) {    TF_SetStatus(status, TF_INVALID_ARGUMENT, \"output index out of range\");  }  if (TF_GetCode(status) == TF_OK) {",
    "method_name": "void TF_ShapeInferenceContextGetInput(TF_ShapeInferenceContext* ctx, int i,"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57042,
  "title": "[XLA] Add debug print to all tests",
  "tags": [
   "awaiting review",
   "comp:xla",
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-08-10T10:32:26Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/compiler/xla/service/copy_insertion_test.cc",
    "code1": " protected:  void InsertCopies(HloModule* module) {    CopyInsertion copy_insertion;    ASSERT_IS_OK(copy_insertion.Run(module).status());  }  const Shape scalar_shape_ = ShapeUtil::MakeShape(F32, {});  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,                          ParseAndReturnVerifiedModule(hlo_string));  InsertCopies(module.get());  VLOG(2) << module->ToString() << \"\\n\";  // An extra copy must be kept inside the loop due to uses in the conditional.  EXPECT_EQ(CountCopies(*module), 3);",
    "code2": " protected:  void InsertCopies(HloModule* module) {    CopyInsertion copy_insertion;    VLOG(3) << \"Before copy inser: \" << module->ToString();    ASSERT_IS_OK(copy_insertion.Run(module).status());    VLOG(2) << \"After copy inser: \" << module->ToString();  }  const Shape scalar_shape_ = ShapeUtil::MakeShape(F32, {});  TF_ASSERT_OK_AND_ASSIGN(std::unique_ptr<HloModule> module,                          ParseAndReturnVerifiedModule(hlo_string));  InsertCopies(module.get());  // An extra copy must be kept inside the loop due to uses in the conditional.  EXPECT_EQ(CountCopies(*module), 3);",
    "method_name": "class CopyInsertionTest : public HloTestBase {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57028,
  "title": "fix build of tensorflow/tensorflow/lite/c with cmake",
  "tags": [
   "comp:lite",
   "size:XS"
  ],
  "closed_time": "2022-08-08T09:25:00Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 57024,
  "title": "r2.10 cherry-pick: [PluggableDevice] Minor Grappler C API fixes",
  "tags": [],
  "closed_time": "2022-08-09T14:58:20Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/c/experimental/grappler/grappler.cc",
    "code1": "}TF_FunctionLibraryDefinition* TF_NewFunctionLibraryDefinition(    TF_Buffer* graph_buf, TF_Status* status) { TF_SetStatus(status, TF_OK, \"\");  tensorflow::GraphDef graph_def;  tensorflow::Status s = tensorflow::BufferToMessage(graph_buf, &graph_def);",
    "code2": "}TF_FunctionLibraryDefinition* TF_NewFunctionLibraryDefinition( const TF_Buffer* graph_buf, TF_Status* status) { TF_SetStatus(status, TF_OK, \"\");  tensorflow::GraphDef graph_def;  tensorflow::Status s = tensorflow::BufferToMessage(graph_buf, &graph_def);",
    "method_name": "void TF_GetOutputPropertiesList(TF_GraphProperties* graph_properties,"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56993,
  "title": "[TF-TRT] Fail gracefully if calibration error occurs",
  "tags": [
   "awaiting review",
   "comp:gpu:tensorrt",
   "ready to pull",
   "size:S"
  ],
  "closed_time": "2022-08-11T22:42:10Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc",
    "code1": "  //  // In both of the above cases, setBatch here returns a boolean value to  // indicate the result of the calibration process.  OP_REQUIRES_ASYNC(ctx, calib_ctx->calibrator_->setBatch(input_data, *stream),                    errors::Internal(\"Failed to feed calibration data\"),                    dummy_async_helper);  VLOG(2) << \"Passed calibration data\";  ExecuteNativeSegment(ctx, async_helper);}    if (!s.ok()) {      LOG(ERROR) << \"Calibration failed: \" << s;      cres->calibrator_->setDone();  // Ignore further pushes    } else {      // Transfer the ownership of the engine to the engine cache, so we can      // dump it out during conversion for TF 2.0.",
    "code2": "  //  // In both of the above cases, setBatch here returns a boolean value to  // indicate the result of the calibration process.  if(!calib_ctx->calibrator_->setBatch(input_data, *stream)) {    VLOG(2) << \"Failed to feed calibration data\";  } else {    VLOG(2) << \"Passed calibration data\";  }  ExecuteNativeSegment(ctx, async_helper);}    if (!s.ok()) {      LOG(ERROR) << \"Calibration failed: \" << s;      cres->calibrator_->setDone();  // Ignore further pushes      cache_res->cache_.emplace(shapes, std::make_unique<EngineContext>());    } else {      // Transfer the ownership of the engine to the engine cache, so we can      // dump it out during conversion for TF 2.0.",
    "method_name": "void TRTEngineOp::ExecuteCalibration(OpKernelContext* ctx,"
   },
   {
    "language": ".py",
    "dir": "tensorflow/python/compiler/tensorrt/trt_convert.py",
    "code1": "def _save_calibration_table(node):  calibration_table = gen_trt_ops.get_calibration_data_op(      _get_canonical_engine_name(node.name))  node.attr[\"calibration_data\"].s = calibration_table.numpy()def _convert_to_tensor(inp):",
    "code2": "def _save_calibration_table(node):  try:    calibration_table = gen_trt_ops.get_calibration_data_op(        _get_canonical_engine_name(node.name))    node.attr[\"calibration_data\"].s = calibration_table.numpy()  except errors.UnknownError:    logging.warning(\"Warning calibration error for %s\", node.name)def _convert_to_tensor(inp):",
    "method_name": "def _apply_inlining(func):"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56956,
  "title": "[XLA] [Docs] Add a known issue: must-be-constant inputs as functions of induction variables",
  "tags": [
   "awaiting review",
   "comp:xla",
   "ready to pull",
   "size:S"
  ],
  "closed_time": "2022-08-01T18:23:23Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56939,
  "title": "Fix out of range index error.",
  "tags": [
   "comp:runtime",
   "prtype:bugfix",
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-08-05T18:55:42Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/c/ops.cc",
    "code1": "                                      TF_Status* status) { TF_SetStatus(status, TF_OK, \"\"); auto* cc_ctx = reinterpret_cast<InferenceContext*>(ctx); if (0 < i || i >= cc_ctx->num_inputs()) { TF_SetStatus(status, TF_INVALID_ARGUMENT, \"input index out of range\");  } if (TF_GetCode(status) == TF_OK) {                                       TF_Status* status) { TF_SetStatus(status, TF_OK, \"\"); auto* cc_ctx = reinterpret_cast<InferenceContext*>(ctx); if (0 < i || i >= cc_ctx->num_outputs()) { TF_SetStatus(status, TF_INVALID_ARGUMENT, \"output index out of range\");  } if (TF_GetCode(status) == TF_OK) {",
    "code2": "                                      TF_Status* status) { TF_SetStatus(status, TF_OK, \"\"); auto* cc_ctx = reinterpret_cast<InferenceContext*>(ctx); if (i < 0 || i >= cc_ctx->num_inputs()) { TF_SetStatus(status, TF_INVALID_ARGUMENT, \"input index out of range\");  } if (TF_GetCode(status) == TF_OK) {                                       TF_Status* status) { TF_SetStatus(status, TF_OK, \"\"); auto* cc_ctx = reinterpret_cast<InferenceContext*>(ctx); if (i < 0 || i >= cc_ctx->num_outputs()) { TF_SetStatus(status, TF_INVALID_ARGUMENT, \"output index out of range\");  } if (TF_GetCode(status) == TF_OK) {",
    "method_name": "void TF_ShapeInferenceContextGetInput(TF_ShapeInferenceContext* ctx, int i,"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56933,
  "title": "Some unit tests are fixed and should be removed from exclude list",
  "tags": [
   "ready to pull",
   "size:S"
  ],
  "closed_time": "2022-07-29T13:37:02Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56914,
  "title": "Fix failing unit tests quantization_ops:quantization_ops_test",
  "tags": [
   "comp:mkl",
   "ready to pull",
   "size:S",
   "type:docs-bug"
  ],
  "closed_time": "2022-07-27T23:30:51Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/mkl/mkl_avgpooling_op.cc",
    "code1": "      if (int8_forward_inference) {        const Tensor& min_input_t = MklGetInput(context, 1);        const Tensor& max_input_t = MklGetInput(context, 2);        const float min_input = min_input_t.flat<float>()(0);        const float max_input = max_input_t.flat<float>()(0);        Tensor* output_min = nullptr;        Tensor* output_max = nullptr;",
    "code2": "      if (int8_forward_inference) {        const Tensor& min_input_t = MklGetInput(context, 1);        const Tensor& max_input_t = MklGetInput(context, 2);        OP_REQUIRES(            context, TensorShapeUtils::IsScalar(min_input_t.shape()),            errors::InvalidArgument(                \"min_input shape must be rank 0 but is rank \",                min_input_t.dims(), \", received shape: \", min_input_t.shape()));        OP_REQUIRES(            context, TensorShapeUtils::IsScalar(max_input_t.shape()),            errors::InvalidArgument(                \"max_input shape must be rank 0 but is rank \",                max_input_t.dims(), \", received shape: \", max_input_t.shape()));        const float min_input = min_input_t.scalar<float>()();        const float max_input = max_input_t.scalar<float>()();        Tensor* output_min = nullptr;        Tensor* output_max = nullptr;",
    "method_name": "class MklAvgPoolingOp : public MklPoolingForwardOpBase<T> {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/mkl/mkl_maxpooling_op.cc",
    "code1": "        // Pass min, max from input to output.        const Tensor& min_input_t = MklGetInput(context, 1);        const Tensor& max_input_t = MklGetInput(context, 2);        const float min_input = min_input_t.flat<float>()(0);        const float max_input = max_input_t.flat<float>()(0);        Tensor* output_min = nullptr;        Tensor* output_max = nullptr;",
    "code2": "        // Pass min, max from input to output.        const Tensor& min_input_t = MklGetInput(context, 1);        const Tensor& max_input_t = MklGetInput(context, 2);        OP_REQUIRES(            context, TensorShapeUtils::IsScalar(min_input_t.shape()),            errors::InvalidArgument(                \"min_input shape must be rank 0 but is rank \",                min_input_t.dims(), \", received shape: \", min_input_t.shape()));        OP_REQUIRES(            context, TensorShapeUtils::IsScalar(max_input_t.shape()),            errors::InvalidArgument(                \"max_input shape must be rank 0 but is rank \",                max_input_t.dims(), \", received shape: \", max_input_t.shape()));        const float min_input = min_input_t.scalar<float>()();        const float max_input = max_input_t.scalar<float>()();        Tensor* output_min = nullptr;        Tensor* output_max = nullptr;",
    "method_name": "class MklMaxPoolingOp : public MklPoolingForwardOpBase<T> {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/mkl/mkl_quantized_pooling_ops_test.cc",
    "code1": "  AddInputFromArray<quint8>(input_quantized.shape(),                            input_quantized.flat<quint8>());  AddInputFromArray<float>(TensorShape({1}), {input_min});  AddInputFromArray<float>(TensorShape({1}), {input_max});  TF_ASSERT_OK(RunOpKernel());  test::FillValues<float>(&expected_float, {11, 12, 15, 16, 27, 28, 31, 32});  AddInputFromArray<quint8>(input_quantized.shape(),                            input_quantized.flat<quint8>());  AddInputFromArray<float>(TensorShape({1}), {input_min});  AddInputFromArray<float>(TensorShape({1}), {input_max});  TF_ASSERT_OK(RunOpKernel());",
    "code2": "  AddInputFromArray<quint8>(input_quantized.shape(),                            input_quantized.flat<quint8>());  AddInputFromArray<float>(TensorShape({}), {input_min});  AddInputFromArray<float>(TensorShape({}), {input_max});  TF_ASSERT_OK(RunOpKernel());  test::FillValues<float>(&expected_float, {11, 12, 15, 16, 27, 28, 31, 32});  AddInputFromArray<quint8>(input_quantized.shape(),                            input_quantized.flat<quint8>());  AddInputFromArray<float>(TensorShape({}), {input_min});  AddInputFromArray<float>(TensorShape({}), {input_max});  TF_ASSERT_OK(RunOpKernel());",
    "method_name": "TEST_F(QuantizedPoolingTest, SmallAveragePooling) {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56911,
  "title": "Correction of some minor errors in the Tile Converter testing procedure.",
  "tags": [
   "awaiting review",
   "comp:gpu:tensorrt",
   "ready to pull",
   "size:M"
  ],
  "closed_time": "2022-07-28T18:43:48Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/compiler/tf2tensorrt/convert/convert_nodes_test.cc",
    "code1": "    int test_ID;    // Expected status of conversion (with concrete error message).    Status status;    // Expected status of BuildAndRun.    Status runtime_status;  };  std::vector<TileParam> test_params = {    for (bool input_is_tensor : {true, false}) {      for (auto p : test_params) {        std::vector<int> num_mults = {static_cast<int>(p.multiplier.size())};        std::vector<std::vector<int>> partial_input_dims_options = {{}};        if (multiplier_is_tensor) {          if (trt_mode_ == TrtTestMode::kImplicitBatch) {            p.status =            num_mults = {1, static_cast<int>(p.multiplier.size())};          } else {            if (p.test_ID == 1) {              // Replacement of statuses, since when multiplier is a tensor AND              // trt_mode_ != TrtTestMode::kImplicitBatch we cannot define these              // statuses in ConvertTile::Validate() for the first two tests              // from test_params.              p.status = Status::OK();              p.runtime_status =                  Status(error::INTERNAL,                         \"Incorrect number of profile config parameters\");            }            if (trt_mode_ == TrtTestMode::kDynamicShape) {              if (p.test_ID == 1)                partial_input_dims_options = {num_mults};              else                partial_input_dims_options = {{}, num_mults};            }          }        }        for (auto partial_input_dims : partial_input_dims_options) {          if (multiplier_is_tensor &&              trt_mode_ != TrtTestMode::kImplicitBatch) {            if (trt_mode_ == TrtTestMode::kDynamicShape) {              if (p.test_ID != 1) {                p.runtime_status =                    partial_input_dims.empty()                        ? Status(error::INTERNAL,                                 \"Failed to build TensorRT engine\")                        : Status::OK();                p.status = Status::OK();              }            }            if (p.test_ID == 2 && (trt_mode_ == TrtTestMode::kExplicitBatch ||                                   !partial_input_dims.empty())) {              p.status = Status(error::INVALID_ARGUMENT,                                \"When replications are defined as a tensor, \"                                \"the number of its elements (4) must be equal \"                                \"to the rank of the input tensor (3).\");            }          }          if (!multiplier_is_tensor &&              trt_mode_ == TrtTestMode::kImplicitBatch && p.multiplier[0] > 1) {            p.status =                Status(error::UNIMPLEMENTED,                       \"The Tile operation along \"                       \"the batch dimension in 'my_tile' is not implemented.\");          }          Reset();          if (input_is_tensor) {            AddTestTensor(\"input\", p.input_dims, p.tensor);          } else {            AddTestWeights(\"input\", p.input_dims, p.tensor, tf_type_);          }          if (multiplier_is_tensor) {            AddTestTensor<int>(\"weights\", num_mults, DT_INT32, p.multiplier,                               partial_input_dims);          } else {            AddTestWeights<int32>(\"weights\", num_mults, p.multiplier);          }          TestOpConverter(\"my_tile\", node_def, p.expected_output_dims, p.status,                          p.runtime_status,                          ElementsAreArray(p.expected_results));        }      }    }  }",
    "code2": "    int test_ID;    // Expected status of conversion (with concrete error message).    Status status;  };  std::vector<TileParam> test_params = {    for (bool input_is_tensor : {true, false}) {      for (auto p : test_params) {        std::vector<int> num_mults = {static_cast<int>(p.multiplier.size())};        std::vector<int> partial_input_dims = {};        if (multiplier_is_tensor) {          if (trt_mode_ == TrtTestMode::kImplicitBatch) {            p.status =            num_mults = {1, static_cast<int>(p.multiplier.size())};          } else {            if (p.test_ID == 1) {              // Skip this test because in that situation it is impossible              // to do a valid check for negative multipliers.              continue;            }            if (trt_mode_ == TrtTestMode::kDynamicShape) {              partial_input_dims = num_mults;              p.status = Status::OK();            }            if (p.test_ID == 2) {              p.status = Status(error::INVALID_ARGUMENT,                                \"When replications are defined as a tensor, \"                                \"the number of its elements (4) must be equal \"                                \"to the rank of the input tensor (3).\");            }          }        } else {          if (trt_mode_ == TrtTestMode::kImplicitBatch && p.multiplier[0] > 1) {            p.status =                Status(error::UNIMPLEMENTED,                       \"The Tile operation along \"                       \"the batch dimension in 'my_tile' is not implemented.\");          }        }        Reset();        if (input_is_tensor) {          AddTestTensor(\"input\", p.input_dims, p.tensor);        } else {          AddTestWeights(\"input\", p.input_dims, p.tensor, tf_type_);        }        if (multiplier_is_tensor) {          AddTestTensor<int>(\"weights\", num_mults, DT_INT32, p.multiplier,                             partial_input_dims);        } else {          AddTestWeights<int32>(\"weights\", num_mults, p.multiplier);        }        TestOpConverter(\"my_tile\", node_def, p.expected_output_dims, p.status,                        Status::OK(), ElementsAreArray(p.expected_results));      }    }  }",
    "method_name": "TEST_P(OpConverter_FP32_Test, ConvertTile) {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56902,
  "title": "Case error",
  "tags": [
   "size:XL"
  ],
  "closed_time": "2022-07-26T10:36:02Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tensorflow/compiler/tests/spacetobatch_op_test.py",
    "code1": "import numpy as npfrom tensorflow.compiler.tests import xla_testfrom tensorflow.python.framework import dtypesfrom tensorflow.python.ops import array_opsfrom tensorflow.python.ops import gen_array_ops self._testOne(x_np, block_size, x_out)class SpaceToBatchNDTest(xla_test.XLATestCase): \"\"\"Tests input-output pairs for the SpaceToBatchND and BatchToSpaceND ops.\"\"\"",
    "code2": "import numpy as npfrom tensorflow.compiler.tests import xla_testfrom tensorflow.python.framework import constant_opfrom tensorflow.python.framework import dtypesfrom tensorflow.python.ops import array_opsfrom tensorflow.python.ops import gen_array_ops self._testOne(x_np, block_size, x_out)class SpaceToBatchNDErrorHandlingTest(xla_test.XLATestCase): def testInvalidBlockShape(self): with self.assertRaisesRegex(ValueError, \"block_shape must be positive\"): with self.session() as sess, self.test_scope(): tf_in = constant_op.constant( -3.5e+35, shape=[10, 20, 20], dtype=dtypes.float32) block_shape = constant_op.constant(-10, shape=[2], dtype=dtypes.int64) paddings = constant_op.constant(0, shape=[2, 2], dtype=dtypes.int32) sess.run(array_ops.space_to_batch_nd(tf_in, block_shape, paddings)) def testOutputSizeOutOfBounds(self): with self.assertRaisesRegex(ValueError, \"Negative.* dimension size caused by overflow\"): with self.session() as sess, self.test_scope(): tf_in = constant_op.constant( -3.5e+35, shape=[10, 19, 22], dtype=dtypes.float32) block_shape = constant_op.constant( 1879048192, shape=[2], dtype=dtypes.int64) paddings = constant_op.constant(0, shape=[2, 2], dtype=dtypes.int32) sess.run(array_ops.space_to_batch_nd(tf_in, block_shape, paddings))class SpaceToBatchNDTest(xla_test.XLATestCase): \"\"\"Tests input-output pairs for the SpaceToBatchND and BatchToSpaceND ops.\"\"\"",
    "method_name": ""
   },
   {
    "language": ".cc",
    "dir": "tensorflow/compiler/tf2xla/kernels/spacetobatch_op.cc",
    "code1": "#include \"tensorflow/compiler/tf2xla/xla_op_kernel.h\"#include \"tensorflow/compiler/tf2xla/xla_op_registry.h\"#include \"tensorflow/compiler/xla/client/xla_builder.h\"namespace tensorflow {namespace { int64_t pad_end = paddings.Get<int64_t>({i, 1}); OP_REQUIRES(ctx, pad_start >= 0 && pad_end >= 0, errors::InvalidArgument(\"Paddings must be non-negative\"));    dim->set_edge_padding_low(pad_start);    dim->set_edge_padding_high(pad_end);    padded_shape[1 + i] += pad_start + pad_end;    block_num_elems *= block_shape[i];  } // Don't pad the remainder dimensions. for (int i = 0; i < remainder_shape.size(); ++i) { OP_REQUIRES(ctx, block_num_elems > 0, errors::InvalidArgument( \"The product of the block dimensions must be positive\"));  xla::XlaOp padded = xla::Pad(input, XlaHelpers::Zero(b, input_dtype), padding_config); //       padded_shape[M] / block_shape[M-1], //       block_shape[M-1]] + //      remaining_shape const int64_t batch_size = input_shape[0];  std::vector<int64_t> reshaped_padded_shape(input_rank + block_rank);  reshaped_padded_shape[0] = batch_size; for (int i = 0; i < block_rank; ++i) { // Determine the length of the prefix of block dims that can be combined // into the batch dimension due to having no padding and block_shape=1.  std::vector<int64_t> output_shape(input_rank);  output_shape[0] = batch_size * block_num_elems; for (int i = 0; i < block_rank; ++i) {    output_shape[1 + i] = padded_shape[1 + i] / block_shape[i];  }",
    "code2": "#include \"tensorflow/compiler/tf2xla/xla_op_kernel.h\"#include \"tensorflow/compiler/tf2xla/xla_op_registry.h\"#include \"tensorflow/compiler/xla/client/xla_builder.h\"#include \"tensorflow/core/util/overflow.h\"namespace tensorflow {namespace { int64_t pad_end = paddings.Get<int64_t>({i, 1}); OP_REQUIRES(ctx, pad_start >= 0 && pad_end >= 0, errors::InvalidArgument(\"Paddings must be non-negative\")); OP_REQUIRES(ctx, block_shape[i] >= 1, errors::InvalidArgument( \"All values in block_shape must be positive, got value, \",                    block_shape[i], \" at index \", i, \".\"));    dim->set_edge_padding_low(pad_start);    dim->set_edge_padding_high(pad_end);    padded_shape[1 + i] += pad_start + pad_end;    block_num_elems = MultiplyWithoutOverflow(block_num_elems, block_shape[i]);  } // Don't pad the remainder dimensions. for (int i = 0; i < remainder_shape.size(); ++i) { OP_REQUIRES(ctx, block_num_elems > 0, errors::InvalidArgument( \"The product of the block dimensions must be positive\")); const int64_t batch_size = input_shape[0]; const int64_t output_dim = MultiplyWithoutOverflow(batch_size, block_num_elems); if (output_dim < 0) { OP_REQUIRES(        ctx, output_dim >= 0, errors::InvalidArgument(\"Negative output dimension size caused by \" \"overflow when multiplying \",                                batch_size, \" and \", block_num_elems));  }  xla::XlaOp padded = xla::Pad(input, XlaHelpers::Zero(b, input_dtype), padding_config); //       padded_shape[M] / block_shape[M-1], //       block_shape[M-1]] + //      remaining_shape  std::vector<int64_t> reshaped_padded_shape(input_rank + block_rank);  reshaped_padded_shape[0] = batch_size; for (int i = 0; i < block_rank; ++i) { // Determine the length of the prefix of block dims that can be combined // into the batch dimension due to having no padding and block_shape=1.  std::vector<int64_t> output_shape(input_rank);  output_shape[0] = output_dim; for (int i = 0; i < block_rank; ++i) {    output_shape[1 + i] = padded_shape[1 + i] / block_shape[i];  }",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/common_runtime/eager/execute.cc",
    "code1": "    const Tensor* tensor;    // TODO(fishx): Avoid blocking here.    TF_RETURN_IF_ERROR(tensor_handle->Tensor(&tensor));    const ResourceHandle& handle = tensor->flat<ResourceHandle>()(0);    device_name = handle.device();  auto status = GetOrCreateKernelAndDevice(op, retvals, num_retvals, &kernel);#ifdef INTEL_MKL  if (IsMKLEnabled() && kernel != nullptr && !ctx.RunEagerOpAsFunction() &&      op->Device() == kVariantDeviceNull) {    // oneDNN optimization pass relies on the op's assigned device to determine    // whether it can be rewritten.",
    "code2": "    const Tensor* tensor;    // TODO(fishx): Avoid blocking here.    TF_RETURN_IF_ERROR(tensor_handle->Tensor(&tensor));    if (tensor->NumElements() == 0) {      return errors::InvalidArgument(\"Empty resource handle\");    }    const ResourceHandle& handle = tensor->flat<ResourceHandle>()(0);    device_name = handle.device();  auto status = GetOrCreateKernelAndDevice(op, retvals, num_retvals, &kernel);#ifdef INTEL_MKL  if (IsMKLEnabled() && kernel != nullptr &&      op->Device() == kVariantDeviceNull) {    // oneDNN optimization pass relies on the op's assigned device to determine    // whether it can be rewritten.",
    "method_name": "Status GetDeviceForInput(const EagerContext& ctx, TensorHandle* tensor_handle,"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/common_runtime/eager/mkl_eager_op_rewrite.cc",
    "code1": "  if (!IsMKLEnabled()) {    return false;  }  // Don't rewrite the op if it should run as a function.  if (op->EagerContext().RunEagerOpAsFunction()) {    return false;  }  DataType data_type;  if (op->Attrs().Get(\"T\", &data_type) != Status::OK()) {    return false;",
    "code2": "  if (!IsMKLEnabled()) {    return false;  }  DataType data_type;  if (op->Attrs().Get(\"T\", &data_type) != Status::OK()) {    return false;",
    "method_name": "bool MklEagerOpRewrite::ShouldRewriteOp(EagerOperation* op) {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/framework/shape_inference.cc",
    "code1": "#include \"tensorflow/core/lib/strings/numbers.h\"#include \"tensorflow/core/lib/strings/scanner.h\"#include \"tensorflow/core/lib/strings/str_util.h\"namespace tensorflow {namespace shape_inference {    *out = UnknownDim();  } else { // Invariant: Both values are known and greater than 1. const int64_t product = first_value * second_value; if (product < 0) { return errors::InvalidArgument( \"Negative dimension size caused by overflow when multiplying \",",
    "code2": "#include \"tensorflow/core/lib/strings/numbers.h\"#include \"tensorflow/core/lib/strings/scanner.h\"#include \"tensorflow/core/lib/strings/str_util.h\"#include \"tensorflow/core/util/overflow.h\"namespace tensorflow {namespace shape_inference {    *out = UnknownDim();  } else { // Invariant: Both values are known and greater than 1. const int64_t product = MultiplyWithoutOverflow(first_value, second_value); if (product < 0) { return errors::InvalidArgument( \"Negative dimension size caused by overflow when multiplying \",",
    "method_name": "limitations under the License."
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/grappler/optimizers/remapper.cc",
    "code1": "    return true;  };  if (ShapesSymbolicallyEqual(prot0_shape, prot1_shape) ||      !ShapesBroadcastable(prot0_shape, prot1_shape))    return false;  if (IsConvOrMatMul(*node_def_0)) {    bias_port = 1;    return (is_supported_shape(prot0_shape, prot1_shape));  } else if (IsConvOrMatMul(*node_def_1)) {    bias_port = 0;    return (is_supported_shape(prot1_shape, prot0_shape));  }  return false;}",
    "code2": "    return true;  };  // This is used only for MatMul+Add fusion.  const auto is_matmul_supported_shape =      [](const TensorShapeProto& shape,         const TensorShapeProto& bcast_shape) -> bool {    if (shape.dim_size() < 2 || bcast_shape.dim_size() != 1) return false;    int channel_dim = shape.dim(shape.dim_size() - 1).size();    return (channel_dim == bcast_shape.dim(0).size());  };  if (ShapesSymbolicallyEqual(prot0_shape, prot1_shape) ||      !ShapesBroadcastable(prot0_shape, prot1_shape))    return false;  // For now block MatMul+Add fusion if Bias dims are more than one.  // TODO(intel-tf): Enable this fusion once it is properly tested.  if (IsConvOrMatMul(*node_def_0)) {    bias_port = 1;    if (IsMatMul(*node_def_0)) {      return (is_matmul_supported_shape(prot0_shape, prot1_shape));    } else {      return (is_supported_shape(prot0_shape, prot1_shape));    }  } else if (IsConvOrMatMul(*node_def_1)) {    bias_port = 0;    if (IsMatMul(*node_def_1)) {      return (is_matmul_supported_shape(prot1_shape, prot0_shape));    } else {      return (is_supported_shape(prot1_shape, prot0_shape));    }  }  return false;}",
    "method_name": "bool IsBiasSemanticAdd(const RemapperContext& ctx,"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/conv_grad_ops_3d.cc",
    "code1": "    TensorShape filter_shape; if (takes_shape_) { const Tensor& filter_sizes = context->input(1); OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(                                  filter_sizes.vec<int32>(), &filter_shape));    } else {    TensorShape filter_shape; if (takes_shape_) { const Tensor& filter_sizes = context->input(1); OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(                                  filter_sizes.vec<int32>(), &filter_shape));    } else {    TensorShape filter_shape; if (takes_shape_) { const Tensor& filter_sizes = context->input(1); OP_REQUIRES_OK(context, tensor::MakeShape(filter_sizes, &filter_shape));    } else {      filter_shape = context->input(1).shape();",
    "code2": "    TensorShape filter_shape; if (takes_shape_) { const Tensor& filter_sizes = context->input(1); OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()), errors::InvalidArgument( \"filter_sizes shape must be rank 1 but is rank \",                      filter_sizes.shape().dims())); OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(                                  filter_sizes.vec<int32>(), &filter_shape));    } else {    TensorShape filter_shape; if (takes_shape_) { const Tensor& filter_sizes = context->input(1); OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()), errors::InvalidArgument( \"filter_sizes shape must be rank 1 but is rank \",                      filter_sizes.shape().dims())); OP_REQUIRES_OK(context, TensorShapeUtils::MakeShape(                                  filter_sizes.vec<int32>(), &filter_shape));    } else {    TensorShape filter_shape; if (takes_shape_) { const Tensor& filter_sizes = context->input(1); OP_REQUIRES(context, TensorShapeUtils::IsVector(filter_sizes.shape()), errors::InvalidArgument( \"filter_sizes shape must be rank 1 but is rank \",                      filter_sizes.shape().dims())); OP_REQUIRES_OK(context, tensor::MakeShape(filter_sizes, &filter_shape));    } else {      filter_shape = context->input(1).shape();",
    "method_name": "class Conv3DBackpropFilterOp : public OpKernel {"
   },
   {
    "language": ".cc",
    "dir": "tensorflow/core/kernels/depthwise_conv_grad_op.cc",
    "code1": "      OP_REQUIRES(context, in_sizes_data[i] >= 0,                  errors::InvalidArgument(\"Dimension \", i, \" of input_sizes must be >= 0\"));      input_shape.AddDim(in_sizes_data[i]);    }    const TensorShape& filter_shape = filter.shape();    EXTRACT_AND_VERIFY_DIMENSIONS(\"DepthwiseConv2DBackpropInput\");      OP_REQUIRES(context, filter_sizes_data[i] >= 0,                  errors::InvalidArgument(\"Dimension \", i, \" of filter_sizes must be >= 0\"));      filter_shape.AddDim(filter_sizes_data[i]);    }    const TensorShape& input_shape = input.shape();",
    "code2": "      OP_REQUIRES(context, in_sizes_data[i] >= 0,                  errors::InvalidArgument(\"Dimension \", i, \" of input_sizes must be >= 0\")); OP_REQUIRES_OK(context, input_shape.AddDimWithStatus(in_sizes_data[i]));    }    const TensorShape& filter_shape = filter.shape();    EXTRACT_AND_VERIFY_DIMENSIONS(\"DepthwiseConv2DBackpropInput\");      OP_REQUIRES(context, filter_sizes_data[i] >= 0,                  errors::InvalidArgument(\"Dimension \", i, \" of filter_sizes must be >= 0\"));      OP_REQUIRES_OK(context,                     filter_shape.AddDimWithStatus(filter_sizes_data[i]));    }    const TensorShape& input_shape = input.shape();",
    "method_name": "class DepthwiseConv2dNativeBackpropInputOp : public OpKernel {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56853,
  "title": "fixed some typos, formatted document",
  "tags": [
   "size:XL"
  ],
  "closed_time": "2022-08-19T05:46:34Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56841,
  "title": "Cache size is too small for machines with more than 49 CPU cores",
  "tags": [
   "awaiting review",
   "comp:data",
   "ready to pull",
   "size:XS",
   "type:bug"
  ],
  "closed_time": "2022-07-22T19:57:02Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tensorflow/python/data/experimental/kernel_tests/service/cross_trainer_cache_test.py",
    "code1": "from tensorflow.python.framework import errorsfrom tensorflow.python.platform import testclass CrossTrainerCacheTest(data_service_test_base.TestBase,                            parameterized.TestCase):      combinations.times(          combinations.combine(tf_api_version=2, mode=[\"eager\", \"graph\"])))  def testConcurrentReaders(self):    cluster = self._create_cluster(        num_workers=1, cross_trainer_cache_size_bytes=18000)    num_readers = 20    num_elements = 50    dataset = dataset_ops.Dataset.range(10000000).repeat()",
    "code2": "from tensorflow.python.framework import errorsfrom tensorflow.python.platform import testimport multiprocessingclass CrossTrainerCacheTest(data_service_test_base.TestBase,                            parameterized.TestCase):      combinations.times(          combinations.combine(tf_api_version=2, mode=[\"eager\", \"graph\"])))  def testConcurrentReaders(self):    # Fetching an element from the dataset will trigger prefetches of more    # elements, one per CPU core which will be placed in the cache.    # However if the number of prefetches exceeds the space available in    # the cache then the sliding window will be moved forward away from    # the element just read thus negating the use of the cache as other    # trainers will not get the correct element.    # Hence the need to calculate the size of the cache based on the    # number of CPU cores and the element size of 363. The extra 8    # entries are simply a bit of margin.    num_cpus = multiprocessing.cpu_count()    cluster = self._create_cluster(        num_workers=1, cross_trainer_cache_size_bytes=(num_cpus + 8) * 363)    num_readers = 20    num_elements = 50    dataset = dataset_ops.Dataset.range(10000000).repeat()",
    "method_name": ""
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56831,
  "title": "Fix compilation issue when building mhlo as an external project.",
  "tags": [
   "awaiting review",
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-07-21T10:25:27Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56811,
  "title": "Remove incorrect test tags so that all expected pip tests are run",
  "tags": [
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-08-02T18:56:52Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56764,
  "title": "[ROCm] Add generic layout optimizer fix for ROCm.",
  "tags": [
   "comp:gpu",
   "size:XS"
  ],
  "closed_time": "2022-07-29T22:30:34Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56730,
  "title": "Fix PyLint error where line is too long",
  "tags": [
   "comp:ops",
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-07-29T23:49:48Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tensorflow/python/ops/array_ops.py",
    "code1": "  arranged along the last axis of `indices`.  This is similar to `tf.gather`, in which `indices` defines slices into the  first dimension of `params`. In `tf.gather_nd`, `indices` defines slices into the  first `N` dimensions of `params`, where `N = indices.shape[-1]`.  Caution: On CPU, if an out of bound index is found, an error is returned.  On GPU, if an out of bound index is found, a 0 is stored in the",
    "code2": "  arranged along the last axis of `indices`.  This is similar to `tf.gather`, in which `indices` defines slices into the  first dimension of `params`. In `tf.gather_nd`, `indices` defines slices into the first `N` dimensions of `params`, where `N = indices.shape[-1]`.  Caution: On CPU, if an out of bound index is found, an error is returned.  On GPU, if an out of bound index is found, a 0 is stored in the",
    "method_name": "def gather_nd(params, indices, name=None, batch_dims=0):"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56728,
  "title": "Fixing the incorrect link in backend.py",
  "tags": [
   "comp:keras",
   "size:XS"
  ],
  "closed_time": "2022-07-13T06:52:03Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tensorflow/python/keras/backend.py",
    "code1": "      ragged: Boolean, whether the placeholder should have a ragged type.          In this case, values of 'None' in the 'shape' argument represent          ragged dimensions. For more information about RaggedTensors, see this          [guide](https://www.tensorflow.org/guide/ragged_tensors).  Raises:      ValueError: If called with sparse = True and ragged = True.",
    "code2": "      ragged: Boolean, whether the placeholder should have a ragged type.          In this case, values of 'None' in the 'shape' argument represent          ragged dimensions. For more information about RaggedTensors, see this          [guide](https://www.tensorflow.org/guide/ragged_tensor).  Raises:      ValueError: If called with sparse = True and ragged = True.",
    "method_name": "def placeholder(shape=None,"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56714,
  "title": "Fix unit tests that rely on unsupported behaviour",
  "tags": [
   "awaiting review",
   "ready to pull",
   "size:S",
   "type:bug"
  ],
  "closed_time": "2022-07-20T23:06:15Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tensorflow/python/kernel_tests/array_ops/array_ops_test.py",
    "code1": "    np_val_grad = (2 * self.varnp * self.varnp)    np_sliceval_grad = np.zeros(self.var.get_shape())    if isinstance(spec, ops.Tensor):      spec = self.test.evaluate([spec])    np_sliceval_grad[spec] = np_val_grad[spec]    # verify gradient    self.test.assertAllEqual(slice_val_grad_evaled, np_sliceval_grad)",
    "code2": "    np_val_grad = (2 * self.varnp * self.varnp)    np_sliceval_grad = np.zeros(self.var.get_shape())    if isinstance(spec, ops.Tensor):      spec = self.test.evaluate(spec)    np_sliceval_grad[spec] = np_val_grad[spec]    # verify gradient    self.test.assertAllEqual(slice_val_grad_evaled, np_sliceval_grad)",
    "method_name": "def __getitem__(self, spec):"
   },
   {
    "language": ".py",
    "dir": "tensorflow/python/kernel_tests/array_ops/concat_op_test.py",
    "code1": "                              cur_offset + params[p[i]].shape[concat_dim])      cur_offset += params[p[i]].shape[concat_dim]      if dtype == dtype_feed:        self.assertAllEqual(result[ind], params[p[i]])      else:        self.assertAllClose(result[ind], params[p[i]], 0.01)  @test_util.run_deprecated_v1  def testRandom(self):          index[concat_dim] = slice(cur_offset,                                    cur_offset + params[p[i]].shape[concat_dim])          cur_offset += params[p[i]].shape[concat_dim]          self.assertAllEqual(result[index], params[p[i]])  def testConcatEmpty(self):    with test_util.use_gpu():",
    "code2": "                              cur_offset + params[p[i]].shape[concat_dim])      cur_offset += params[p[i]].shape[concat_dim]      if dtype == dtype_feed:        self.assertAllEqual(result[tuple(ind)], params[p[i]])      else:        self.assertAllClose(result[tuple(ind)], params[p[i]], 0.01)  @test_util.run_deprecated_v1  def testRandom(self):          index[concat_dim] = slice(cur_offset,                                    cur_offset + params[p[i]].shape[concat_dim])          cur_offset += params[p[i]].shape[concat_dim]          self.assertAllEqual(result[tuple(index)], params[p[i]])  def testConcatEmpty(self):    with test_util.use_gpu():",
    "method_name": "def _testRandom(self, dtype):"
   },
   {
    "language": ".py",
    "dir": "tensorflow/python/kernel_tests/array_ops/slice_op_test.py",
    "code1": "    slices = []    for i in range(len(input_shape)):      slices.append(slice(slice_begin[i], slice_begin[i] + slice_size[i]))    np_ans[slices] = grads    self.assertAllClose(np_ans, result)    slices = []    for i in range(len(input_shape)):      slices.append(slice(slice_begin[i], slice_begin[i] + slice_size[i]))    np_ans[slices] = grads    self.assertAllClose(np_ans, result)",
    "code2": "    slices = []    for i in range(len(input_shape)):      slices.append(slice(slice_begin[i], slice_begin[i] + slice_size[i]))    np_ans[tuple(slices)] = grads    self.assertAllClose(np_ans, result)    slices = []    for i in range(len(input_shape)):      slices.append(slice(slice_begin[i], slice_begin[i] + slice_size[i]))    np_ans[tuple(slices)] = grads    self.assertAllClose(np_ans, result)",
    "method_name": "def _testGradientSlice(self, input_shape, slice_begin, slice_size):"
   },
   {
    "language": ".py",
    "dir": "tensorflow/python/kernel_tests/array_ops/split_op_test.py",
    "code1": "    for i in range(num_split):      slices[split_dim] = slice(offset, offset + size_splits[i])      offset += size_splits[i]      self.assertAllEqual(result[i], inp[slices])  def _testSpecialCasesVariable(self):    inp = np.random.rand(4, 4).astype(\"f\")    for i in range(num_split):      slices[split_dim] = slice(offset, offset + size_splits[i])      offset += size_splits[i]      self.assertAllEqual(result[i], inp[slices])  @test_util.run_in_graph_and_eager_modes  def testSpecialCasesVariable(self):    for i in range(num_split):      slices[split_dim] = slice(offset, offset + length)      offset += length      self.assertAllEqual(result[i], inp[slices])  @test_util.run_in_graph_and_eager_modes  def testRandom(self):",
    "code2": "    for i in range(num_split):      slices[split_dim] = slice(offset, offset + size_splits[i])      offset += size_splits[i]      self.assertAllEqual(result[i], inp[tuple(slices)])  def _testSpecialCasesVariable(self):    inp = np.random.rand(4, 4).astype(\"f\")    for i in range(num_split):      slices[split_dim] = slice(offset, offset + size_splits[i])      offset += size_splits[i]      self.assertAllEqual(result[i], inp[tuple(slices)])  @test_util.run_in_graph_and_eager_modes  def testSpecialCasesVariable(self):    for i in range(num_split):      slices[split_dim] = slice(offset, offset + length)      offset += length      self.assertAllEqual(result[i], inp[tuple(slices)])  @test_util.run_in_graph_and_eager_modes  def testRandom(self):",
    "method_name": "def _RunAndVerifyVariable(self, dtype, large_num_splits=False):"
   },
   {
    "language": ".py",
    "dir": "tensorflow/python/kernel_tests/control_flow/scan_ops_test.py",
    "code1": "  ix = [      slice(None, None, -1) if i == axis else slice(None) for i in range(length)  ]  return x[ix]def handle_options(func, x, axis, exclusive, reverse):        slice(0, -1) if i == axis else slice(None) for i in range(length)    ]    if func == np.cumsum:      init = np.zeros_like(x[ix_head])    elif func == np.cumprod:      init = np.ones_like(x[ix_head])    else:      raise ValueError(\"Unknown scan function.\")    x = np.concatenate([init, func(x[ix_init], axis)], axis=axis)  else:    x = func(x, axis=axis)",
    "code2": "  ix = [      slice(None, None, -1) if i == axis else slice(None) for i in range(length)  ]  return x[tuple(ix)]def handle_options(func, x, axis, exclusive, reverse):        slice(0, -1) if i == axis else slice(None) for i in range(length)    ]    if func == np.cumsum:      init = np.zeros_like(x[tuple(ix_head)])    elif func == np.cumprod:      init = np.ones_like(x[tuple(ix_head)])    else:      raise ValueError(\"Unknown scan function.\")    x = np.concatenate([init, func(x[tuple(ix_init)], axis)], axis=axis)  else:    x = func(x, axis=axis)",
    "method_name": "def numpy_reverse(x, axis):"
   },
   {
    "language": ".py",
    "dir": "tensorflow/python/ops/array_ops.py",
    "code1": "  if mode == \"CONSTANT\":    # TODO(rjryan): Once the forward compatibility period (3 weeks) have passed    # remove the \"Pad\" fallback here.    if not tensor_util.is_tf_type(constant_values) and constant_values == 0:      result = gen_array_ops.pad(tensor, paddings, name=name)    else:      result = gen_array_ops.pad_v2(",
    "code2": "  if mode == \"CONSTANT\":    # TODO(rjryan): Once the forward compatibility period (3 weeks) have passed    # remove the \"Pad\" fallback here.    if not tensor_util.is_tf_type(constant_values) \\        and np.isscalar(constant_values) and constant_values == 0:      result = gen_array_ops.pad(tensor, paddings, name=name)    else:      result = gen_array_ops.pad_v2(",
    "method_name": "def pad(tensor, paddings, mode=\"CONSTANT\", name=None, constant_values=0):  # pyl"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56691,
  "title": "[NVIDIA TF] Fix cublas build issues and update stub",
  "tags": [
   "comp:xla",
   "ready to pull",
   "size:M"
  ],
  "closed_time": "2022-08-10T21:33:22Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56682,
  "title": "[ROCm] Upstream rocm fixes 2",
  "tags": [
   "awaiting review",
   "comp:gpu",
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-07-07T15:45:24Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/compiler/xla/tests/convolution_test.cc",
    "code1": "XLA_TEST_F(ConvolutionHloTest, TestBooleanInput) {  const bool isCudaPlatform =      GetTestPlatform()->id() == stream_executor::cuda::kCudaPlatformId;  const bool isAutotuneDisabled =      GetDebugOptionsForTest().xla_gpu_autotune_level() == 0;                      isAutotuneDisabled ? \"Unimplemented convolution\"                                         : \"Unsupported convolution datatype\"));    }  } else {    EXPECT_TRUE(result);  }",
    "code2": "XLA_TEST_F(ConvolutionHloTest, TestBooleanInput) {  const bool isCudaPlatform =      GetTestPlatform()->id() == stream_executor::cuda::kCudaPlatformId;  const bool isROCmPlatform =      GetTestPlatform()->id() == stream_executor::rocm::kROCmPlatformId;  const bool isAutotuneDisabled =      GetDebugOptionsForTest().xla_gpu_autotune_level() == 0;                      isAutotuneDisabled ? \"Unimplemented convolution\"                                         : \"Unsupported convolution datatype\"));    }  } else if (isROCmPlatform) {    EXPECT_FALSE(result);  } else {    EXPECT_TRUE(result);  }",
    "method_name": "ENTRY TestComputation {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56631,
  "title": "Fixing a typo",
  "tags": [
   "comp:grappler",
   "size:XS",
   "type:docs-bug"
  ],
  "closed_time": "2022-08-09T13:54:09Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/grappler/costs/graph_properties.cc",
    "code1": " return OkStatus();  } VLOG(1) << \"Checking any conflics in shapes and dimensions ...\"; int64_t num_incompatible_shapes = 0; for (const NodeDef& node : graph_def.node()) { auto ctx = refiner->GetNodeContext(&node);",
    "code2": " return OkStatus();  } VLOG(1) << \"Checking any conflicts in shapes and dimensions ...\"; int64_t num_incompatible_shapes = 0; for (const NodeDef& node : graph_def.node()) { auto ctx = refiner->GetNodeContext(&node);",
    "method_name": "Status ValidateSymbolicShapeManager(const GraphDef& graph_def,"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56609,
  "title": "[ROCm] Upstream rocm fixes",
  "tags": [
   "comp:gpu",
   "size:S"
  ],
  "closed_time": "2022-07-11T11:10:36Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/compiler/xla/service/gpu/amdgpu_compiler.cc",
    "code1": "  // tf2xla bridge, DepthwiseConvolutionConverter and GpuConvRewriter  // introduces reshapes and transposes that can be eliminated using  // AlgebraicSimplifier  We run algsimp to a fixed point.  //  // When transposes appear in a fusion node, we can easily adjust the  // multi-dimensional index to create the one needed for the operand. This  // is not as easy with bitcasts, because we don't have the information  // readily available which dimensions are permuted. In addition to that,  // if we have a transpose and a reshape next to each other, they will both  // be replaced by a bitcast, and we replace bitcast(bitcast) with one  // bitcast. This leads to having to linearize and then delinearize the  // index.  AlgebraicSimplifierOptions options;  options.set_replace_transpose_with_bitcast(false);  options.set_enable_conv_operand_swap(false);  pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(options);",
    "code2": "  // tf2xla bridge, DepthwiseConvolutionConverter and GpuConvRewriter  // introduces reshapes and transposes that can be eliminated using  // AlgebraicSimplifier  We run algsimp to a fixed point.  AlgebraicSimplifierOptions options;  options.set_enable_conv_operand_swap(false);  pipeline.AddPass<HloPassFix<AlgebraicSimplifier>>(options);",
    "method_name": "Status AMDGPUCompiler::OptimizeHloConvolutionCanonicalization("
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56581,
  "title": "Fix several typos and grammar mistakes ",
  "tags": [
   "comp:core",
   "ready to pull",
   "size:S"
  ],
  "closed_time": "2022-07-29T16:27:52Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/core/util/strided_slice_op.cc",
    "code1": "bool StridedSliceAssignBCast::RemapDimensions(    int64_t num_dims, const StridedSliceAssignBCast::Vec& dimension_map) {  // Each element in the map corresponds to the original result shape, so  // the the sizes must be equal.  if (dimension_map.size() != result_shape_.size()) {    return false;  }",
    "code2": "bool StridedSliceAssignBCast::RemapDimensions(    int64_t num_dims, const StridedSliceAssignBCast::Vec& dimension_map) {  // Each element in the map corresponds to the original result shape, so  // the sizes must be equal.  if (dimension_map.size() != result_shape_.size()) {    return false;  }",
    "method_name": "StridedSliceAssignBCast::StridedSliceAssignBCast("
   },
   {
    "language": ".py",
    "dir": "tensorflow/python/eager/function.py",
    "code1": "      jit_compile=original_function._jit_compile)  # pylint: enable=protected-access  # We wrap the the bound method with tf_decorator so inspection works correctly  wrapped_instance_func = tf_decorator.make_decorator(bound_method,                                                      instance_func)  return wrapped_instance_func",
    "code2": "      jit_compile=original_function._jit_compile)  # pylint: enable=protected-access  # We wrap the bound method with tf_decorator so inspection works correctly  wrapped_instance_func = tf_decorator.make_decorator(bound_method,                                                      instance_func)  return wrapped_instance_func",
    "method_name": "def bound_method_wrapper(*args, **kwargs):"
   },
   {
    "language": ".py",
    "dir": "tensorflow/python/keras/mixed_precision/loss_scale_optimizer.py",
    "code1": "  examples of converting the use of the experimental class to the equivalent  non-experimental class.  >>> # In all of the the examples below, `opt1` and `opt2` are identical  >>> opt1 = tf.keras.mixed_precision.experimental.LossScaleOptimizer(  ...     tf.keras.optimizers.SGD(), loss_scale='dynamic')  >>> opt2 = tf.keras.mixed_precision.LossScaleOptimizer(",
    "code2": "  examples of converting the use of the experimental class to the equivalent  non-experimental class.  >>> # In all of the examples below, `opt1` and `opt2` are identical  >>> opt1 = tf.keras.mixed_precision.experimental.LossScaleOptimizer(  ...     tf.keras.optimizers.SGD(), loss_scale='dynamic')  >>> opt2 = tf.keras.mixed_precision.LossScaleOptimizer(",
    "method_name": "class LossScaleOptimizerV1(LossScaleOptimizer):"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56524,
  "title": "fixing the alloc dealloc behavior of jit execute",
  "tags": [
   "awaiting review",
   "ready to pull",
   "size:S"
  ],
  "closed_time": "2022-06-22T22:27:18Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/compiler/mlir/tools/kernel_gen/ir/tf_framework_ops.cc",
    "code1": "      .getResult();}::tensorflow::error::Code ConvertAttrToEnumValue(ErrorCode error_code) {  using ::tensorflow::error::Code;  switch (error_code) {",
    "code2": "      .getResult();}//===----------------------------------------------------------------------===//// JITExecuteOp//===----------------------------------------------------------------------===//Optional<Operation *> JITExecuteOp::buildDealloc(OpBuilder &builder, Value alloc) {  auto funcop = alloc.getParentRegion()->getParentOfType<func::FuncOp>();  return builder      .create<TFDeallocOp>(alloc.getLoc(), funcop.getArgument(0), alloc)      .getOperation();}Optional<Value> JITExecuteOp::buildClone(OpBuilder &builder, Value alloc) {  // TODO(herhut): We should have our own clone op if one of these survives.  return builder.create<mlir::bufferization::CloneOp>(alloc.getLoc(), alloc)      .getResult();}::tensorflow::error::Code ConvertAttrToEnumValue(ErrorCode error_code) {  using ::tensorflow::error::Code;  switch (error_code) {",
    "method_name": "Optional<Value> TFAllocOp::buildClone(OpBuilder &builder, Value alloc) {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56516,
  "title": "fix: bullet point list has been refactored",
  "tags": [
   "cla: yes",
   "size:S"
  ],
  "closed_time": "2022-08-09T15:37:51Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56494,
  "title": "Fix typo error of @tf.experimental.dispatch_for_api",
  "tags": [
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-06-22T01:25:56Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tensorflow/python/util/dispatch.py",
    "code1": "  >>> # Define a type and register a dispatcher to override `tf.abs`:  >>> class MyTensor(tf.experimental.ExtensionType):  ...   value: tf.Tensor  >>> @dispatch_for_api(tf.abs)  ... def my_abs(x: MyTensor):  ...   return MyTensor(tf.abs(x.value))  >>> tf.abs(MyTensor(5))",
    "code2": "  >>> # Define a type and register a dispatcher to override `tf.abs`:  >>> class MyTensor(tf.experimental.ExtensionType):  ...   value: tf.Tensor  >>> @tf.experimental.dispatch_for_api(tf.abs)  ... def my_abs(x: MyTensor):  ...   return MyTensor(tf.abs(x.value))  >>> tf.abs(MyTensor(5))",
    "method_name": "def unregister_dispatch_for(dispatch_target):"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56471,
  "title": "Fix a potential wrong operator bug",
  "tags": [
   "awaiting review",
   "comp:autograph",
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-08-19T00:42:57Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tensorflow/python/autograph/pyct/cfg.py",
    "code1": " def _add_new_node(self, ast_node): \"\"\"Grows the graph by adding a CFG node following the current leaves.\"\"\" if ast_node is self.node_index: raise ValueError('%s added twice' % ast_node) # Assumption: All CFG nodes have identical life spans, because the graph # owns them. Nodes should never be used outside the context of an existing",
    "code2": " def _add_new_node(self, ast_node): \"\"\"Grows the graph by adding a CFG node following the current leaves.\"\"\" if ast_node in self.node_index: raise ValueError('%s added twice' % ast_node) # Assumption: All CFG nodes have identical life spans, because the graph # owns them. Nodes should never be used outside the context of an existing",
    "method_name": "def _connect_nodes(self, first, second):"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56421,
  "title": "Fix type error of tf.experimental.BatchableExtensionType",
  "tags": [
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-06-13T14:39:20Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tensorflow/python/framework/extension_type.py",
    "code1": "  `BatchableExtensionType`s can be used with APIs that require batching or  unbatching, including `Keras`, `tf.data.Dataset`, and `tf.map_fn`.  E.g.:  >>> class Vehicle(BatchableExtensionType):  ...   top_speed: tf.Tensor  ...   mpg: tf.Tensor  >>> batch = Vehicle([120, 150, 80], [30, 40, 12])",
    "code2": "  `BatchableExtensionType`s can be used with APIs that require batching or  unbatching, including `Keras`, `tf.data.Dataset`, and `tf.map_fn`.  E.g.:  >>> class Vehicle(tf.experimental.BatchableExtensionType):  ...   top_speed: tf.Tensor  ...   mpg: tf.Tensor  >>> batch = Vehicle([120, 150, 80], [30, 40, 12])",
    "method_name": "class BatchableExtensionType(ExtensionType):"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56417,
  "title": "[R2.9] Fixed Build error on example/minimal.",
  "tags": [],
  "closed_time": "2022-06-21T01:51:28Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56400,
  "title": "Typo fix in command_line_flags comment",
  "tags": [
   "size:XS"
  ],
  "closed_time": "2022-06-09T06:43:58Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56394,
  "title": "[tf-mlir-translate] fixed bug in parsing command line input data types",
  "tags": [
   "awaiting review",
   "ready to pull",
   "size:M"
  ],
  "closed_time": "2022-06-09T10:24:57Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/compiler/mlir/tensorflow/translate/mlir_roundtrip_flags.cc",
    "code1": "    }    if (inside_subtype) continue;    if (c == ',') {      dtypes.push_back(std::string(data_types_str.substr(cur_pos, i)));      cur_pos = i + 1;    }  }  }  if (!data_types_str.empty()) {    dtypes.push_back(        std::string(data_types_str.substr(cur_pos, data_types_str.size())));  }  return dtypes;}",
    "code2": "    }    if (inside_subtype) continue;    if (c == ',') {      dtypes.push_back(std::string(data_types_str.substr(cur_pos, i - cur_pos)));      cur_pos = i + 1;    }  }  }  if (!data_types_str.empty()) {    dtypes.push_back(        std::string(data_types_str.substr(cur_pos, data_types_str.size() - cur_pos)));  }  return dtypes;}",
    "method_name": "static StatusOr<std::vector<std::string>> ParseDTypesHelper("
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56390,
  "title": "Change the documentation for TensorArray",
  "tags": [
   "awaiting review",
   "comp:ops",
   "ready to pull",
   "size:XS",
   "type:docs-bug"
  ],
  "closed_time": "2022-08-15T18:47:59Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tensorflow/python/ops/tensor_array_ops.py",
    "code1": "# pylint:disable=line-too-long@tf_export(\"TensorArray\")class TensorArray:  \"\"\"Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.  This class is meant to be used with dynamic iteration primitives such as  `while_loop` and `map_fn`.  It supports gradient back-propagation via special  \"flow\" control flow dependencies.  Example 1: Plain reading and writing.  >>> ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)",
    "code2": "# pylint:disable=line-too-long@tf_export(\"TensorArray\")class TensorArray:  \"\"\"Class wrapping dynamic-sized, per-time-step Tensor arrays.  This class is meant to be used with dynamic iteration primitives such as  `while_loop` and `map_fn`.  It supports gradient back-propagation via special  \"flow\" control flow dependencies.  Note that although the array can be read multiple times and positions can be  overwritten, behavior may be undefined when storing multiple references to  the same array and clear_after_read is False. In particular, avoid using  methods like concat() to convert an intermediate TensorArray to a Tensor,  then further modifying the TensorArray, particularly if you need to backprop  through it later.  Example 1: Plain reading and writing.  >>> ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)",
    "method_name": "def close(self, name=None):"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56388,
  "title": "Fixed tensorflow-lite label_image build error.",
  "tags": [
   "comp:lite",
   "size:XS"
  ],
  "closed_time": "2022-06-30T22:19:15Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56369,
  "title": "Fixed a bug in the xnnpack delegate option for label_image.",
  "tags": [
   "comp:lite",
   "prtype:bugfix",
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-06-12T22:30:31Z",
  "code_diffs": [
   {
    "language": ".cc",
    "dir": "tensorflow/lite/examples/label_image/label_image.cc",
    "code1": "                     \"XNNPACK delegate isn't supported on the platform!\";      } else {        params_.Set<bool>(\"use_xnnpack\", true);        params_.Set<bool>(\"num_threads\", s.number_of_threads);      }    }  }",
    "code2": "                     \"XNNPACK delegate isn't supported on the platform!\";      } else {        params_.Set<bool>(\"use_xnnpack\", true);        params_.Set<int32_t>(\"num_threads\", s.number_of_threads);      }    }  }",
    "method_name": "class DelegateProviders {"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56368,
  "title": "Fixed a bug in the xnnpack delegate option for label_image.",
  "tags": [
   "size:XL"
  ],
  "closed_time": "2022-06-06T12:36:54Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56351,
  "title": "Update nsync from 1.24.0 to 1.25.0.",
  "tags": [
   "comp:core",
   "prtype:bugfix",
   "size:XS"
  ],
  "closed_time": "2022-06-30T15:52:30Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56338,
  "title": "Trigger ARM CI on push commits for easy debug",
  "tags": [
   "awaiting review",
   "ready to pull",
   "size:S"
  ],
  "closed_time": "2022-06-03T20:47:54Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56221,
  "title": "Remove debug logs from saver.py",
  "tags": [
   "awaiting review",
   "ready to pull",
   "size:XS"
  ],
  "closed_time": "2022-05-25T19:04:42Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tensorflow/python/training/saver.py",
    "code1": "  # Gather all files beginning with prefix (.index plus sharded data files).  files = glob.glob(\"{}*\".format(prefix))  for file in files:    logging.info(file)    # Use TensorFlow's C++ FileSystem API.    size += metrics.CalculateFileSize(file)    logging.info(size)  return size",
    "code2": "  # Gather all files beginning with prefix (.index plus sharded data files).  files = glob.glob(\"{}*\".format(prefix))  for file in files:    # Use TensorFlow's C++ FileSystem API.    size += metrics.CalculateFileSize(file)  return size",
    "method_name": "def _get_checkpoint_size(prefix):"
   }
  ]
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56219,
  "title": "Patch Compute Library to pass correctly arguments to activation function",
  "tags": [
   "ready to pull",
   "size:S"
  ],
  "closed_time": "2022-07-21T22:35:12Z",
  "code_diffs": []
 },
 {
  "group_id": "tensorflow",
  "artifact_id": "tensorflow",
  "oid": 56209,
  "title": "Fix spelling mistakes",
  "tags": [
   "ready to pull",
   "size:S"
  ],
  "closed_time": "2022-06-13T14:39:20Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83697,
  "title": "[MPS] Fix torch.full for uint8",
  "tags": [
   "ciflow/mps",
   "cla signed",
   "Merged",
   "release notes: mps",
   "topic: bug fixes"
  ],
  "closed_time": "2022-08-18T21:59:26Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "test/test_mps.py",
    "code1": " self.assertEqual(t, t_mps.cpu()) # See https://github.com/pytorch/pytorch/issues/82427 # Test should not crash def test_bool_full(self): x = torch.full((3, 3), True, device='mps') # See https://github.com/pytorch/pytorch/issues/82663 def test_bool_expand(self):",
    "code2": " self.assertEqual(t, t_mps.cpu()) # See https://github.com/pytorch/pytorch/issues/82427 # and https://github.com/pytorch/pytorch/issues/83692 def test_full_bugs(self): # Test should not crash x = torch.full((3, 3), True, device='mps') # torch.full should work for uint8 y_mps = torch.full((2, 2), 247, device='mps', dtype=torch.uint8) y_cpu = torch.full((2, 2), 247, device='cpu', dtype=torch.uint8) self.assertEqual(y_mps, y_cpu) # See https://github.com/pytorch/pytorch/issues/82663 def test_bool_expand(self):",
    "method_name": "def test_storage_offset_greater_than_src_nbytes(self):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83645,
  "title": "fixing define_constant pybind signature to match std::complex scalar",
  "tags": [
   "cla signed",
   "Merged",
   "oncall: jit",
   "open source",
   "Reverted"
  ],
  "closed_time": "2022-08-18T04:52:37Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "torch/csrc/jit/codegen/cuda/python_frontend/python_bindings.cpp",
    "code1": "      .def( \"define_constant\",          [](nvfuser::FusionDefinition& self, c10::complex<double> val) -> nvfuser::Scalar* {            nvfuser::Scalar* out = self.defineScalar();            self.defineRecord(new nvfuser::ConstantRecord<                              torch::jit::fuser::cuda::ComplexDouble,                              c10::complex<double>>({out->index}, val)); return out;          },          py::return_value_policy::reference)",
    "code2": "      .def( \"define_constant\",          [](nvfuser::FusionDefinition& self, std::complex<double> val) -> nvfuser::Scalar* {            nvfuser::Scalar* out = self.defineScalar();            self.defineRecord(new nvfuser::ConstantRecord<                              torch::jit::fuser::cuda::ComplexDouble,                              c10::complex<double>>(                {out->index}, static_cast<c10::complex<double>>(val))); return out;          },          py::return_value_policy::reference)",
    "method_name": "void initNvFuserPythonBindings(PyObject* module) {"
   },
   {
    "language": ".py",
    "dir": "torch/testing/_internal/common_methods_invocations.py",
    "code1": "    PythonRefInfo(        \"_refs.addcdiv\",        torch_opinfo_name=\"addcdiv\",        supports_nvfuser=False,    ),    ElementwiseBinaryPythonRefInfo(        \"_refs.clamp_min\",",
    "code2": "    PythonRefInfo(        \"_refs.addcdiv\",        torch_opinfo_name=\"addcdiv\",    ),    ElementwiseBinaryPythonRefInfo(        \"_refs.clamp_min\",",
    "method_name": "def __init__("
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83644,
  "title": "Minifier fix for non tensor inputs",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-19T00:39:33Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "functorch/functorch/_src/fx_minifier.py",
    "code1": "# inplace modifies node/inpsdef _convert_node_to_placeholder(node, inps): if node.op == 'output': return node.op = 'placeholder' node.args = () node.target = node.name concrete_val = node.meta['concrete_value'] if isinstance(concrete_val, torch.Tensor): inps.append(concrete_val) else: if node.op == 'output': output = node break output_args = sorted(output.args[0], key=lambda x: x.idx if isinstance(x, fx.Node) else int(1e9)) if len(output_args) == 1: return None for idx in range(0, len(output_args), granularity): output.args = (output_args[:idx] + output_args[idx + granularity:],) if graph_fails(cur_graph, cur_inps):",
    "code2": "# inplace modifies node/inpsdef _convert_node_to_placeholder(node, inps): if node.op == 'output' or node.op == \"placeholder\": return node.op = 'placeholder' node.args = () node.target = node.name concrete_val = node.meta.get('concrete_value', None) if isinstance(concrete_val, torch.Tensor): inps.append(concrete_val) else: if node.op == 'output': output = node break output_args = sorted(output.args[0], key=lambda x: x.idx if isinstance(x, fx.Node) else int(1e9)) if len(output_args) == 1: return None for idx in range(0, len(output_args), granularity): output.args = (output_args[:idx] + output_args[idx + granularity:],) if graph_fails(cur_graph, cur_inps):",
    "method_name": "def propagate(self, *args):"
   },
   {
    "language": ".py",
    "dir": "functorch/test/test_minifier.py",
    "code1": " self.assertEqual(len(min_f.graph.nodes), 2) self.assertEqual(len(inps), 1)if __name__ == \"__main__\": run_tests()",
    "code2": " self.assertEqual(len(min_f.graph.nodes), 2) self.assertEqual(len(inps), 1) def test_tup_use(self): def f(a, b): tup = torch.std_mean(a) return (tup[0] + b * tup[1],) inps = [torch.randn(3), torch.randn(3)] def has_add(fx_g, inps): return (torch.ops.aten.add.Tensor in set([i.target for i in fx_g.graph.nodes])) failing_f = make_fx(f)(*inps) min_f, inps = minifier(failing_f, inps, has_add) self.assertEqual(len(min_f.graph.nodes), 4) self.assertEqual(len(inps), 2)if __name__ == \"__main__\": run_tests()",
    "method_name": "def inputs_returned(fx_g, inps):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83636,
  "title": "Bugfix4",
  "tags": [],
  "closed_time": "2022-08-17T23:50:31Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83633,
  "title": "[TESTING] Debug API rate limit",
  "tags": [
   "cla signed"
  ],
  "closed_time": "2022-08-18T07:13:23Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": ".github/scripts/get_workflow_job_id.py",
    "code1": "    \"Authorization\": \"token \" + GITHUB_TOKEN,}response = requests.get(    f\"{PYTORCH_GITHUB_API}/actions/runs/{args.workflow_run_id}/jobs?per_page=100\",    headers=REQUEST_HEADERS,",
    "code2": "    \"Authorization\": \"token \" + GITHUB_TOKEN,}print(f\"=== HEADERS {REQUEST_HEADERS}\")debug = requests.get(\"https://api.github.com/rate_limit\", headers=REQUEST_HEADERS)print(f\"==== RESPONSE {debug.json()}\")response = requests.get(    f\"{PYTORCH_GITHUB_API}/actions/runs/{args.workflow_run_id}/jobs?per_page=100\",    headers=REQUEST_HEADERS,",
    "method_name": ""
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83623,
  "title": "Release the GIL when munmap'ing tensors - fixes #77139",
  "tags": [
   "cla signed",
   "Merged",
   "open source"
  ],
  "closed_time": "2022-08-18T15:24:25Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "torch/csrc/autograd/python_variable.cpp",
    "code1": "    }  } TORCH_INTERNAL_ASSERT(!isResurrectable((THPVariable*)self));  self->cdata = MaybeOwned<Variable>(); return 0;}",
    "code2": "    }  } TORCH_INTERNAL_ASSERT(!isResurrectable((THPVariable*)self));  { // MapAllocator can take significant time to release large tensors; // release the GIL here to avoid impacting main thread perf.    pybind11::gil_scoped_release no_gil;    self->cdata = MaybeOwned<Variable>();  } return 0;}",
    "method_name": "static int THPVariable_clear(THPVariable* self) {"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83608,
  "title": "Reset compile cache to fix flaky test",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-17T20:12:23Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "functorch/test/test_pythonkey.py",
    "code1": "class TestAOTAutograd(TestCase):    def verify_aot_autograd(self, f, inp):        if isinstance(f, nn.Module):            compiled_f = aot_module(f, nop)",
    "code2": "class TestAOTAutograd(TestCase):    def setUp(self):        super().setUp()        clear_compile_cache()    def verify_aot_autograd(self, f, inp):        if isinstance(f, nn.Module):            compiled_f = aot_module(f, nop)",
    "method_name": "def _outs_and_grads(fn, inps):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83489,
  "title": "S3 third-party deps sync workflow: specify correct secrets",
  "tags": [
   "cla signed",
   "Merged",
   "Reverted",
   "topic: not user facing"
  ],
  "closed_time": "2022-08-16T02:16:19Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83484,
  "title": "[BE] Better test stats errors",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-16T07:51:16Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tools/stats/import_test_stats.py",
    "code1": "def get_test_times(dirpath: str, filename: str) -> Dict[str, Dict[str, float]]:    url = \"https://raw.githubusercontent.com/pytorch/test-infra/generated-stats/stats/test-times.json\"    def process_response(the_response: Dict[str, Any]) -> Any:        build_environment = os.environ[\"BUILD_ENVIRONMENT\"]        return the_response[build_environment]    try:",
    "code2": "def get_test_times(dirpath: str, filename: str) -> Dict[str, Dict[str, float]]:    url = \"https://raw.githubusercontent.com/pytorch/test-infra/generated-stats/stats/test-times.json\"    build_environment = os.environ.get(\"BUILD_ENVIRONMENT\")    if build_environment is None:        test_times = fetch_and_cache(dirpath, filename, url, lambda x: x)        raise RuntimeError(            f\"BUILD_ENVIRONMENT is not defined, available keys are {test_times.keys()}\"        )    def process_response(the_response: Dict[str, Any]) -> Any:        if build_environment not in the_response:            raise RuntimeError(                f\"{build_environment} not found, available envs are: {the_response.keys()}\"            )        return the_response[build_environment]    try:",
    "method_name": "def get_slow_tests("
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83463,
  "title": "[FuncTorch] Fix compilation with -Werror",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-16T07:50:25Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/CompileCache.cpp",
    "code1": "    cache_.emplace(cacheKey, compileFn);  } const int64_t size() const { return cache_.size(); } /// Clear the cache. void clear() { cache_.clear(); }",
    "code2": "    cache_.emplace(cacheKey, compileFn);  } int64_t size() const { return cache_.size(); } /// Clear the cache. void clear() { cache_.clear(); }",
    "method_name": "struct CompileCache {"
   },
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/DynamicLayer.cpp",
    "code1": " return VmapInterpreterPtr(&interpreter_).randomness();}constexpr DispatchKeySet kFrontBackKeys({kDynamicLayerBackModeKey, kDynamicLayerFrontModeKey});using DynmetaData = std::unordered_map<int64_t, std::shared_ptr<bool>>;DynmetaData kDynMetaDataSingleton;",
    "code2": " return VmapInterpreterPtr(&interpreter_).randomness();}using DynmetaData = std::unordered_map<int64_t, std::shared_ptr<bool>>;DynmetaData kDynMetaDataSingleton;",
    "method_name": "RandomnessType DynamicLayer::randomness() const {"
   },
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/dim/dim.cpp",
    "code1": "    0,                              /* tp_descr_get */    0,                              /* tp_descr_set */    0,                              /* tp_dictoffset */    (initproc) Dim_init,      /* tp_init */    0,                              /* tp_alloc */    Dim::new_stub,                      /* tp_new */};        } else {            bound_ = true;            dims_.resize(size);            for (ssize_t i = 0; i < size; ++i) {                dims_[i] = Dim::create(py::unicode_from_format(\"%S%i\", name_.ptr(), (int)i));            }        }    py::sequence_view seq = sizes;    auto size = seq.size();    self->bind_len(size);    for (ssize_t i = 0; i < size; ++i) {        self->dims_[i]->set_size(py::to_int(seq[i]));    }    Py_RETURN_NONE;}static PyMethodDef DimList_methods[] = {    {\"bind\", (PyCFunction) DimList_bind, METH_FASTCALL | METH_KEYWORDS},    {\"bind_len\", (PyCFunction) DimList_bind_len, METH_FASTCALL | METH_KEYWORDS},    {NULL, NULL, 0, NULL}        /* Sentinel */};    if (!self->is_bound()) {        py::raise_error(DimensionBindError(), \"DimList not bound\");    }    if (idx >= self->dims_.size()) {        py::raise_error(PyExc_IndexError, \"index out of bounds\");    }    py::object r = self->dims_[idx];PyMappingMethods DimList_mapping = {    0, //lenfunc mp_length;    (binaryfunc) DimList_subscript, //binaryfunc mp_subscript;    0, //objobjargproc mp_ass_subscript;};            std::vector<py::obj<Dim>> dims;            size_t size = s.size();            dims.reserve(size);            for (ssize_t i = 0; i < size; ++i) {                auto r = s[i];                if (py::is_int(r)) {                    dims.emplace_back(Dim::create(py::unicode_from_format(\"%S%i\", self->name_.ptr(), (int)i),  py::to_int(r)));    PY_END(nullptr)}PyMethodDef py_unflatten_def = {\"unflatten\", (PyCFunction) py_unflatten, METH_FASTCALL | METH_KEYWORDS};void free_unflatten_arena(PyObject * pc) {    delete (UnflattenArena*) PyCapsule_GetPointer(pc, \"arena\");    if (!py::is_none(size)) {        d->set_size(py::to_int(size));    }    return d;}py::object create_dimlist(py::object name, py::handle size) {            }        }    }    return d;}template<py::object (*create_object)(py::object, py::handle)>    if (lhs_list && rhs_list) {        py::sequence_view dv(dims);        py::sequence_view ind(indices); size_t N = dv.size();        if (N != ind.size()) {            py::raise_error(PyExc_TypeError, \"dims (%d) and indices (%d) must have the same length\", int(N), int(ind.size()));        }    } else {        dim_name_str = \"dim\";    }    auto info = WrappedOperator::create(py::object::borrow(orig), (PyCFunction) patched_dim_method, std::move(dim_name_str));    if (dim_offset.ptr()) {        info->dim_offset = py::to_int(dim_offset);    }        auto dim = py::import(\"functorch.dim\");        pointwise = dim.attr(\"pointwise\");    }    auto info = WrappedOperator::create(py::object::borrow(orig), (PyCFunction) call_torch_function);    info->is_pointwise = pointwise.contains(orig);    return PyInstanceMethod_New(info->function().release());    PY_END(nullptr);)\"\"\";static PyMethodDef methods[] = {    {\"dims\", (PyCFunction) _dims<create_dim>, METH_FASTCALL | METH_KEYWORDS, dims_doc},    {\"dimlists\", (PyCFunction) _dims<create_dimlist>, METH_FASTCALL | METH_KEYWORDS},    {\"_test_c\", (PyCFunction) test_c, METH_FASTCALL | METH_KEYWORDS},    {\"_wrap_method\", (PyCFunction) _wrap_method, METH_FASTCALL | METH_KEYWORDS},    {\"Tensor_from_positional\", (PyCFunction) py_Tensor_from_positional, METH_FASTCALL | METH_KEYWORDS},    {\"__torch_function__\", (PyCFunction) py___torch_function__, METH_FASTCALL | METH_KEYWORDS},    {\"tree_flatten\", (PyCFunction) py_tree_flatten, METH_FASTCALL | METH_KEYWORDS},    {\"order\", (PyCFunction) order, METH_FASTCALL | METH_KEYWORDS},    {\"index\", (PyCFunction) py_index, METH_FASTCALL | METH_KEYWORDS},    {\"stack\", (PyCFunction) py_stack, METH_FASTCALL | METH_KEYWORDS},    {\"split\", (PyCFunction) py_split, METH_FASTCALL | METH_KEYWORDS},    {\"expand\", (PyCFunction) expand, METH_FASTCALL | METH_KEYWORDS},    {\"__getitem__\", (PyCFunction) py___getitem__, METH_FASTCALL | METH_KEYWORDS},    {\"__setitem__\", (PyCFunction) py___setitem__, METH_FASTCALL | METH_KEYWORDS},    {\"_wrap\", (PyCFunction) _wrap, METH_FASTCALL | METH_KEYWORDS},    {\"Tensor_sum\", (PyCFunction) Tensor_sum, METH_FASTCALL | METH_KEYWORDS},    {\"_parse_test\", (PyCFunction) _parse_test, METH_FASTCALL | METH_KEYWORDS},    {\"_set_pointwise_optimize\", (PyCFunction) _set_pointwise_optimize, METH_FASTCALL | METH_KEYWORDS},    {\"_patch_tensor_class\", (PyCFunction) _patch_tensor_class, METH_FASTCALL | METH_KEYWORDS},    {NULL, NULL, 0, NULL}        /* Sentinel */};",
    "code2": "    0,                              /* tp_descr_get */    0,                              /* tp_descr_set */    0,                              /* tp_dictoffset */    (initproc)(void*) Dim_init,     /* tp_init */    0,                              /* tp_alloc */    Dim::new_stub,                      /* tp_new */};        } else {            bound_ = true;            dims_.resize(size);            for (Py_ssize_t i = 0; i < size; ++i) {                dims_[i] = Dim::create(py::unicode_from_format(\"%S%i\", name_.ptr(), (int)i));            }        }    py::sequence_view seq = sizes;    auto size = seq.size();    self->bind_len(size);    for (Py_ssize_t i = 0; i < size; ++i) {        self->dims_[i]->set_size(py::to_int(seq[i]));    }    Py_RETURN_NONE;}static PyMethodDef DimList_methods[] = {    {\"bind\", (PyCFunction)(void*) DimList_bind, METH_FASTCALL | METH_KEYWORDS},    {\"bind_len\", (PyCFunction)(void*) DimList_bind_len, METH_FASTCALL | METH_KEYWORDS},    {NULL, NULL, 0, NULL}        /* Sentinel */};    if (!self->is_bound()) {        py::raise_error(DimensionBindError(), \"DimList not bound\");    }    if (idx < 0 || (size_t) idx >= self->dims_.size()) {        py::raise_error(PyExc_IndexError, \"index out of bounds\");    }    py::object r = self->dims_[idx];PyMappingMethods DimList_mapping = {    0, //lenfunc mp_length;    (binaryfunc)(void*) DimList_subscript, //binaryfunc mp_subscript;    0, //objobjargproc mp_ass_subscript;};            std::vector<py::obj<Dim>> dims;            size_t size = s.size();            dims.reserve(size);            for (size_t i = 0; i < size; ++i) {                auto r = s[i];                if (py::is_int(r)) {                    dims.emplace_back(Dim::create(py::unicode_from_format(\"%S%i\", self->name_.ptr(), (int)i),  py::to_int(r)));    PY_END(nullptr)}PyMethodDef py_unflatten_def = {\"unflatten\", (PyCFunction)(void*) py_unflatten, METH_FASTCALL | METH_KEYWORDS};void free_unflatten_arena(PyObject * pc) {    delete (UnflattenArena*) PyCapsule_GetPointer(pc, \"arena\");    if (!py::is_none(size)) {        d->set_size(py::to_int(size));    }    return std::move(d);}py::object create_dimlist(py::object name, py::handle size) {            }        }    }    return std::move(d);}template<py::object (*create_object)(py::object, py::handle)>    if (lhs_list && rhs_list) {        py::sequence_view dv(dims);        py::sequence_view ind(indices); Py_ssize_t N = dv.size();        if (N != ind.size()) {            py::raise_error(PyExc_TypeError, \"dims (%d) and indices (%d) must have the same length\", int(N), int(ind.size()));        }    } else {        dim_name_str = \"dim\";    }    auto info = WrappedOperator::create(py::object::borrow(orig), (PyCFunction)(void*) patched_dim_method, std::move(dim_name_str));    if (dim_offset.ptr()) {        info->dim_offset = py::to_int(dim_offset);    }        auto dim = py::import(\"functorch.dim\");        pointwise = dim.attr(\"pointwise\");    }    auto info = WrappedOperator::create(py::object::borrow(orig), (PyCFunction)(void*) call_torch_function);    info->is_pointwise = pointwise.contains(orig);    return PyInstanceMethod_New(info->function().release());    PY_END(nullptr);)\"\"\";static PyMethodDef methods[] = {    {\"dims\", (PyCFunction)(void*) _dims<create_dim>, METH_FASTCALL | METH_KEYWORDS, dims_doc},    {\"dimlists\", (PyCFunction)(void*) _dims<create_dimlist>, METH_FASTCALL | METH_KEYWORDS},    {\"_test_c\", (PyCFunction)(void*) test_c, METH_FASTCALL | METH_KEYWORDS},    {\"_wrap_method\", (PyCFunction)(void*) _wrap_method, METH_FASTCALL | METH_KEYWORDS},    {\"Tensor_from_positional\", (PyCFunction)(void*) py_Tensor_from_positional, METH_FASTCALL | METH_KEYWORDS},    {\"__torch_function__\", (PyCFunction)(void*) py___torch_function__, METH_FASTCALL | METH_KEYWORDS},    {\"tree_flatten\", (PyCFunction)(void*) py_tree_flatten, METH_FASTCALL | METH_KEYWORDS},    {\"order\", (PyCFunction)(void*) order, METH_FASTCALL | METH_KEYWORDS},    {\"index\", (PyCFunction)(void*) py_index, METH_FASTCALL | METH_KEYWORDS},    {\"stack\", (PyCFunction)(void*) py_stack, METH_FASTCALL | METH_KEYWORDS},    {\"split\", (PyCFunction)(void*) py_split, METH_FASTCALL | METH_KEYWORDS},    {\"expand\", (PyCFunction)(void*) expand, METH_FASTCALL | METH_KEYWORDS},    {\"__getitem__\", (PyCFunction)(void*) py___getitem__, METH_FASTCALL | METH_KEYWORDS},    {\"__setitem__\", (PyCFunction)(void*) py___setitem__, METH_FASTCALL | METH_KEYWORDS},    {\"_wrap\", (PyCFunction)(void*) _wrap, METH_FASTCALL | METH_KEYWORDS},    {\"Tensor_sum\", (PyCFunction)(void*) Tensor_sum, METH_FASTCALL | METH_KEYWORDS},    {\"_parse_test\", (PyCFunction)(void*) _parse_test, METH_FASTCALL | METH_KEYWORDS},    {\"_set_pointwise_optimize\", (PyCFunction)(void*) _set_pointwise_optimize, METH_FASTCALL | METH_KEYWORDS},    {\"_patch_tensor_class\", (PyCFunction)(void*) _patch_tensor_class, METH_FASTCALL | METH_KEYWORDS},    {NULL, NULL, 0, NULL}        /* Sentinel */};",
    "method_name": "PyTypeObject Dim::Type = {"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83457,
  "title": "fix native_layer_norm meta kernel parity w cuda",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-16T23:38:07Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/layer_norm.cpp",
    "code1": " at::empty_like(input, c10::TensorOptions().dtype(result_type))    );  }  at::Tensor input_reshaped = input.view({1, M, -1}); // Unlike Batch Normalization, which applies scalar scale and bias for each // entire channel/plane with the affine option, Layer Normalization applies // per-element scale and bias. E.g. For input {N, C, H, W}, weight for",
    "code2": " at::empty_like(input, c10::TensorOptions().dtype(result_type))    );  }  at::Tensor input_reshaped = input.reshape({1, M, -1}); // Unlike Batch Normalization, which applies scalar scale and bias for each // entire channel/plane with the affine option, Layer Normalization applies // per-element scale and bias. E.g. For input {N, C, H, W}, weight for",
    "method_name": "std::tuple<Tensor, Tensor, Tensor> math_native_layer_norm("
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83432,
  "title": "[vulkan][fix] Fix unsafe direct array access",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-16T17:30:57Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/vulkan/ops/Utils.cpp",
    "code1": "      },  };  api::UniformParamsBuffer params(context, block);  api::PipelineBarrier pipeline_barrier{};  bool is_quantized = v_self.is_quantized();  api::utils::uvec3 copy_extents;  copy_extents.data[0u] = 1;  copy_extents.data[1u] = 1;  copy_extents.data[2u] =      ((v_self.sizes()[1] * v_self.sizes()[2] * v_self.sizes()[3]) / 4);  api::ShaderSource kernel = is_quantized ? VK_KERNEL(image_to_nchw_quantized)                                          : VK_KERNEL(image_to_nchw);  api::utils::uvec3 extents_to_use = is_quantized ? copy_extents : extents;  context->submit_compute_job(      // shader descriptor      kernel,      // pipeline barrier      pipeline_barrier,      // global work group size extents_to_use,      // local work group size      adaptive_work_group_size(extents_to_use),      // fence handle      fence_handle,      // shader arguments",
    "code2": "      },  };  bool is_quantized = v_self.is_quantized();  api::utils::uvec3 pack_extents = extents;  if (is_quantized) {    pack_extents.data[0u] = 1;    pack_extents.data[1u] = 1;    pack_extents.data[2u] =        api::utils::safe_downcast<uint32_t>(v_self.numtexels());  }  api::UniformParamsBuffer params(context, block);  api::PipelineBarrier pipeline_barrier{};  api::ShaderSource kernel = is_quantized ? VK_KERNEL(image_to_nchw_quantized)                                          : VK_KERNEL(image_to_nchw);  context->submit_compute_job(      // shader descriptor      kernel,      // pipeline barrier      pipeline_barrier,      // global work group size pack_extents,      // local work group size      adaptive_work_group_size(pack_extents),      // fence handle      fence_handle,      // shader arguments",
    "method_name": "void pack_vtensor_to_staging("
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83424,
  "title": "shard trunk / linux-bionic-cuda10.2-py3.9-gcc7 / test (default from 2 -> 4",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-15T20:03:13Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83395,
  "title": "Fix issue with compiling under with_grad",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-15T16:08:50Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "functorch/functorch/_src/aot_autograd.py",
    "code1": "        if config.debug_graphs:            print(fw_module.code, bw_module.code)        with track_graph_compiling(\"forward\"):            compiled_fw = aot_config.fw_compiler(fw_module, flat_args)        # TODO: Delay this backwards compilation until the backwards pass        with torch.no_grad():            fw_outs = call_func_with_args(compiled_fw, flat_args)        if config.debug_partitioner:            activation_sizes = 0            for out in fw_outs[num_outs:]:                if isinstance(out, torch.Tensor):                    activation_sizes += out.storage().nbytes()            print(f\"Real Activations Stored(GB): {activation_sizes/1e9}\")        bw_args = fw_outs[num_outs:] + fw_outs[0:num_outs]        with track_graph_compiling(\"backward\", True):            compiled_bw = aot_config.bw_compiler(bw_module, bw_args)    class CompiledFunction(torch.autograd.Function):        @staticmethod",
    "code2": "        if config.debug_graphs:            print(fw_module.code, bw_module.code)        with torch.no_grad():            with track_graph_compiling(\"forward\"):                compiled_fw = aot_config.fw_compiler(fw_module, flat_args)            # TODO: Delay this backwards compilation until the backwards pass            with torch.no_grad():                fw_outs = call_func_with_args(compiled_fw, flat_args)            if config.debug_partitioner:                activation_sizes = 0                for out in fw_outs[num_outs:]:                    if isinstance(out, torch.Tensor):                        activation_sizes += out.storage().nbytes()                print(f\"Real Activations Stored(GB): {activation_sizes/1e9}\")            bw_args = fw_outs[num_outs:] + fw_outs[0:num_outs]            with track_graph_compiling(\"backward\", True):                compiled_bw = aot_config.bw_compiler(bw_module, bw_args)    class CompiledFunction(torch.autograd.Function):        @staticmethod",
    "method_name": "def fake_fn(primals, tangents):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83391,
  "title": "[fix] cat : support different dtype tensor with 0-dim like before",
  "tags": [
   "cla signed",
   "Merged",
   "open source",
   "triaged"
  ],
  "closed_time": "2022-08-15T14:32:01Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "aten/src/ATen/native/TensorShape.cpp",
    "code1": "    size_t size_at_dim = 0;    for (const auto i : c10::irange(materialized.size())) {      const Tensor& t = materialized[i];      if (!at::native::cat_should_skip_tensor(t)) {        at::native::check_cat_shape_except_dim(materialized[valid], t, dim, i);        size_at_dim += t.size(dim);        all_contiguous = all_contiguous && t.is_contiguous(memory_format);        all_same_dtype = all_same_dtype && out_dtype == t.scalar_type();        all_same_sizes_and_stride = all_same_sizes_and_stride &&            t.sizes() == materialized[valid].get().sizes() &&            t.strides() == materialized[valid].get().strides();",
    "code2": "    size_t size_at_dim = 0;    for (const auto i : c10::irange(materialized.size())) {      const Tensor& t = materialized[i];      all_same_dtype = all_same_dtype && out_dtype == t.scalar_type();      if (!at::native::cat_should_skip_tensor(t)) {        at::native::check_cat_shape_except_dim(materialized[valid], t, dim, i);        size_at_dim += t.size(dim);        all_contiguous = all_contiguous && t.is_contiguous(memory_format);        all_same_sizes_and_stride = all_same_sizes_and_stride &&            t.sizes() == materialized[valid].get().sizes() &&            t.strides() == materialized[valid].get().strides();",
    "method_name": "TORCH_PRECOMPUTE_META_FUNC(cat)(ITensorListRef tensors, int64_t dim) {"
   },
   {
    "language": ".py",
    "dir": "test/test_type_promotion.py",
    "code1": "            res = torch.cat([x, y])            self.assertEqual(res, expected_res, exact_dtype=True)    @onlyNativeDeviceTypes    def test_cat_out_different_dtypes(self, device):        dtypes = all_types_and_complex_and(torch.half)",
    "code2": "            res = torch.cat([x, y])            self.assertEqual(res, expected_res, exact_dtype=True)            # cat: full and an empty tensor.            y = torch.tensor([], device=device, dtype=y_dtype)            res_dtype = torch.result_type(x, y)            expected_res = torch.tensor(x_vals + [], device=device, dtype=res_dtype)            res = torch.cat([x, y])            self.assertEqual(res, expected_res, exact_dtype=True)    @onlyNativeDeviceTypes    def test_cat_out_different_dtypes(self, device):        dtypes = all_types_and_complex_and(torch.half)",
    "method_name": "def test_cat_different_dtypes(self, device):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83377,
  "title": "Grammatically docs fix",
  "tags": [
   "cla signed",
   "open source"
  ],
  "closed_time": "2022-08-14T05:04:38Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "torch/_lobpcg.py",
    "code1": "def _symeig_backward_complete_eigenspace(D_grad, U_grad, A, D, U):    # compute F, such that F_ij = (d_j - d_i)^{-1} for i != j, F_ii = 0    F = D.unsqueeze(-2) - D.unsqueeze(-1)    F.diagonal(dim1=-2, dim2=-1).fill_(float(\"inf\"))    F.pow_(-1)    proj_U_ortho = -U.matmul(Ut)    proj_U_ortho.diagonal(dim1=-2, dim2=-1).add_(1)    # compute U_ortho, a basis for the orthogonal complement to the span(U),    # by projecting a random [..., m, m - k] matrix onto the subspace spanned    # by the columns of U.    #        series_acc += U_grad_projected * poly_D.unsqueeze(-2)        U_grad_projected = A.matmul(U_grad_projected)    # compute chr_poly_D(A) which essentially is:    #    # chr_poly_D_at_A = A.new_zeros(A.shape)    # for k in range(chr_poly_D.size(-1)):def _symeig_backward(D_grad, U_grad, A, D, U, largest):    # if `U` is square, then the columns of `U` is a complete eigenspace    if U.size(-1) == U.size(-2):        return _symeig_backward_complete_eigenspace(D_grad, U_grad, A, D, U)    else:        if largest is None:            largest = True        # symeig backward        if B is None:            A_grad = _symeig_backward(D_grad, U_grad, A, D, U, largest)      not recommended but there exist cases where the usage of the      basic method may be preferred.    .. warning:: The backward method does not support sparse and complex inputs.      It works only when `B` is not provided (i.e. `B == None`).      We are actively working on extensions, and the details of      the algorithms are going to be published promptly.",
    "code2": "def _symeig_backward_complete_eigenspace(D_grad, U_grad, A, D, U):    # Compute F, such that F_ij = (d_j - d_i)^{-1} for i != j, F_ii = 0    F = D.unsqueeze(-2) - D.unsqueeze(-1)    F.diagonal(dim1=-2, dim2=-1).fill_(float(\"inf\"))    F.pow_(-1)    proj_U_ortho = -U.matmul(Ut)    proj_U_ortho.diagonal(dim1=-2, dim2=-1).add_(1)    # Compute U_ortho, a basis for the orthogonal complement to the span(U),    # by projecting a random [..., m, m - k] matrix onto the subspace spanned    # by the columns of U.    #        series_acc += U_grad_projected * poly_D.unsqueeze(-2)        U_grad_projected = A.matmul(U_grad_projected)    # Compute chr_poly_D(A) which essentially is:    #    # chr_poly_D_at_A = A.new_zeros(A.shape)    # for k in range(chr_poly_D.size(-1)):def _symeig_backward(D_grad, U_grad, A, D, U, largest):    # If `U` is square, then the columns of `U` is a complete eigenspace    if U.size(-1) == U.size(-2):        return _symeig_backward_complete_eigenspace(D_grad, U_grad, A, D, U)    else:        if largest is None:            largest = True        # Symeig backward        if B is None:            A_grad = _symeig_backward(D_grad, U_grad, A, D, U, largest)      not recommended but there exist cases where the usage of the      basic method may be preferred.    .. arning:: The backward method does not support sparse and complex inputs.      It works only when `B` is not provided (i.e. `B == None`).      We are actively working on extensions, and the details of      the algorithms are going to be published promptly.",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "torch/_torch_docs.py",
    "code1": "common_args = parse_kwargs(    \"\"\" input (Tensor): the input tensor.    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling    out (Tensor, optional): the output tensor.    memory_format (:class:`torch.memory_format`, optional): the desired memory format of",
    "code2": "common_args = parse_kwargs(    \"\"\" Input (Tensor): the input tensor.    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling    out (Tensor, optional): the output tensor.    memory_format (:class:`torch.memory_format`, optional): the desired memory format of",
    "method_name": "def merge_dicts(*dicts):"
   },
   {
    "language": ".py",
    "dir": "torch/_utils_internal.py",
    "code1": "import tempfile# this arbitrary-looking assortment of functionality is provided here# to have a central place for overrideable behavior. The motivating# use is the FB build environment, where this source file is replaced# by an equivalent.",
    "code2": "import tempfile# This arbitrary-looking assortment of functionality is provided here# to have a central place for overrideable behavior. The motivating# use is the FB build environment, where this source file is replaced# by an equivalent.",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "torch/hub.py",
    "code1": "from urllib.parse import urlparse  # noqa: F401try:    from tqdm.auto import tqdm  # automatically select proper tqdm submodule if availableexcept ImportError:    try:        from tqdm import tqdm    except ImportError:        # fake tqdm if it's not installed        class tqdm(object):  # type: ignore[no-redef]            def __init__(self, total=None, disable=False,                         unit=None, unit_scale=None, unit_divisor=None):                self.total = total                self.disable = disable                self.n = 0                # ignore unit, unit_scale, unit_divisor; they're just for real tqdm            def update(self, n):                if self.disable:    'set_dir',]# matches bfd8deac from resnet18-bfd8deac.pthHASH_REGEX = re.compile(r'-([a-f0-9]*)\\.')_TRUSTED_REPO_OWNERS = (\"facebookresearch\", \"facebookincubator\", \"pytorch\", \"fairinternal\")",
    "code2": "from urllib.parse import urlparse  # noqa: F401try:    from tqdm.auto import tqdm  # Automatically select proper tqdm submodule if availableexcept ImportError:    try:        from tqdm import tqdm    except ImportError:        # Fake tqdm if it's not installed        class tqdm(object):  # type: ignore[no-redef]            def __init__(self, total=None, disable=False,                         unit=None, unit_scale=None, unit_divisor=None):                self.total = total                self.disable = disable                self.n = 0                # Ignore unit, unit_scale, unit_divisor; they're just for real tqdm            def update(self, n):                if self.disable:    'set_dir',]# Matches bfd8deac from resnet18-bfd8deac.pthHASH_REGEX = re.compile(r'-([a-f0-9]*)\\.')_TRUSTED_REPO_OWNERS = (\"facebookresearch\", \"facebookincubator\", \"pytorch\", \"fairinternal\")",
    "method_name": ""
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83341,
  "title": "fix quantization/core/test_docs for Buck2",
  "tags": [
   "cla signed",
   "fb-exported",
   "Merged"
  ],
  "closed_time": "2022-08-18T13:03:05Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "test/quantization/core/test_docs.py",
    "code1": "        def get_correct_path(path_from_pytorch):            r\"\"\"            Current working directory when CI is running test seems to vary, this function            looks for docs and if it finds it looks for the path to the            file and if the file exists returns that path, otherwise keeps looking. Will            only work if cwd contains pytorch or docs or a parent contains docs.            \"\"\"            # get cwd            cur_dir_path = Path(\".\").resolve()            # check if cwd contains pytorch, use that if it does            if (cur_dir_path / \"pytorch\").is_dir():                cur_dir_path = (cur_dir_path / \"pytorch\").resolve()            # need to find the file, so we check current directory            # and all parent directories to see if the path leads to it            check_dir = cur_dir_path            while not check_dir == check_dir.parent:                file_path = (check_dir / path_from_pytorch).resolve()                if file_path.is_file():                    return file_path                check_dir = check_dir.parent.resolve()            # no longer passing when file not found            raise FileNotFoundError(\"could not find {}\".format(path_from_pytorch))        path_to_file = get_correct_path(path_from_pytorch)        if path_to_file:",
    "code2": "        def get_correct_path(path_from_pytorch):            r\"\"\"            Current working directory when CI is running test seems to vary, this function            looks for docs relative to this test file.            \"\"\"            core_dir = Path(__file__).parent            assert core_dir.match(\"test/quantization/core/\"), (                \"test_docs.py is in an unexpected location. If you've been \"                \"moving files around, ensure that the test and build files have \"                \"been updated to have the correct relative path between \"                \"test_docs.py and the docs.\"            )            pytorch_root = core_dir.parent.parent.parent            return pytorch_root / path_from_pytorch        path_to_file = get_correct_path(path_from_pytorch)        if path_to_file:",
    "method_name": "def _get_code("
   },
   {
    "language": ".py",
    "dir": "test/test_quantization.py",
    "code1": "from quantization.core.test_workflow_module import TestFusedObsFakeQuantModule  # noqa: F401from quantization.core.test_backend_config import TestBackendConfig  # noqa: F401from quantization.core.test_utils import TestUtils  # noqa: F401from quantization.core.test_docs import TestQuantizationDocs  # noqa: F401# Eager Mode Workflow. Tests for the functionality of APIs and different features implemented# using eager mode.",
    "code2": "from quantization.core.test_workflow_module import TestFusedObsFakeQuantModule  # noqa: F401from quantization.core.test_backend_config import TestBackendConfig  # noqa: F401from quantization.core.test_utils import TestUtils  # noqa: F401try:    # This test has extra data dependencies, so in some environments, e.g. Meta internal    # Buck, it has its own test runner.    from quantization.core.test_docs import TestQuantizationDocs  # noqa: F401except ImportError:    pass# Eager Mode Workflow. Tests for the functionality of APIs and different features implemented# using eager mode.",
    "method_name": ""
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83339,
  "title": "fix resnet50_quantized_qat and mobilenet_v2_quantized_qat <> functionalization",
  "tags": [
   "cla signed"
  ],
  "closed_time": "2022-08-16T23:38:06Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "functorch/functorch/_src/aot_autograd.py",
    "code1": "    with preserve_rng_state(), fake_mode as mode:        def process_inputs(flat_args):            flat_args = pytree.tree_map(                lambda x: x.detach().requires_grad_(x.requires_grad)                if isinstance(x, Tensor)                else x,                flat_args,            )            fake_flat_tensor_args = pytree.tree_map(                lambda x: mode.from_tensor(x)                if mode                else x                if isinstance(x, Tensor)                else x,                flat_args,            )            return fake_flat_tensor_args        fake_flat_tensor_args = process_inputs(flat_args)",
    "code2": "    with preserve_rng_state(), fake_mode as mode:        def process_inputs(flat_args):            if mode:                fake_flat_tensor_args = pytree.tree_map_only(                    Tensor, mode.from_tensor, flat_args                )            else:                # The detach().requires_grad_() pattern can cause some subtle bugs.                # These will be fixed once FakeTensor is always-on for AOTAutograd.                #                # For models that might resize their inputs, the input tensors                # must have allow_tensor_metadata_change() set to true.                # detach() returns a view tensor, but with that field set to false.                #                # Specifically, this breaks quantized models                # (resnet50_quantized_qat and mobilenet_v2_quantized_qat)                # because they use a \"running-mean\" style op that requires                # resizing the running counter buffers stored on the module.                fake_flat_tensor_args = pytree.tree_map_only(                    Tensor,                    lambda x: x.detach().requires_grad_(x.requires_grad),                    flat_args,                )            return fake_flat_tensor_args        fake_flat_tensor_args = process_inputs(flat_args)",
    "method_name": "def create_aot_dispatcher_function("
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83337,
  "title": "fix a comment since the options in arg parser no longer require Declarations.yaml",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-12T21:10:44Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tools/autograd/gen_autograd.py",
    "code1": "repository, run:python -m tools.autograd.gen_autograd \\       build/aten/src/ATen/Declarations.yaml \\       aten/src/ATen/native/native_functions.yaml \\       aten/src/ATen/native/tags.yaml \\       $OUTPUT_DIR \\",
    "code2": "repository, run:python -m tools.autograd.gen_autograd \\       aten/src/ATen/native/native_functions.yaml \\       aten/src/ATen/native/tags.yaml \\       $OUTPUT_DIR \\",
    "method_name": ""
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83323,
  "title": "fix quantization/core/test_docs for Buck2",
  "tags": [
   "cla signed"
  ],
  "closed_time": "2022-08-13T15:35:36Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "test/quantization/core/test_docs.py",
    "code1": "        def get_correct_path(path_from_pytorch):            r\"\"\"            Current working directory when CI is running test seems to vary, this function            looks for docs and if it finds it looks for the path to the            file and if the file exists returns that path, otherwise keeps looking. Will            only work if cwd contains pytorch or docs or a parent contains docs.            \"\"\"            # get cwd            cur_dir_path = Path(\".\").resolve()            # check if cwd contains pytorch, use that if it does            if (cur_dir_path / \"pytorch\").is_dir():                cur_dir_path = (cur_dir_path / \"pytorch\").resolve()            # need to find the file, so we check current directory            # and all parent directories to see if the path leads to it            check_dir = cur_dir_path            while not check_dir == check_dir.parent:                file_path = (check_dir / path_from_pytorch).resolve()                if file_path.is_file():                    return file_path                check_dir = check_dir.parent.resolve()            # no longer passing when file not found            raise FileNotFoundError(\"could not find {}\".format(path_from_pytorch))        path_to_file = get_correct_path(path_from_pytorch)        if path_to_file:",
    "code2": "        def get_correct_path(path_from_pytorch):            r\"\"\"            Current working directory when CI is running test seems to vary, this function            looks for docs relative to this test file.            \"\"\"            pytorch_root = Path(__file__).parent.parent.parent.parent            return pytorch_root / path_from_pytorch        path_to_file = get_correct_path(path_from_pytorch)        if path_to_file:",
    "method_name": "def _get_code("
   },
   {
    "language": ".py",
    "dir": "test/test_quantization.py",
    "code1": "from quantization.core.test_workflow_module import TestFusedObsFakeQuantModule  # noqa: F401from quantization.core.test_backend_config import TestBackendConfig  # noqa: F401from quantization.core.test_utils import TestUtils  # noqa: F401from quantization.core.test_docs import TestQuantizationDocs  # noqa: F401# Eager Mode Workflow. Tests for the functionality of APIs and different features implemented# using eager mode.",
    "code2": "from quantization.core.test_workflow_module import TestFusedObsFakeQuantModule  # noqa: F401from quantization.core.test_backend_config import TestBackendConfig  # noqa: F401from quantization.core.test_utils import TestUtils  # noqa: F401try:    from quantization.core.test_docs import TestQuantizationDocs  # noqa: F401except ImportError:    pass# Eager Mode Workflow. Tests for the functionality of APIs and different features implemented# using eager mode.",
    "method_name": ""
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83284,
  "title": "[ONNX] Update the default opset to version 14",
  "tags": [
   "cla signed",
   "Merged",
   "module: onnx",
   "onnx-needs-import",
   "open source",
   "release notes: onnx",
   "topic: bc_breaking",
   "topic: improvements"
  ],
  "closed_time": "2022-08-18T19:13:41Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83275,
  "title": "Fix building with Werror",
  "tags": [
   "cla signed",
   "Merged",
   "module: build warnings",
   "open source",
   "topic: not user facing"
  ],
  "closed_time": "2022-08-12T22:36:22Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "c10/cuda/CUDACachingAllocator.cpp",
    "code1": "    BlockPool& pool = *p.pool;    // because of std::unique_ptr, block cannot be trivially copied    Block key(0, 0, 0);    std::memcpy(&key, &p.search_key, sizeof(Block));    key.history.release();    key.size = (key.size < CachingAllocatorConfig::max_split_size())        ? CachingAllocatorConfig::max_split_size()        : key.size;",
    "code2": "    BlockPool& pool = *p.pool;    // because of std::unique_ptr, block cannot be trivially copied    Block key(        p.search_key.device,        p.search_key.stream,        p.search_key.size,        p.search_key.pool,        p.search_key.ptr);    key.size = (key.size < CachingAllocatorConfig::max_split_size())        ? CachingAllocatorConfig::max_split_size()        : key.size;",
    "method_name": "class DeviceCachingAllocator {"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83273,
  "title": "Fix issues with Werror=range-loop-construct",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-11T17:45:21Z",
  "code_diffs": []
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83269,
  "title": "[JIT Test] Add more debugging information for JIT opinfo tests",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-12T00:29:10Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "test/test_ops_jit.py",
    "code1": "# Owner(s): [\"module: unknown\"]from functools import partialimport torch variants = {'method': getattr(torch.Tensor, op.name)} samples = op.sample_inputs(device, dtype, requires_grad=False) support_script = op.supports_scripting tested = False for sample in samples: continue tested = True # Create accessor for script function variant name = op.name + '_' if func_type == 'inplace' else op.name # run with disable_autodiff_subgraph_inlining(True) to test #   autodiff support. Context manager forces the graph to contain #   DifferentiableGraph nodes if they are present with disable_autodiff_subgraph_inlining(): # Check scripted forward, grad, and grad grad if support_script: script_fn = create_script_fn(self, name, func_type) def out_fn(output): # Processes the output for autograd if sample.output_process_fn_grad is not None: return sample.output_process_fn_grad(output) return output def get_sample(): return clone_input_helper(sample.input) if op.name[-1] == '_' else sample.input if support_script: check_against_reference(self, script_fn, func, out_fn,                                                (get_sample(),) + sample.args, sample.kwargs, no_grad=not _requires_grad, no_gradgrad=not op.supports_gradgrad) # Check traced forward, grad, and grad grad # TODO: fix tracing here supports_tracing = op.supports_tracing and not has_fake_function if op.assert_jit_shape_analysis: self.assertTrue(supports_tracing) if supports_tracing: traced_fn = create_traced_fn(self, variant) check_against_reference(self, traced_fn, func, out_fn,                                                (get_sample(),) + sample.args, sample.kwargs, no_grad=not _requires_grad, no_gradgrad=not op.supports_gradgrad) # Check alias annotation schema for correctness (make #   sure inputs that aren't supposed to be modified aren't) # Note: only runs in float32 because schema isn't affected by dtype, #   so running it on all dtypes is would be excessive if dtype == torch.float32: # TODO: no reason why we cant run this with tracing graph if support_script and op.name != \"rsub\": check_alias_annotation(name, (get_sample(),) + sample.args, sample.kwargs, func_type=func_type, aten_name=op.aten_name) # TODO: use script graph as well checked_shape_analysis = False if supports_tracing: out = variant(get_sample(), *sample.args, **sample.kwargs) # right now, tuple of outputs and tensor output supported # TODO: list of tensor outputs tuple_of_tensors = isinstance(out, tuple) and all([isinstance(elem, torch.Tensor) for elem in out]) if isinstance(out, torch.Tensor) or tuple_of_tensors: if tuple_of_tensors: sizes = [elem.size() for elem in out] else: sizes = out.size() self.checkShapeAnalysis(sizes, traced_fn.graph, op.assert_jit_shape_analysis) checked_shape_analysis = True if op.assert_jit_shape_analysis: self.assertTrue(checked_shape_analysis) # Check autodifferentiation of nodes for traced and scripted graphs, only need to check once per sample if dtype is torch.float32: # Sandcastle doesn't fuse nodes if IS_SANDCASTLE: # fusible nodes are expected to be found in FusionGroups in the DifferentiableGraphs nonfusible_nodes = op.autodiff_nonfusible_nodes + op.autodiff_fusible_nodes fusible_nodes = [] else: nonfusible_nodes = op.autodiff_nonfusible_nodes fusible_nodes = op.autodiff_fusible_nodes if supports_tracing: self.assertAutodiffNode(traced_fn.last_graph, op.assert_autodiffed, nonfusible_nodes, fusible_nodes) if support_script: self.assertAutodiffNode(script_fn.last_graph, op.assert_autodiffed, nonfusible_nodes, fusible_nodes) assert tested, \"JIT Test does not execute any logic\" # alias testing is only done with torch.float for the same reason _alias_ops = partial(ops, dtypes=OpDTypes.supported,",
    "code2": "# Owner(s): [\"module: unknown\"]from functools import partialfrom textwrap import dedentimport torch variants = {'method': getattr(torch.Tensor, op.name)} samples = op.sample_inputs(device, dtype, requires_grad=False) tested = False for sample in samples: continue tested = True try: self.indiv_variant_test_jit(device, dtype, op, sample, func_type, variant, has_fake_function) except Exception as e: variant_error_info = dedent(f\"\"\"                        Error testing {op.name} {func_type} variant                        with dtype: {dtype}                        with inputs {sample}:                    \"\"\") raise Exception(variant_error_info) from e assert tested, \"JIT Test does not execute any logic\" def indiv_variant_test_jit(self, device, dtype, op, sample, func_type, variant, has_fake_function): _requires_grad = (dtype in op.supported_backward_dtypes(torch.device(device).type)) support_script = op.supports_scripting # Create accessor for script function variant name = op.name + '_' if func_type == 'inplace' else op.name # run with disable_autodiff_subgraph_inlining(True) to test #   autodiff support. Context manager forces the graph to contain #   DifferentiableGraph nodes if they are present with disable_autodiff_subgraph_inlining(): # Check scripted forward, grad, and grad grad if support_script: script_fn = create_script_fn(self, name, func_type) def out_fn(output): # Processes the output for autograd if sample.output_process_fn_grad is not None: return sample.output_process_fn_grad(output) return output def get_sample(): return clone_input_helper(sample.input) if op.name[-1] == '_' else sample.input if support_script: check_against_reference(self, script_fn, op.get_op(), out_fn,                                        (get_sample(),) + sample.args, sample.kwargs, no_grad=not _requires_grad, no_gradgrad=not op.supports_gradgrad) # Check traced forward, grad, and grad grad # TODO: fix tracing here supports_tracing = op.supports_tracing and not has_fake_function if op.assert_jit_shape_analysis: self.assertTrue(supports_tracing) if supports_tracing: traced_fn = create_traced_fn(self, variant) check_against_reference(self, traced_fn, op.get_op(), out_fn,                                        (get_sample(),) + sample.args, sample.kwargs, no_grad=not _requires_grad, no_gradgrad=not op.supports_gradgrad) # Check alias annotation schema for correctness (make #   sure inputs that aren't supposed to be modified aren't) # Note: only runs in float32 because schema isn't affected by dtype, #   so running it on all dtypes is would be excessive if dtype == torch.float32: # TODO: no reason why we cant run this with tracing graph if support_script and op.name != \"rsub\": check_alias_annotation(name, (get_sample(),) + sample.args, sample.kwargs, func_type=func_type, aten_name=op.aten_name) # TODO: use script graph as well checked_shape_analysis = False if supports_tracing: out = variant(get_sample(), *sample.args, **sample.kwargs) # right now, tuple of outputs and tensor output supported # TODO: list of tensor outputs tuple_of_tensors = isinstance(out, tuple) and all([isinstance(elem, torch.Tensor) for elem in out]) if isinstance(out, torch.Tensor) or tuple_of_tensors: if tuple_of_tensors: sizes = [elem.size() for elem in out] else: sizes = out.size() self.checkShapeAnalysis(sizes, traced_fn.graph, op.assert_jit_shape_analysis) checked_shape_analysis = True if op.assert_jit_shape_analysis: self.assertTrue(checked_shape_analysis) # Check autodifferentiation of nodes for traced and scripted graphs, only need to check once per sample if dtype is torch.float32: # Sandcastle doesn't fuse nodes if IS_SANDCASTLE: # fusible nodes are expected to be found in FusionGroups in the DifferentiableGraphs nonfusible_nodes = op.autodiff_nonfusible_nodes + op.autodiff_fusible_nodes fusible_nodes = [] else: nonfusible_nodes = op.autodiff_nonfusible_nodes fusible_nodes = op.autodiff_fusible_nodes if supports_tracing: self.assertAutodiffNode(traced_fn.last_graph, op.assert_autodiffed, nonfusible_nodes, fusible_nodes) if support_script: self.assertAutodiffNode(script_fn.last_graph, op.assert_autodiffed, nonfusible_nodes, fusible_nodes) # alias testing is only done with torch.float for the same reason _alias_ops = partial(ops, dtypes=OpDTypes.supported,",
    "method_name": ""
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83217,
  "title": "Fix lint errors",
  "tags": [
   "cla signed",
   "fx"
  ],
  "closed_time": "2022-08-10T23:06:04Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "torch/ao/quantization/backend_config/_common_operator_config_utils.py",
    "code1": "    linear_configs.append(        BackendPatternConfig(torch.nn.functional.linear)            .set_observation_type(observation_type)  # noqa: E131            .set_dtype_configs(dtype_configs))            ._set_input_type_to_index({\"weight\": 1, \"bias\": 2})    # (2) Linear + relu    # -------------------        conv_configs.append(            BackendPatternConfig(convs.func)                .set_observation_type(observation_type)  # noqa: E131                .set_dtype_configs(dtype_configs))                ._set_input_type_to_index()                ._set_input_type_to_index({\"weight\": 1, \"bias\": 2})        # (2) Conv + relu        # -----------------",
    "code2": "    linear_configs.append(        BackendPatternConfig(torch.nn.functional.linear)            .set_observation_type(observation_type)  # noqa: E131            .set_dtype_configs(dtype_configs)            ._set_input_type_to_index({\"weight\": 1, \"bias\": 2}))    # (2) Linear + relu    # -------------------        conv_configs.append(            BackendPatternConfig(convs.func)                .set_observation_type(observation_type)  # noqa: E131                .set_dtype_configs(dtype_configs)                ._set_input_type_to_index({\"weight\": 1, \"bias\": 2}))        # (2) Conv + relu        # -----------------",
    "method_name": "def _get_linear_configs(dtype_configs: List[DTypeConfig]) -> List[BackendPattern"
   },
   {
    "language": ".py",
    "dir": "torch/ao/quantization/fx/prepare.py",
    "code1": "    pattern_to_input_type_to_index = get_pattern_to_input_type_to_index(backend_config)    for pattern, input_type_to_index in pattern_to_input_type_to_index.items():        for input_type, index in input_type_to_index.items():            index_dicts = {                \"weight\": {},                \"bias\": {},                \"input\": {}  # not used right now    if isinstance(node, Node) and node.op == 'call_function' and \\            node.target in weight_index_dict:        for i, node_arg in enumerate(node.args):            if arg is node_arg and i in \\                    weight_index_dict[node.target]:  # type: ignore[index]                return True        for kwarg_name, kwarg_value in node.kwargs.items():        return False    for i, node_arg in enumerate(node.args):        if arg is node_arg and i in \\           bias_index_dict[node.target]:  # type: ignore[index]            return True    is supported by the backend or not    \"\"\"    if isinstance(arg, (list, tuple)):        return all(map(lambda a: is_input_arg_dtype_supported_by_backend(a, node, node_name_to_target_dtype, dtype_config, weight_index_dict, bias_index_dict), arg))    if not isinstance(arg, Node):        return True    # TODO: support check for standalone module        for arg in input_node.args:            supported = supported and \\                is_input_arg_dtype_supported_by_backend(                    arg, input_node, node_name_to_target_dtype, dtype_config, weight_index_dict, bias_index_dict)        for k, arg in input_node.kwargs.items():            supported = supported and \\                is_input_arg_dtype_supported_by_backend(                    arg, input_node, node_name_to_target_dtype, dtype_config)        # check if output dtype is supported        supported = supported and is_output_dtype_supported_by_backend(    if isinstance(arg, (list, tuple)):        new_arg_to_return = []        for inner_arg in arg:            new_inner_arg = maybe_insert_input_observer_for_arg_or_kwarg(                node, inner_arg, qconfig, model, modules,                graph, node_name_to_target_dtype,                qhandler,            qconfig.activation        arg_as_output_target_dtype = get_arg_target_dtype_as_output(arg, modules, node_name_to_target_dtype)        arg_as_input_target_dtype = get_arg_target_dtype_as_input_to_node(arg, node, modules, node_name_to_target_dtype, weight_index_dict, bias_index_dict)        arg_as_input_target_compute_dtype = \\            get_arg_target_compute_dtype_as_input_to_node(                arg, node, modules, node_name_to_target_dtype, weight_index_dict, bias_index_dict)            node_name_to_target_dtype,            qhandler,            prepare_custom_config,            backend_config            weight_index_dict,            bias_index_dict,)        new_kwargs[k] = new_kwarg    custom_module_classes = get_custom_module_class_keys(prepare_custom_config.float_to_observed_mapping)    matches_without_qconfig = find_matches(        model.graph, modules, patterns, root_node_getter_mapping,        standalone_module_names, standalone_module_classes, custom_module_classes)    # map qconfig instances to matches",
    "code2": "    pattern_to_input_type_to_index = get_pattern_to_input_type_to_index(backend_config)    for pattern, input_type_to_index in pattern_to_input_type_to_index.items():        for input_type, index in input_type_to_index.items():            index_dicts = {                \"weight\": {},                \"bias\": {},                \"input\": {}  # not used right now    if isinstance(node, Node) and node.op == 'call_function' and \\            node.target in weight_index_dict:        for i, node_arg in enumerate(node.args):            if arg is node_arg and i in \\                    weight_index_dict[node.target]:  # type: ignore[index]                return True        for kwarg_name, kwarg_value in node.kwargs.items():        return False    for i, node_arg in enumerate(node.args):        if arg is node_arg and i in \\           bias_index_dict[node.target]:  # type: ignore[index]            return True    is supported by the backend or not    \"\"\"    if isinstance(arg, (list, tuple)):        return all(map(lambda a: is_input_arg_dtype_supported_by_backend(a, node, node_name_to_target_dtype, dtype_config, weight_index_dict, bias_index_dict), arg))    if not isinstance(arg, Node):        return True    # TODO: support check for standalone module        for arg in input_node.args:            supported = supported and \\                is_input_arg_dtype_supported_by_backend(                    arg, input_node, node_name_to_target_dtype, dtype_config, weight_index_dict, bias_index_dict)        for k, arg in input_node.kwargs.items():            supported = supported and \\                is_input_arg_dtype_supported_by_backend(                    arg, input_node, node_name_to_target_dtype, dtype_config)        # check if output dtype is supported        supported = supported and is_output_dtype_supported_by_backend(    if isinstance(arg, (list, tuple)):        new_arg_to_return = []        for inner_arg in arg:            new_inner_arg = maybe_insert_input_observer_for_arg_or_kwarg(                node, inner_arg, qconfig, model, modules,                graph, node_name_to_target_dtype,                qhandler,            qconfig.activation        arg_as_output_target_dtype = get_arg_target_dtype_as_output(arg, modules, node_name_to_target_dtype)        arg_as_input_target_dtype = get_arg_target_dtype_as_input_to_node(arg, node, modules, node_name_to_target_dtype, weight_index_dict, bias_index_dict)        arg_as_input_target_compute_dtype = \\            get_arg_target_compute_dtype_as_input_to_node(                arg, node, modules, node_name_to_target_dtype, weight_index_dict, bias_index_dict)            node_name_to_target_dtype,            qhandler,            prepare_custom_config,            backend_config,            weight_index_dict,            bias_index_dict,)        new_kwargs[k] = new_kwarg    custom_module_classes = get_custom_module_class_keys(prepare_custom_config.float_to_observed_mapping)    matches_without_qconfig = find_matches(        model.graph, modules, patterns, root_node_getter_mapping,        standalone_module_names, standalone_module_classes, custom_module_classes)    # map qconfig instances to matches",
    "method_name": ""
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83207,
  "title": "fix functionalization <> resnet18, make ProxyTensor work with tensor-less decomps",
  "tags": [
   "cla signed",
   "fx",
   "Merged"
  ],
  "closed_time": "2022-08-12T01:07:35Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "test/test_proxy_tensor.py",
    "code1": "            self.assertFalse(is_any_sigmoid(traced))  # this fails, sigmoid is traced with LoggingTensor            self.assertTrue(is_any_digamma(traced))    def test_make_fx_reentrant_dispatch(self):        def f(x):            return torch.ops.aten.norm.Scalar(x, 2.0)",
    "code2": "            self.assertFalse(is_any_sigmoid(traced))  # this fails, sigmoid is traced with LoggingTensor            self.assertTrue(is_any_digamma(traced))    def test_proxy_tensor_mode_with_decomp_table_preserves_proxy(self):        def f(x):            y = x.new_zeros(x.size())            y.copy_(x)            return y        def _new_zeros_decomp(inp, size, dtype=None, layout=None, device=None, pin_memory=None):            return torch.zeros(size, dtype=inp.dtype, device=inp.device)        factory_func_decomp = {torch.ops.aten.new_zeros.default: _new_zeros_decomp}        # When new_zeros() decomposes into torch.zero(), we expect ProxyTensorMode        # to still be (re-entrantly) enabled, so that the `torch.zero()` call        # returns a ProxyTensor.        out = make_fx(f, decomposition_table=factory_func_decomp)(torch.ones(2))        self.assertExpectedInline(out.code, \"\"\"\\def forward(self, x_1):    zeros = torch.ops.aten.zeros.default([2], dtype = torch.float32, device = device(type='cpu'), pin_memory = False)    copy__default = torch.ops.aten.copy_.default(zeros, x_1);  zeros = x_1 = None    return copy__default    \"\"\")    def test_make_fx_reentrant_dispatch(self):        def f(x):            return torch.ops.aten.norm.Scalar(x, 2.0)",
    "method_name": "def f2_logging_tensor(x):"
   },
   {
    "language": ".py",
    "dir": "torch/fx/experimental/proxy_tensor.py",
    "code1": " func = func_overload.overloadpacket if func_overload in CURRENT_DECOMPOSITION_TABLE: return CURRENT_DECOMPOSITION_TABLE[func_overload](*args, **kwargs) with proxy_mode.restore(): r = func_overload.decompose(*args, **kwargs) if r is not NotImplemented:",
    "code2": " func = func_overload.overloadpacket if func_overload in CURRENT_DECOMPOSITION_TABLE: with proxy_mode.restore(): return CURRENT_DECOMPOSITION_TABLE[func_overload](*args, **kwargs) with proxy_mode.restore(): r = func_overload.decompose(*args, **kwargs) if r is not NotImplemented:",
    "method_name": "def proxy_call(proxy_mode, func_overload, args, kwargs=None):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83206,
  "title": "fix resnet18 <> functionalization, remove problematic decomp",
  "tags": [
   "cla signed"
  ],
  "closed_time": "2022-08-10T22:13:06Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "functorch/functorch/_src/aot_autograd.py",
    "code1": "    return aten.view(x, shape)@register_decomposition(aten.new_zeros, aot_autograd_decompositions)def new_zeros(inp, size, dtype=None, layout=None, device=None, pin_memory=None):    return torch.zeros(size, dtype=inp.dtype, device=inp.device)@register_decomposition(aten.new_full, aot_autograd_decompositions)def new_full(inp, size, value, dtype=None, layout=None, device=None, pin_memory=None):    return torch.full(size, value, dtype=inp.dtype, device=inp.device)",
    "code2": "    return aten.view(x, shape)@register_decomposition(aten.new_full, aot_autograd_decompositions)def new_full(inp, size, value, dtype=None, layout=None, device=None, pin_memory=None):    return torch.full(size, value, dtype=inp.dtype, device=inp.device)",
    "method_name": "def _reshape_alias(x, shape, strides):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83198,
  "title": "[primTorch] Fix off by 1 in canonicalize_dim",
  "tags": [
   "bug",
   "cla signed",
   "Merged",
   "module: primTorch",
   "open source",
   "topic: bug fixes"
  ],
  "closed_time": "2022-08-16T17:57:14Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "c10/core/WrapDimMinimal.cpp",
    "code1": " int64_t dim, int64_t dim_post_expr, bool wrap_scalar) { if (dim_post_expr <= 0) { TORCH_CHECK_INDEX(        wrap_scalar, \"dimension specified as \",        dim, \" but tensor has no dimensions\"); return c10::maybe_wrap_dim(dim, /*dim_post_expr=*/1, /*wrap_scalar=*/false);",
    "code2": " int64_t dim, int64_t dim_post_expr, bool wrap_scalar) { TORCH_CHECK_INDEX(      dim_post_expr >= 0, \"Rank cannot be negative but got \", dim_post_expr); if (dim_post_expr == 0) { TORCH_CHECK_INDEX(        wrap_scalar, \"Dimension specified as \",        dim, \" but tensor has no dimensions\"); return c10::maybe_wrap_dim(dim, /*dim_post_expr=*/1, /*wrap_scalar=*/false);",
    "method_name": "int64_t maybe_wrap_dim_slow("
   },
   {
    "language": ".py",
    "dir": "test/test_torch.py",
    "code1": "            torch.tensor([1]).unflatten(0, [])        with self.assertRaisesRegex(RuntimeError, r\"Provided sizes \\[2, 2\\] don't multiply up to the size of dim 0 \\(1\\)\"):            torch.tensor([1]).unflatten(0, [2, 2])        with self.assertRaisesRegex(IndexError, r\"dimension specified as 0 but tensor has no dimensions\"):            torch.tensor(1).unflatten(0, [0])        with self.assertRaisesRegex(RuntimeError, r\"only one dimension can be inferred\"):            torch.randn(5, 10).unflatten(1, (-1, -1))",
    "code2": "            torch.tensor([1]).unflatten(0, [])        with self.assertRaisesRegex(RuntimeError, r\"Provided sizes \\[2, 2\\] don't multiply up to the size of dim 0 \\(1\\)\"):            torch.tensor([1]).unflatten(0, [2, 2])        with self.assertRaisesRegex(IndexError, r\"Dimension specified as 0 but tensor has no dimensions\"):            torch.tensor(1).unflatten(0, [0])        with self.assertRaisesRegex(RuntimeError, r\"only one dimension can be inferred\"):            torch.randn(5, 10).unflatten(1, (-1, -1))",
    "method_name": "def test_unflatten(self):"
   },
   {
    "language": ".py",
    "dir": "torch/_prims/__init__.py",
    "code1": ")def expand_dims(a: TensorLikeType, dimensions: DimsSequenceType) -> TensorLikeType:    \"\"\"    Creates a view of a with a.ndim + len(dimensions) dimensions, with new    dimensions of length one at the dimensions specified by dimensions.    \"\"\"    dims = sorted(utils.canonicalize_dims(a.ndim, dimensions))  # type: ignore[arg-type]    if len(set(dims)) != len(dims):        msg = \"Received duplicate dimensions to expand in {0}\".format(str(dimensions))        raise ValueError(msg)",
    "code2": ")def expand_dims(    a: TensorLikeType, dimensions: DimsSequenceType, ndim=None) -> TensorLikeType:    \"\"\"    Creates a view of a with a.ndim + len(dimensions) dimensions, with new    dimensions of length one at the dimensions specified by dimensions.    \"\"\"    if ndim is not None:        # TODO: this is only here to support the unsqueeze ref        dims = sorted(utils.canonicalize_dims(ndim, dimensions))  # type: ignore[arg-type]    else:        dims = sorted(utils.canonicalize_dims(a.ndim, dimensions))  # type: ignore[arg-type]    if len(set(dims)) != len(dims):        msg = \"Received duplicate dimensions to expand in {0}\".format(str(dimensions))        raise ValueError(msg)",
    "method_name": "def _conj_meta(a: TensorLikeType) -> TensorLikeType:"
   },
   {
    "language": ".py",
    "dir": "torch/_prims_common/__init__.py",
    "code1": "# \"Wraps\" a dim (up to one time) for the given rank, allowing# dims to be specified using negative indicesdef canonicalize_dim(rank: int, idx: int) -> int: # TODO: add a comment for why this is _rank = rank if rank != 0 else 1 if idx >= 0 and idx < _rank: return idx if idx < 0: _idx = idx + _rank else: _idx = idx if _idx < 0 or _idx > _rank: # Same error message as in aten/src/ATen/WrapDimUtils.h:49 msg = \"Dimension out of range (expected to be in range of [{0}, {1}], but got {2})\".format( -rank, rank - 1, idx",
    "code2": "# \"Wraps\" a dim (up to one time) for the given rank, allowing# dims to be specified using negative indicesdef canonicalize_dim(rank: int, idx: int, wrap_scalar: bool = True) -> int: if rank < 0: msg = f\"Rank cannot be negative but got {rank}\" raise IndexError(msg) if rank == 0: if not wrap_scalar: msg = f\"Dimension specified as {idx} but tensor has no dimensions\" raise IndexError(msg) rank = 1 if idx >= 0 and idx < rank: return idx if idx < 0: _idx = idx + rank else: _idx = idx if _idx < 0 or _idx >= rank: # Same error message as in aten/src/ATen/WrapDimUtils.h:49 msg = \"Dimension out of range (expected to be in range of [{0}, {1}], but got {2})\".format( -rank, rank - 1, idx",
    "method_name": "def validate_exclusive_idx(rank: int, ex_idx: int):"
   },
   {
    "language": ".py",
    "dir": "torch/_refs/__init__.py",
    "code1": "@out_wrapper()def column_stack(tensors: TensorSequenceType) -> TensorLikeType:    aligned_tensors = tuple(        x if x.ndim > 1 else prims.expand_dims(x, list(range(x.ndim, 2)))        for x in tensors    )    return cat(aligned_tensors, 1)    a: TensorLikeType, k: int = 1, dims: DimsSequenceType = (0, 1)) -> TensorLikeType:    \"\"\"Reference implementation of :func:`torch.rot90`.\"\"\"    dims_ = utils.canonicalize_dims(a.ndim, dims)    # Required to silence MyPy errors    assert isinstance(dims_, (tuple, list))    dims = dims_    if len(dims) != 2:        raise RuntimeError(            f\"expected total rotation dims == 2, but got dims = {len(dims)}\"        )    if a.ndim < 2:        raise RuntimeError(f\"expected total dims >= 2, but got total dims = {a.ndim}\")    if dims[0] == dims[1]:        raise RuntimeError(            f\"expected rotation dims to be different, but got dim0 = {dims[0]} and dim1 = {dims[1]}\"def unsqueeze(a: TensorLikeType, dim: int) -> TensorLikeType:    # Note that unsqueeze canonicalizes with rank + 1 because it allows    # a new innermost dimension to be specified    dim = utils.canonicalize_dim(a.ndim + 1, dim)    return prims.expand_dims(a, (dim,))# NOTE: shape is a vararg because Tensor.reshape can be called with as",
    "code2": "@out_wrapper()def column_stack(tensors: TensorSequenceType) -> TensorLikeType:    aligned_tensors = tuple(        x if x.ndim > 1 else x.reshape((x.numel(), 1)) for x in tensors    )    return cat(aligned_tensors, 1)    a: TensorLikeType, k: int = 1, dims: DimsSequenceType = (0, 1)) -> TensorLikeType:    \"\"\"Reference implementation of :func:`torch.rot90`.\"\"\"    if len(dims) != 2:        raise RuntimeError(            f\"expected total rotation dims == 2, but got dims = {len(dims)}\"        )    if a.ndim < 2:        raise RuntimeError(f\"expected total dims >= 2, but got total dims = {a.ndim}\")    # Do this after the initial checks to be compatible with the behavior in    # core.    dims = utils.canonicalize_dims(a.ndim, dims)    if dims[0] == dims[1]:        raise RuntimeError(            f\"expected rotation dims to be different, but got dim0 = {dims[0]} and dim1 = {dims[1]}\"def unsqueeze(a: TensorLikeType, dim: int) -> TensorLikeType:    # Note that unsqueeze canonicalizes with rank + 1 because it allows    # a new innermost dimension to be specified    ndim = a.ndim + 1    dim = utils.canonicalize_dim(ndim, dim)    return prims.expand_dims(a, (dim,), ndim=ndim)# NOTE: shape is a vararg because Tensor.reshape can be called with as",
    "method_name": "def cat(tensors: TensorSequenceType, dim: int = 0) -> TensorLikeType:"
   },
   {
    "language": ".py",
    "dir": "torch/testing/_internal/common_methods_invocations.py",
    "code1": "def error_inputs_unbind(op_info, device):    make_arg = partial(make_tensor, dtype=torch.int32, device=device, requires_grad=False)    yield ErrorInput(SampleInput(make_arg(()), args=(0,)), error_type=IndexError,                     error_regex=\"dimension specified as 0 but tensor has no dimensions\")    yield ErrorInput(SampleInput(make_arg((2,)), args=(2,)), error_type=IndexError,                     error_regex=\"Dimension out of range\")",
    "code2": "def error_inputs_unbind(op_info, device):    make_arg = partial(make_tensor, dtype=torch.int32, device=device, requires_grad=False)    yield ErrorInput(SampleInput(make_arg(()), args=(0,)), error_type=IndexError,                     error_regex=\"Dimension specified as 0 but tensor has no dimensions\")    yield ErrorInput(SampleInput(make_arg((2,)), args=(2,)), error_type=IndexError,                     error_regex=\"Dimension out of range\")",
    "method_name": "def sample_inputs_unbind(op_info, device, dtype, requires_grad, **kwargs):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83190,
  "title": "properly raise an error in torch.ops.aten.foo.decompose when foo doesnt have a composite",
  "tags": [
   "cla signed"
  ],
  "closed_time": "2022-08-10T18:39:39Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "torch/_ops.py",
    "code1": "        if torch._C._dispatch_has_kernel_for_dispatch_key(self.name, dk):            return self._op_dk(dk, *args, **kwargs)        else: return NotImplemented    @property    def overloadpacket(self):",
    "code2": "        if torch._C._dispatch_has_kernel_for_dispatch_key(self.name, dk):            return self._op_dk(dk, *args, **kwargs)        else: raise NotImplementedError    @property    def overloadpacket(self):",
    "method_name": "def decompose(self, *args, **kwargs):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83174,
  "title": "Fix hash for Tensor subclasses",
  "tags": [
   "cla signed",
   "Merged",
   "release notes: python_frontend",
   "topic: bug fixes"
  ],
  "closed_time": "2022-08-10T19:23:59Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "test/test_overrides.py",
    "code1": "    TorchFunctionMode)from torch.utils._mode_utils import find_outermost_mode, all_same_mode, all_same_mode_scopeTensor = torch.Tensor            finally:                del gif __name__ == '__main__':    run_tests()",
    "code2": "    TorchFunctionMode)from torch.utils._mode_utils import find_outermost_mode, all_same_mode, all_same_mode_scopefrom torch.utils._pytree import tree_mapTensor = torch.Tensor            finally:                del g    def test_subclass_hash(self):        class DiagTensor(torch.Tensor):            def __init__(self, diag):                self._diag = diag            @classmethod            def __torch_function__(cls, func, types, args=(), kwargs=None):                kwargs = kwargs or {}                def get_full_matrices(t):                    if isinstance(t, DiagTensor):                        return torch.diag_embed(t._diag)                    else:                        return t                return func(*tree_map(get_full_matrices, args), **tree_map(get_full_matrices, kwargs))        d = torch.rand(2)        a = DiagTensor(d)        self.assertEqual((a + 1), torch.diag_embed(d) + 1)        # If the hash function was returning the same value, this would        # fail inside `Tensor.__eq__`.        # If __hash__ was going through torch_function, the implementation above would        # be wrong as it would compute the hash on a temporary Tensor thus not ensuring        # the uniqueness of the hash that we rely on for Tensors.        s = set()        s.add(a)        s.add(DiagTensor(d))if __name__ == '__main__':    run_tests()",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "test/test_profiler_tree.py",
    "code1": "                      <built-in method append of list object at 0xXXXXXXXXXXXX>                      <built-in method append of list object at 0xXXXXXXXXXXXX>                      torch/_tensor.py(...): __hash__                        <built-in function _has_torch_function_unary>                        <built-in function id>                      <built-in method append of list object at 0xXXXXXXXXXXXX>                      <built-in method append of list object at 0xXXXXXXXXXXXX>                      <built-in method append of list object at 0xXXXXXXXXXXXX>                      torch/_tensor.py(...): __hash__                        <built-in function _has_torch_function_unary>                        <built-in function id>                      <built-in method append of list object at 0xXXXXXXXXXXXX>                      torch/optim/sgd.py(...): sgd                              cudaLaunchKernel                                void at::native::vectorized_elementwise_kernel<...>(...)                      torch/_tensor.py(...): __hash__                        <built-in function _has_torch_function_unary>                        <built-in function id>                      torch/_tensor.py(...): __hash__                        <built-in function _has_torch_function_unary>                        <built-in function id>                    torch/autograd/grad_mode.py(...): __init__                      <built-in function is_grad_enabled>",
    "code2": "                      <built-in method append of list object at 0xXXXXXXXXXXXX>                      <built-in method append of list object at 0xXXXXXXXXXXXX>                      torch/_tensor.py(...): __hash__                        <built-in function id>                      <built-in method append of list object at 0xXXXXXXXXXXXX>                      <built-in method append of list object at 0xXXXXXXXXXXXX>                      <built-in method append of list object at 0xXXXXXXXXXXXX>                      torch/_tensor.py(...): __hash__                        <built-in function id>                      <built-in method append of list object at 0xXXXXXXXXXXXX>                      torch/optim/sgd.py(...): sgd                              cudaLaunchKernel                                void at::native::vectorized_elementwise_kernel<...>(...)                      torch/_tensor.py(...): __hash__                        <built-in function id>                      torch/_tensor.py(...): __hash__                        <built-in function id>                    torch/autograd/grad_mode.py(...): __init__                      <built-in function is_grad_enabled>",
    "method_name": "def step():"
   },
   {
    "language": ".py",
    "dir": "torch/_tensor.py",
    "code1": "        return iter(self.unbind(0))    def __hash__(self):        if has_torch_function_unary(self):            return handle_torch_function(Tensor.__hash__, (self,), self)        return id(self)    def __dir__(self):",
    "code2": "        return iter(self.unbind(0))    def __hash__(self):        # Do NOT handle __torch_function__ here as user's default        # implementation that handle most functions will most likely do it wrong.        # It can be easily overridden by defining this method on the user        # subclass if needed.        return id(self)    def __dir__(self):",
    "method_name": "def __iter__(self):"
   },
   {
    "language": ".py",
    "dir": "torch/overrides.py",
    "code1": "        Tensor.__new__,        Tensor.__class__,        Tensor.__subclasshook__,        Tensor.as_subclass,        Tensor.reinforce,        Tensor.new,        Tensor.__deepcopy__: lambda self, memo: -1,        Tensor.__int__: lambda self: -1,        Tensor.__long__: lambda self: -1,        Tensor.__hash__: lambda self: -1,        Tensor.__index__: lambda self: -1,        Tensor.__len__: lambda self: -1,        Tensor.__format__: lambda self, format_spec: -1,",
    "code2": "        Tensor.__new__,        Tensor.__class__,        Tensor.__subclasshook__,        Tensor.__hash__,        Tensor.as_subclass,        Tensor.reinforce,        Tensor.new,        Tensor.__deepcopy__: lambda self, memo: -1,        Tensor.__int__: lambda self: -1,        Tensor.__long__: lambda self: -1,        Tensor.__index__: lambda self: -1,        Tensor.__len__: lambda self: -1,        Tensor.__format__: lambda self, format_spec: -1,",
    "method_name": "def get_ignored_functions() -> Set[Callable]:"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83141,
  "title": "[torchgen] Fix selective build error on custom namespace",
  "tags": [
   "cla signed",
   "fb-exported",
   "Merged"
  ],
  "closed_time": "2022-08-10T21:27:09Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "tools/test/test_selective_build.py",
    "code1": "import unittestfrom torchgen.selective_build.operator import *from torchgen.selective_build.selector import (    combine_selective_builders,    SelectiveBuilder,        self.assertTrue(selector.is_kernel_dtype_selected(\"add_kernel\", \"int16\"))        self.assertTrue(selector.is_kernel_dtype_selected(\"add1_kernel\", \"int32\"))        self.assertTrue(selector.is_kernel_dtype_selected(\"add_kernel\", \"float\"))",
    "code2": "import unittestfrom torchgen.selective_build.operator import *from torchgen.model import Location, NativeFunctionfrom torchgen.selective_build.selector import (    combine_selective_builders,    SelectiveBuilder,        self.assertTrue(selector.is_kernel_dtype_selected(\"add_kernel\", \"int16\"))        self.assertTrue(selector.is_kernel_dtype_selected(\"add1_kernel\", \"int32\"))        self.assertTrue(selector.is_kernel_dtype_selected(\"add_kernel\", \"float\"))    def test_custom_namespace_selected_correctly(self):        yaml_config = \"\"\"operators:  aten::add.int:    is_used_for_training: No    is_root_operator: Yes    include_all_overloads: No  custom::add:    is_used_for_training: Yes    is_root_operator: No    include_all_overloads: Yes\"\"\"        selector = SelectiveBuilder.from_yaml_str(yaml_config)        native_function, _ = NativeFunction.from_yaml(            {\"func\": \"custom::add() -> Tensor\"},            loc=Location(__file__, 1),            valid_tags=set(),        )        self.assertTrue(selector.is_native_function_selected(native_function))",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "torchgen/selective_build/selector.py",
    "code1": "def op_name_from_native_function(f: NativeFunction) -> str:    # This was originally read from the 'operator_name_with_overload' field in the    # declaration dict, which was the part before the first '(' in 'schema_string'.    return f\"aten::{f.func.name}\"",
    "code2": "def op_name_from_native_function(f: NativeFunction) -> str:    # This was originally read from the 'operator_name_with_overload' field in the    # declaration dict, which was the part before the first '(' in 'schema_string'.    return f\"{f.namespace}::{f.func.name}\"",
    "method_name": "def combine_selective_builders("
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83124,
  "title": "[MPS] Fix for matmul errors in test consistency",
  "tags": [
   "ciflow/mps",
   "ciflow/trunk",
   "cla signed",
   "Merged",
   "open source",
   "triaged"
  ],
  "closed_time": "2022-08-12T23:28:42Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "test/test_mps.py",
    "code1": "        'nn.functional.hinge_embedding_loss': ['torch.float32'],        'nn.functional.kl_div': ['torch.float32'],        'nn.functional.l1_loss': ['torch.float32'],        'nn.functional.huber_loss': ['torch.float32'],        'nn.functional.leaky_relu': ['torch.float32'],        'nn.functional.mse_loss': ['torch.float16', 'torch.float32'],",
    "code2": "        'nn.functional.hinge_embedding_loss': ['torch.float32'],        'nn.functional.kl_div': ['torch.float32'],        'nn.functional.l1_loss': ['torch.float32'],        'nn.functional.linear': ['torch.float32'],        'nn.functional.huber_loss': ['torch.float32'],        'nn.functional.leaky_relu': ['torch.float32'],        'nn.functional.mse_loss': ['torch.float16', 'torch.float32'],",
    "method_name": "class TestConsistency(TestCase):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83122,
  "title": "Handle redispatch correctly with tensor subclasses in ProxyTensor mode",
  "tags": [
   "cla signed",
   "fx",
   "Merged"
  ],
  "closed_time": "2022-08-11T01:31:22Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "functorch/op_analysis/gen_data.py",
    "code1": "    composite_ops = get_ops_for_key('CompositeImplicitAutograd')    noncomposite_ops = all_ops - composite_ops    ops = yaml.load(open('../../pytorch/aten/src/ATen/native/native_functions.yaml', 'r').read(), Loader=yaml.CLoader)    annotated_ops = {a.strip(): b.strip() for a, b in list(csv.reader(open('annotated_ops')))}    from collections import defaultdict",
    "code2": "    composite_ops = get_ops_for_key('CompositeImplicitAutograd')    noncomposite_ops = all_ops - composite_ops    ops = yaml.load(open('../../aten/src/ATen/native/native_functions.yaml', 'r').read(), Loader=yaml.CLoader)    annotated_ops = {a.strip(): b.strip() for a, b in list(csv.reader(open('annotated_ops')))}    from collections import defaultdict",
    "method_name": "def gen_data(special_op_lists, analysis_name):"
   },
   {
    "language": ".py",
    "dir": "test/test_proxy_tensor.py",
    "code1": "from torch._decomp import decomposition_tablefrom torch.testing._internal.common_device_type import opsfrom torch.fx.experimental.proxy_tensor import make_fx, DecompositionInterpreter, get_isolated_graphmodulefrom torch.utils._pytree import tree_mapfrom torch import nn    else:        return torch.rand_like(x)class TestGenericProxyTensor(TestCase):    # WARNING: if any of your inputs are index tensors, DO NOT use this    # function            torch.allclose(fx_f(input, params, buffers)[1], f(input, params, buffers)[1], atol=1e-03)        )class TestGenericProxyTensorReal(TestGenericProxyTensor):    tracing_mode = \"real\"    \"test_make_fx_model_fwd_bwd\",    \"test_proxy_tensor\",    \"test_resnet18_backward_trace\",])class TestGenericProxyTensorSymbolic(TestGenericProxyTensor):    tracing_mode = \"symbolic\"",
    "code2": "from torch._decomp import decomposition_tablefrom torch.testing._internal.common_device_type import opsfrom torch._C import _disabled_torch_function_implfrom torch.fx.experimental.proxy_tensor import make_fx, DecompositionInterpreter, get_isolated_graphmodulefrom torch.utils._pytree import tree_mapfrom torch import nn    else:        return torch.rand_like(x)\"\"\"Delays a cos being executed on the unwraptensor until its used. Simulates a CommTensor used\"\"\"class UnwrapTensor(torch.Tensor):    @staticmethod    def __new__(cls, tensor: torch.Tensor):        r = torch.Tensor._make_wrapper_subclass(            cls,            tensor.size(),            dtype=tensor.dtype,            device=tensor.device,            layout=tensor.layout,            requires_grad=tensor.requires_grad,        )        r._tensor = tensor        return r    def __repr__(self):        # TODO: consider all_gather the local tensors for better debugging        return f\"UnwrapTensor({self._tensor})\"    __torch_function__ = _disabled_torch_function_impl    @classmethod    def __torch_dispatch__(cls, func, types, args=(), kwargs=None):        def unwrap(e):            ret = e            if isinstance(e, UnwrapTensor):                ret = e._tensor.cos()            return ret        args = tree_map(unwrap, args)        kwargs = tree_map(unwrap, kwargs)        return func(*args, **kwargs)class TestGenericProxyTensor(TestCase):    # WARNING: if any of your inputs are index tensors, DO NOT use this    # function            torch.allclose(fx_f(input, params, buffers)[1], f(input, params, buffers)[1], atol=1e-03)        )    def test_trace_subclasses(self):        def f(x):            x = UnwrapTensor(x)            y = x * 2            return y        inp = [torch.randn(5)]        self._test(f, [torch.randn(5)])class TestGenericProxyTensorReal(TestGenericProxyTensor):    tracing_mode = \"real\"    \"test_make_fx_model_fwd_bwd\",    \"test_proxy_tensor\",    \"test_resnet18_backward_trace\",    \"test_trace_subclasses\",])class TestGenericProxyTensorSymbolic(TestGenericProxyTensor):    tracing_mode = \"symbolic\"",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "torch/fx/experimental/proxy_tensor.py",
    "code1": "        #        # This is what the overload modification does.        elif self.trace_factory_functions:            if func_overload is torch.ops.aten.lift_fresh.default:                func_overload = torch.ops.aten.lift_fresh_copy.default",
    "code2": "        #        # This is what the overload modification does.        elif self.trace_factory_functions:            flat_args = pytree.tree_flatten((args, kwargs))[0]            handled_types = [torch.Tensor, ProxyTensor, torch.nn.Parameter]            # If there are any tensor subclasses, we need to handle those tensor subclasses first            if any([isinstance(arg, torch.Tensor) and type(arg) not in handled_types for arg in flat_args]):                return NotImplemented            if func_overload is torch.ops.aten.lift_fresh.default:                func_overload = torch.ops.aten.lift_fresh_copy.default",
    "method_name": "def __torch_dispatch__(self, func_overload, types, args=(), kwargs=None):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83121,
  "title": "Add the Conv1D support for NHWC format.",
  "tags": [
   "ciflow/trunk",
   "cla signed",
   "Merged",
   "open source",
   "release notes: mps",
   "topic: bug fixes",
   "triaged"
  ],
  "closed_time": "2022-08-10T14:30:25Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "test/test_mps.py",
    "code1": "                        helper((N, C_out, H, W), (C_out, C_in, kH, kW), bias_shape=(C_in), stride=stride,                               padding=padding, output_padding=output_padding, dilation=dilation)    # Test sigmoid    def test_sigmoid(self):        def helper(shape):",
    "code2": "                        helper((N, C_out, H, W), (C_out, C_in, kH, kW), bias_shape=(C_in), stride=stride,                               padding=padding, output_padding=output_padding, dilation=dilation)    def test_conv1d_channels_last(self):        model_cpu = torch.nn.Conv1d(1, 128, 3)        a_cpu = torch.arange((128 * 176), dtype=torch.float32)        a_cpu = a_cpu.view(128, 176, 1).permute(0, 2, 1)        out_cpu = model_cpu(a_cpu)  # pass        a_mps = a_cpu.detach().clone().to(\"mps\")        model_mps = model_cpu.to(\"mps\")        out_mps = model_mps(a_mps)        self.assertEqual(out_cpu, out_mps.cpu(), rtol=2.6e-05, atol=2e-04)    def test_conv1d_contiguous(self):        model_cpu = torch.nn.Conv1d(1, 128, 3)        a_cpu = torch.ones(128, 1, 176)        out_cpu = model_cpu(a_cpu)        a_mps = a_cpu.detach().clone().to(\"mps\")        model_mps = model_cpu.to(\"mps\")        out_mps = model_mps(a_mps)        self.assertEqual(out_cpu.shape, out_mps.shape)        self.assertEqual(out_cpu, out_mps.cpu())    # Test sigmoid    def test_sigmoid(self):        def helper(shape):",
    "method_name": "def helper(input_shape, wt_shape,"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83106,
  "title": "Reland \"[functorch] add error inputs check to vmap test\"",
  "tags": [
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-10T21:36:03Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "functorch/functorch/csrc/BatchRulesActivation.cpp",
    "code1": " const auto input_ = moveBatchDimToFront(input, input_bdim); auto weight_flatten = moveBatchDimToFront(weight, weight_bdim); if (weight_flatten.dim() > 1) { // for an input [N, C, ...] // weight can be a non-vector but the total number of elements must be the same as C",
    "code2": " const auto input_ = moveBatchDimToFront(input, input_bdim); auto weight_flatten = moveBatchDimToFront(weight, weight_bdim); const auto weight_logical_dim = rankWithoutBatchDim(weight, weight_bdim); TORCH_CHECK(weight_logical_dim == 0 || weight_logical_dim == 1, \"prelu: Expected `weight` to be a scalar or 1D tensor, but got ndim = \",      weight_logical_dim); if (weight_flatten.dim() > 1) { // for an input [N, C, ...] // weight can be a non-vector but the total number of elements must be the same as C",
    "method_name": "std::tuple<Tensor,optional<int64_t>> prelu_batch_rule("
   },
   {
    "language": ".py",
    "dir": "functorch/test/test_vmap.py",
    "code1": "    def opinfo_vmap_test(self, device, dtype, op, check_has_batch_rule, skip_inplace=()):        def test():            sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)            aliases, inplace_aliases = discover_variants(op)            check_shape_only = op.name in ('empty_like', 'new_empty')",
    "code2": "    def opinfo_vmap_test(self, device, dtype, op, check_has_batch_rule, skip_inplace=()):        def test():            # Error inputs check            if op.error_inputs_func is not None:                error_inputs = op.error_inputs(device)                for error_input in error_inputs:                    sample_input = error_input.sample_input                    args = (sample_input.input,) + tuple(sample_input.args)                    kwargs = sample_input.kwargs                    for args, in_dims, _ in generate_vmap_inputs(args, {}):                        with self.assertRaises(Exception):                            vmap(op, in_dims)(*args, **kwargs)            # Sample inputs check            sample_inputs_itr = op.sample_inputs(device, dtype, requires_grad=False)            aliases, inplace_aliases = discover_variants(op)            check_shape_only = op.name in ('empty_like', 'new_empty')",
    "method_name": "def vmap_inplace_test(self, func, args, kwargs, in_dims):"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83105,
  "title": "Fix error_inputs for linalg.lstsq; assert SampleInput args are tuple, take2",
  "tags": [
   "ciflow/trunk",
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-10T17:37:32Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "test/test_jit_cuda_fuser.py",
    "code1": " def _get_extremal_sample(sample: SampleInput, val, dtype): extremal_sample = SampleInput( input=_get_extremal_input(sample.input, val, dtype), args=[_get_extremal_input(x, val, dtype) for x in sample.args], kwargs={k: _get_extremal_input(v, val, dtype) for k, v in sample.kwargs.items()},            ) return extremal_sample vals = [float('inf'), float('-inf'), float('nan')] if dtype.is_complex: complex_vals = itertools.product(vals, vals) vals = list(map(lambda x: complex(*x), complex_vals)) for val in vals: yield _get_extremal_sample(sample, val, dtype)",
    "code2": " def _get_extremal_sample(sample: SampleInput, val, dtype): extremal_sample = SampleInput( input=_get_extremal_input(sample.input, val, dtype), args=tuple(_get_extremal_input(x, val, dtype) for x in sample.args), kwargs={k: _get_extremal_input(v, val, dtype) for k, v in sample.kwargs.items()},            ) return extremal_sample vals = [float('inf'), float('-inf'), float('nan')] if dtype.is_complex: complex_vals = itertools.product(vals, vals) vals = tuple(map(lambda x: complex(*x), complex_vals)) for val in vals: yield _get_extremal_sample(sample, val, dtype)",
    "method_name": "def _get_extremal_input(x, val, dtype):"
   },
   {
    "language": ".py",
    "dir": "torch/testing/_internal/common_methods_invocations.py",
    "code1": "    for shape in shapes:        inp, *arg0 = shape        yield SampleInput(inp, args=arg0)def sample_inputs_add_sub(op, device, dtype, requires_grad, **kwargs):    yield from sample_inputs_elementwise_binary(op, device, dtype, requires_grad, **kwargs)def error_inputs_lstsq(op_info, device, **kwargs):    zero_d = torch.randn((), device=device)    yield ErrorInput(SampleInput(zero_d, args=(zero_d)), error_type=TypeError,                     error_regex=\"iteration over a 0-d tensor\")def error_inputs_eig(op_info, device, **kwargs):    zero_d = torch.randn((), device=device)           supports_out=False,           dtypes=floating_and_complex_types(),           sample_inputs_func=sample_inputs_linalg_lstsq,           error_inputs_func=error_inputs_lstsq,           # Runs very slowly on slow gradcheck - alternatively reduce input sizes           gradcheck_fast_mode=True,           supports_autograd=True,",
    "code2": "    for shape in shapes:        inp, *arg0 = shape        yield SampleInput(inp, args=tuple(arg0))def sample_inputs_add_sub(op, device, dtype, requires_grad, **kwargs):    yield from sample_inputs_elementwise_binary(op, device, dtype, requires_grad, **kwargs)def error_inputs_lstsq(op_info, device, **kwargs):    zero_d = torch.randn((), device=device)    yield ErrorInput(SampleInput(zero_d, args=(zero_d,)), error_type=RuntimeError,                     error_regex=\"at least 2 dimensions\")def error_inputs_lstsq_grad_oriented(op_info, device, **kwargs):    zero_d = torch.randn((), device=device)    yield ErrorInput(SampleInput(zero_d, args=(zero_d, None)), error_type=RuntimeError,                     error_regex=\"at least 2 dimensions\")def error_inputs_eig(op_info, device, **kwargs):    zero_d = torch.randn((), device=device)           supports_out=False,           dtypes=floating_and_complex_types(),           sample_inputs_func=sample_inputs_linalg_lstsq,           error_inputs_func=error_inputs_lstsq_grad_oriented,           # Runs very slowly on slow gradcheck - alternatively reduce input sizes           gradcheck_fast_mode=True,           supports_autograd=True,",
    "method_name": "def sample_inputs_broadcast_shapes(op, device, dtype, requires_grad, **kwargs):"
   },
   {
    "language": ".py",
    "dir": "torch/testing/_internal/opinfo/core.py",
    "code1": " # This follows the typical pattern where for Tensor inputs op(t, ...) = t.op(...). self.input = input self.args = args self.kwargs = kwargs if kwargs is not None else {} self.output_process_fn_grad = output_process_fn_grad self.name = name",
    "code2": " # This follows the typical pattern where for Tensor inputs op(t, ...) = t.op(...). self.input = input self.args = args assert isinstance(self.args, tuple) self.kwargs = kwargs if kwargs is not None else {} assert isinstance(self.kwargs, dict) self.output_process_fn_grad = output_process_fn_grad self.name = name",
    "method_name": "def __init__(self, input, *, args=tuple(), kwargs=None, output_process_fn_grad=l"
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83097,
  "title": "Set default qengine to QNNPACK on ARM for quantization tests",
  "tags": [
   "ciflow/periodic",
   "ciflow/trunk",
   "cla signed"
  ],
  "closed_time": "2022-08-11T20:02:03Z",
  "code_diffs": [
   {
    "language": ".py",
    "dir": "test/quantization/core/test_docs.py",
    "code1": "# Owner(s): [\"oncall: quantization\"]import reimport unittestfrom pathlib import Pathimport torch    QuantizationTestCase,    SingleLayerLinearModel,)from torch.testing._internal.common_utils import IS_ARM64    they can be imported either in the test file or passed as a global input    \"\"\"    def _get_code(        self, path_from_pytorch, unique_identifier, offset=2, short_snippet=False    ):            expr = compile(code, \"test\", \"exec\")            exec(expr, global_inputs)    @unittest.skipIf(IS_ARM64, \"Not working on arm\")    def test_quantization_doc_ptdq(self):        path_from_pytorch = \"docs/source/quantization.rst\"        unique_identifier = \"PTDQ API Example::\"        code = self._get_code(path_from_pytorch, unique_identifier)        self._test_code(code)    @unittest.skipIf(IS_ARM64, \"Not working on arm\")    def test_quantization_doc_ptsq(self):        path_from_pytorch = \"docs/source/quantization.rst\"        unique_identifier = \"PTSQ API Example::\"        code = self._get_code(path_from_pytorch, unique_identifier)        self._test_code(code)    @unittest.skipIf(IS_ARM64, \"Not working on arm\")    def test_quantization_doc_qat(self):        path_from_pytorch = \"docs/source/quantization.rst\"        unique_identifier = \"QAT API Example::\"        input_fp32 = torch.randn(1, 1, 1, 1)        global_inputs = {\"training_loop\": _dummy_func, \"input_fp32\": input_fp32}        code = self._get_code(path_from_pytorch, unique_identifier)        self._test_code(code, global_inputs)    @unittest.skipIf(IS_ARM64, \"Not working on arm\")    def test_quantization_doc_fx(self):        path_from_pytorch = \"docs/source/quantization.rst\"        unique_identifier = \"FXPTQ API Example::\"        code = self._get_code(path_from_pytorch, unique_identifier)        self._test_code(code, global_inputs)    @unittest.skipIf(IS_ARM64, \"Not working on arm\")    def test_quantization_doc_custom(self):        path_from_pytorch = \"docs/source/quantization.rst\"        unique_identifier = \"Custom API Example::\"",
    "code2": "# Owner(s): [\"oncall: quantization\"]import reimport contextlibfrom pathlib import Pathimport torch    QuantizationTestCase,    SingleLayerLinearModel,)from torch.testing._internal.common_quantized import override_quantized_enginefrom torch.testing._internal.common_utils import IS_ARM64    they can be imported either in the test file or passed as a global input    \"\"\"    def run(self, result=None):        with override_quantized_engine(\"qnnpack\") if IS_ARM64 else contextlib.nullcontext():            super(TestQuantizationDocs, self).run(result)    def _get_code(        self, path_from_pytorch, unique_identifier, offset=2, short_snippet=False    ):            expr = compile(code, \"test\", \"exec\")            exec(expr, global_inputs)    def test_quantization_doc_ptdq(self):        path_from_pytorch = \"docs/source/quantization.rst\"        unique_identifier = \"PTDQ API Example::\"        code = self._get_code(path_from_pytorch, unique_identifier)        self._test_code(code)    def test_quantization_doc_ptsq(self):        path_from_pytorch = \"docs/source/quantization.rst\"        unique_identifier = \"PTSQ API Example::\"        code = self._get_code(path_from_pytorch, unique_identifier)        self._test_code(code)    def test_quantization_doc_qat(self):        path_from_pytorch = \"docs/source/quantization.rst\"        unique_identifier = \"QAT API Example::\"        input_fp32 = torch.randn(1, 1, 1, 1)        global_inputs = {\"training_loop\": _dummy_func, \"input_fp32\": input_fp32}        code = self._get_code(path_from_pytorch, unique_identifier)        self._test_code(code, global_inputs)    def test_quantization_doc_fx(self):        path_from_pytorch = \"docs/source/quantization.rst\"        unique_identifier = \"FXPTQ API Example::\"        code = self._get_code(path_from_pytorch, unique_identifier)        self._test_code(code, global_inputs)    def test_quantization_doc_custom(self):        path_from_pytorch = \"docs/source/quantization.rst\"        unique_identifier = \"Custom API Example::\"",
    "method_name": ""
   },
   {
    "language": ".py",
    "dir": "test/quantization/fx/test_quantize_fx.py",
    "code1": "from collections import OrderedDictimport osimport torchimport torch.nn.functional as Fimport torch.nn as nn                self.assertEqual(out.device.type, device_after)    @skip_if_no_torchvision    @unittest.skipIf(IS_ARM64, \"Not working on arm\")    def test_model_dropout(self):        from torchvision import models        m = models.mobilenet_v3_small()        qconfig_mapping = torch.ao.quantization.get_default_qat_qconfig_mapping('fbgemm')        example_inputs = (torch.randn(1, 3, 224, 224),)        mp = prepare_qat_fx(m, qconfig_mapping, example_inputs=example_inputs)        mp(*example_inputs)        mq = convert_fx(mp)        res = mq(*example_inputs)    def _test_model_impl(            self, mode, name, model, eager_quantizable_model,",
    "code2": "from collections import OrderedDictimport osimport contextlibimport torchimport torch.nn.functional as Fimport torch.nn as nn                self.assertEqual(out.device.type, device_after)    @skip_if_no_torchvision    def test_model_dropout(self):        from torchvision import models        m = models.mobilenet_v3_small()        qconfig_mapping = torch.ao.quantization.get_default_qat_qconfig_mapping('fbgemm')        example_inputs = (torch.randn(1, 3, 224, 224),)        mp = prepare_qat_fx(m, qconfig_mapping, example_inputs=example_inputs)        mp(*example_inputs)        with override_quantized_engine(\"qnnpack\") if IS_ARM64 else contextlib.nullcontext():            mq = convert_fx(mp)        mq(*example_inputs)    def _test_model_impl(            self, mode, name, model, eager_quantizable_model,",
    "method_name": ""
   }
  ]
 },
 {
  "group_id": "pytorch",
  "artifact_id": "pytorch",
  "oid": 83049,
  "title": "Build MacOS binaries with -Werror",
  "tags": [
   "ciflow/trunk",
   "cla signed",
   "Merged"
  ],
  "closed_time": "2022-08-10T17:29:48Z",
  "code_diffs": [
   {
    "language": ".cpp",
    "dir": "test/cpp/api/dataloader.cpp",
    "code1": "  auto data_loader = torch::data::make_data_loader(      TestIndexDataset(23), TestIndexSampler(23), kBatchSize);  size_t i = 0;  for (auto batch : *data_loader) {    for (const auto j : c10::irange(kBatchSize)) {      ASSERT_EQ(batch.at(j), 10 + j);    }    i += 1;  }}",
    "code2": "  auto data_loader = torch::data::make_data_loader(      TestIndexDataset(23), TestIndexSampler(23), kBatchSize);  for (auto batch : *data_loader) {    for (const auto j : c10::irange(kBatchSize)) {      ASSERT_EQ(batch.at(j), 10 + j);    }  }}",
    "method_name": "TEST(DataTest, CanUseCustomTypeAsIndexType) {"
   }
  ]
 }
]